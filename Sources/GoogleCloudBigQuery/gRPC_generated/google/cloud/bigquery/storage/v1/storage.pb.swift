// DO NOT EDIT.
// swift-format-ignore-file
// swiftlint:disable all
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/bigquery/storage/v1/storage.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// Request message for `CreateReadSession`.
package struct Google_Cloud_Bigquery_Storage_V1_CreateReadSessionRequest: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The request project that owns the session, in the form of
  /// `projects/{project_id}`.
  package var parent: String = String()

  /// Required. Session to be created.
  package var readSession: Google_Cloud_Bigquery_Storage_V1_ReadSession {
    get {return _readSession ?? Google_Cloud_Bigquery_Storage_V1_ReadSession()}
    set {_readSession = newValue}
  }
  /// Returns true if `readSession` has been explicitly set.
  package var hasReadSession: Bool {return self._readSession != nil}
  /// Clears the value of `readSession`. Subsequent reads from it will return its default value.
  package mutating func clearReadSession() {self._readSession = nil}

  /// Max initial number of streams. If unset or zero, the server will
  /// provide a value of streams so as to produce reasonable throughput. Must be
  /// non-negative. The number of streams may be lower than the requested number,
  /// depending on the amount parallelism that is reasonable for the table.
  /// There is a default system max limit of 1,000.
  ///
  /// This must be greater than or equal to preferred_min_stream_count.
  /// Typically, clients should either leave this unset to let the system to
  /// determine an upper bound OR set this a size for the maximum "units of work"
  /// it can gracefully handle.
  package var maxStreamCount: Int32 = 0

  /// The minimum preferred stream count. This parameter can be used to inform
  /// the service that there is a desired lower bound on the number of streams.
  /// This is typically a target parallelism of the client (e.g. a Spark
  /// cluster with N-workers would set this to a low multiple of N to ensure
  /// good cluster utilization).
  ///
  /// The system will make a best effort to provide at least this number of
  /// streams, but in some cases might provide less.
  package var preferredMinStreamCount: Int32 = 0

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _readSession: Google_Cloud_Bigquery_Storage_V1_ReadSession? = nil
}

/// Request message for `ReadRows`.
package struct Google_Cloud_Bigquery_Storage_V1_ReadRowsRequest: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Stream to read rows from.
  package var readStream: String = String()

  /// The offset requested must be less than the last row read from Read.
  /// Requesting a larger offset is undefined. If not specified, start reading
  /// from offset zero.
  package var offset: Int64 = 0

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}
}

/// Information on if the current connection is being throttled.
package struct Google_Cloud_Bigquery_Storage_V1_ThrottleState: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// How much this connection is being throttled. Zero means no throttling,
  /// 100 means fully throttled.
  package var throttlePercent: Int32 = 0

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}
}

/// Estimated stream statistics for a given read Stream.
package struct Google_Cloud_Bigquery_Storage_V1_StreamStats: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Represents the progress of the current stream.
  package var progress: Google_Cloud_Bigquery_Storage_V1_StreamStats.Progress {
    get {return _progress ?? Google_Cloud_Bigquery_Storage_V1_StreamStats.Progress()}
    set {_progress = newValue}
  }
  /// Returns true if `progress` has been explicitly set.
  package var hasProgress: Bool {return self._progress != nil}
  /// Clears the value of `progress`. Subsequent reads from it will return its default value.
  package mutating func clearProgress() {self._progress = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package struct Progress: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// The fraction of rows assigned to the stream that have been processed by
    /// the server so far, not including the rows in the current response
    /// message.
    ///
    /// This value, along with `at_response_end`, can be used to interpolate
    /// the progress made as the rows in the message are being processed using
    /// the following formula: `at_response_start + (at_response_end -
    /// at_response_start) * rows_processed_from_response / rows_in_response`.
    ///
    /// Note that if a filter is provided, the `at_response_end` value of the
    /// previous response may not necessarily be equal to the
    /// `at_response_start` value of the current response.
    package var atResponseStart: Double = 0

    /// Similar to `at_response_start`, except that this value includes the
    /// rows in the current response.
    package var atResponseEnd: Double = 0

    package var unknownFields = SwiftProtobuf.UnknownStorage()

    package init() {}
  }

  package init() {}

  fileprivate var _progress: Google_Cloud_Bigquery_Storage_V1_StreamStats.Progress? = nil
}

/// Response from calling `ReadRows` may include row data, progress and
/// throttling information.
package struct Google_Cloud_Bigquery_Storage_V1_ReadRowsResponse: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Row data is returned in format specified during session creation.
  package var rows: Google_Cloud_Bigquery_Storage_V1_ReadRowsResponse.OneOf_Rows? = nil

  /// Serialized row data in AVRO format.
  package var avroRows: Google_Cloud_Bigquery_Storage_V1_AvroRows {
    get {
      if case .avroRows(let v)? = rows {return v}
      return Google_Cloud_Bigquery_Storage_V1_AvroRows()
    }
    set {rows = .avroRows(newValue)}
  }

  /// Serialized row data in Arrow RecordBatch format.
  package var arrowRecordBatch: Google_Cloud_Bigquery_Storage_V1_ArrowRecordBatch {
    get {
      if case .arrowRecordBatch(let v)? = rows {return v}
      return Google_Cloud_Bigquery_Storage_V1_ArrowRecordBatch()
    }
    set {rows = .arrowRecordBatch(newValue)}
  }

  /// Number of serialized rows in the rows block.
  package var rowCount: Int64 = 0

  /// Statistics for the stream.
  package var stats: Google_Cloud_Bigquery_Storage_V1_StreamStats {
    get {return _stats ?? Google_Cloud_Bigquery_Storage_V1_StreamStats()}
    set {_stats = newValue}
  }
  /// Returns true if `stats` has been explicitly set.
  package var hasStats: Bool {return self._stats != nil}
  /// Clears the value of `stats`. Subsequent reads from it will return its default value.
  package mutating func clearStats() {self._stats = nil}

  /// Throttling state. If unset, the latest response still describes
  /// the current throttling status.
  package var throttleState: Google_Cloud_Bigquery_Storage_V1_ThrottleState {
    get {return _throttleState ?? Google_Cloud_Bigquery_Storage_V1_ThrottleState()}
    set {_throttleState = newValue}
  }
  /// Returns true if `throttleState` has been explicitly set.
  package var hasThrottleState: Bool {return self._throttleState != nil}
  /// Clears the value of `throttleState`. Subsequent reads from it will return its default value.
  package mutating func clearThrottleState() {self._throttleState = nil}

  /// The schema for the read. If read_options.selected_fields is set, the
  /// schema may be different from the table schema as it will only contain
  /// the selected fields. This schema is equivalent to the one returned by
  /// CreateSession. This field is only populated in the first ReadRowsResponse
  /// RPC.
  package var schema: Google_Cloud_Bigquery_Storage_V1_ReadRowsResponse.OneOf_Schema? = nil

  /// Output only. Avro schema.
  package var avroSchema: Google_Cloud_Bigquery_Storage_V1_AvroSchema {
    get {
      if case .avroSchema(let v)? = schema {return v}
      return Google_Cloud_Bigquery_Storage_V1_AvroSchema()
    }
    set {schema = .avroSchema(newValue)}
  }

  /// Output only. Arrow schema.
  package var arrowSchema: Google_Cloud_Bigquery_Storage_V1_ArrowSchema {
    get {
      if case .arrowSchema(let v)? = schema {return v}
      return Google_Cloud_Bigquery_Storage_V1_ArrowSchema()
    }
    set {schema = .arrowSchema(newValue)}
  }

  /// Optional. If the row data in this ReadRowsResponse is compressed, then
  /// uncompressed byte size is the original size of the uncompressed row data.
  /// If it is set to a value greater than 0, then decompress into a buffer of
  /// size uncompressed_byte_size using the compression codec that was requested
  /// during session creation time and which is specified in
  /// TableReadOptions.response_compression_codec in ReadSession.
  /// This value is not set if no response_compression_codec was not requested
  /// and it is -1 if the requested compression would not have reduced the size
  /// of this ReadRowsResponse's row data. This attempts to match Apache Arrow's
  /// behavior described here https://github.com/apache/arrow/issues/15102 where
  /// the uncompressed length may be set to -1 to indicate that the data that
  /// follows is not compressed, which can be useful for cases where compression
  /// does not yield appreciable savings. When uncompressed_byte_size is not
  /// greater than 0, the client should skip decompression.
  package var uncompressedByteSize: Int64 {
    get {return _uncompressedByteSize ?? 0}
    set {_uncompressedByteSize = newValue}
  }
  /// Returns true if `uncompressedByteSize` has been explicitly set.
  package var hasUncompressedByteSize: Bool {return self._uncompressedByteSize != nil}
  /// Clears the value of `uncompressedByteSize`. Subsequent reads from it will return its default value.
  package mutating func clearUncompressedByteSize() {self._uncompressedByteSize = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Row data is returned in format specified during session creation.
  package enum OneOf_Rows: Equatable, Sendable {
    /// Serialized row data in AVRO format.
    case avroRows(Google_Cloud_Bigquery_Storage_V1_AvroRows)
    /// Serialized row data in Arrow RecordBatch format.
    case arrowRecordBatch(Google_Cloud_Bigquery_Storage_V1_ArrowRecordBatch)

  }

  /// The schema for the read. If read_options.selected_fields is set, the
  /// schema may be different from the table schema as it will only contain
  /// the selected fields. This schema is equivalent to the one returned by
  /// CreateSession. This field is only populated in the first ReadRowsResponse
  /// RPC.
  package enum OneOf_Schema: Equatable, Sendable {
    /// Output only. Avro schema.
    case avroSchema(Google_Cloud_Bigquery_Storage_V1_AvroSchema)
    /// Output only. Arrow schema.
    case arrowSchema(Google_Cloud_Bigquery_Storage_V1_ArrowSchema)

  }

  package init() {}

  fileprivate var _stats: Google_Cloud_Bigquery_Storage_V1_StreamStats? = nil
  fileprivate var _throttleState: Google_Cloud_Bigquery_Storage_V1_ThrottleState? = nil
  fileprivate var _uncompressedByteSize: Int64? = nil
}

/// Request message for `SplitReadStream`.
package struct Google_Cloud_Bigquery_Storage_V1_SplitReadStreamRequest: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Name of the stream to split.
  package var name: String = String()

  /// A value in the range (0.0, 1.0) that specifies the fractional point at
  /// which the original stream should be split. The actual split point is
  /// evaluated on pre-filtered rows, so if a filter is provided, then there is
  /// no guarantee that the division of the rows between the new child streams
  /// will be proportional to this fractional value. Additionally, because the
  /// server-side unit for assigning data is collections of rows, this fraction
  /// will always map to a data storage boundary on the server side.
  package var fraction: Double = 0

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}
}

/// Response message for `SplitReadStream`.
package struct Google_Cloud_Bigquery_Storage_V1_SplitReadStreamResponse: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Primary stream, which contains the beginning portion of
  /// |original_stream|. An empty value indicates that the original stream can no
  /// longer be split.
  package var primaryStream: Google_Cloud_Bigquery_Storage_V1_ReadStream {
    get {return _primaryStream ?? Google_Cloud_Bigquery_Storage_V1_ReadStream()}
    set {_primaryStream = newValue}
  }
  /// Returns true if `primaryStream` has been explicitly set.
  package var hasPrimaryStream: Bool {return self._primaryStream != nil}
  /// Clears the value of `primaryStream`. Subsequent reads from it will return its default value.
  package mutating func clearPrimaryStream() {self._primaryStream = nil}

  /// Remainder stream, which contains the tail of |original_stream|. An empty
  /// value indicates that the original stream can no longer be split.
  package var remainderStream: Google_Cloud_Bigquery_Storage_V1_ReadStream {
    get {return _remainderStream ?? Google_Cloud_Bigquery_Storage_V1_ReadStream()}
    set {_remainderStream = newValue}
  }
  /// Returns true if `remainderStream` has been explicitly set.
  package var hasRemainderStream: Bool {return self._remainderStream != nil}
  /// Clears the value of `remainderStream`. Subsequent reads from it will return its default value.
  package mutating func clearRemainderStream() {self._remainderStream = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _primaryStream: Google_Cloud_Bigquery_Storage_V1_ReadStream? = nil
  fileprivate var _remainderStream: Google_Cloud_Bigquery_Storage_V1_ReadStream? = nil
}

/// Request message for `CreateWriteStream`.
package struct Google_Cloud_Bigquery_Storage_V1_CreateWriteStreamRequest: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Reference to the table to which the stream belongs, in the format
  /// of `projects/{project}/datasets/{dataset}/tables/{table}`.
  package var parent: String = String()

  /// Required. Stream to be created.
  package var writeStream: Google_Cloud_Bigquery_Storage_V1_WriteStream {
    get {return _writeStream ?? Google_Cloud_Bigquery_Storage_V1_WriteStream()}
    set {_writeStream = newValue}
  }
  /// Returns true if `writeStream` has been explicitly set.
  package var hasWriteStream: Bool {return self._writeStream != nil}
  /// Clears the value of `writeStream`. Subsequent reads from it will return its default value.
  package mutating func clearWriteStream() {self._writeStream = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _writeStream: Google_Cloud_Bigquery_Storage_V1_WriteStream? = nil
}

/// Request message for `AppendRows`.
///
/// Because AppendRows is a bidirectional streaming RPC, certain parts of the
/// AppendRowsRequest need only be specified for the first request before
/// switching table destinations. You can also switch table destinations within
/// the same connection for the default stream.
///
/// The size of a single AppendRowsRequest must be less than 10 MB in size.
/// Requests larger than this return an error, typically `INVALID_ARGUMENT`.
package struct Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The write_stream identifies the append operation. It must be
  /// provided in the following scenarios:
  ///
  /// * In the first request to an AppendRows connection.
  ///
  /// * In all subsequent requests to an AppendRows connection, if you use the
  /// same connection to write to multiple tables or change the input schema for
  /// default streams.
  ///
  /// For explicitly created write streams, the format is:
  ///
  /// * `projects/{project}/datasets/{dataset}/tables/{table}/streams/{id}`
  ///
  /// For the special default stream, the format is:
  ///
  /// * `projects/{project}/datasets/{dataset}/tables/{table}/streams/_default`.
  ///
  /// An example of a possible sequence of requests with write_stream fields
  /// within a single connection:
  ///
  /// * r1: {write_stream: stream_name_1}
  ///
  /// * r2: {write_stream: /*omit*/}
  ///
  /// * r3: {write_stream: /*omit*/}
  ///
  /// * r4: {write_stream: stream_name_2}
  ///
  /// * r5: {write_stream: stream_name_2}
  ///
  /// The destination changed in request_4, so the write_stream field must be
  /// populated in all subsequent requests in this stream.
  package var writeStream: String = String()

  /// If present, the write is only performed if the next append offset is same
  /// as the provided value. If not present, the write is performed at the
  /// current end of stream. Specifying a value for this field is not allowed
  /// when calling AppendRows for the '_default' stream.
  package var offset: SwiftProtobuf.Google_Protobuf_Int64Value {
    get {return _offset ?? SwiftProtobuf.Google_Protobuf_Int64Value()}
    set {_offset = newValue}
  }
  /// Returns true if `offset` has been explicitly set.
  package var hasOffset: Bool {return self._offset != nil}
  /// Clears the value of `offset`. Subsequent reads from it will return its default value.
  package mutating func clearOffset() {self._offset = nil}

  /// Input rows. The `writer_schema` field must be specified at the initial
  /// request and currently, it will be ignored if specified in following
  /// requests. Following requests must have data in the same format as the
  /// initial request.
  package var rows: Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.OneOf_Rows? = nil

  /// Rows in proto format.
  package var protoRows: Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.ProtoData {
    get {
      if case .protoRows(let v)? = rows {return v}
      return Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.ProtoData()
    }
    set {rows = .protoRows(newValue)}
  }

  /// Rows in arrow format. This is an experimental feature only selected for
  /// allowlisted customers.
  package var arrowRows: Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.ArrowData {
    get {
      if case .arrowRows(let v)? = rows {return v}
      return Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.ArrowData()
    }
    set {rows = .arrowRows(newValue)}
  }

  /// Id set by client to annotate its identity. Only initial request setting is
  /// respected.
  package var traceID: String = String()

  /// A map to indicate how to interpret missing value for some fields. Missing
  /// values are fields present in user schema but missing in rows. The key is
  /// the field name. The value is the interpretation of missing values for the
  /// field.
  ///
  /// For example, a map {'foo': NULL_VALUE, 'bar': DEFAULT_VALUE} means all
  /// missing values in field foo are interpreted as NULL, all missing values in
  /// field bar are interpreted as the default value of field bar in table
  /// schema.
  ///
  /// If a field is not in this map and has missing values, the missing values
  /// in this field are interpreted as NULL.
  ///
  /// This field only applies to the current request, it won't affect other
  /// requests on the connection.
  ///
  /// Currently, field name can only be top-level column name, can't be a struct
  /// field path like 'foo.bar'.
  package var missingValueInterpretations: Dictionary<String,Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.MissingValueInterpretation> = [:]

  /// Optional. Default missing value interpretation for all columns in the
  /// table. When a value is specified on an `AppendRowsRequest`, it is applied
  /// to all requests on the connection from that point forward, until a
  /// subsequent `AppendRowsRequest` sets it to a different value.
  /// `missing_value_interpretation` can override
  /// `default_missing_value_interpretation`. For example, if you want to write
  /// `NULL` instead of using default values for some columns, you can set
  /// `default_missing_value_interpretation` to `DEFAULT_VALUE` and at the same
  /// time, set `missing_value_interpretations` to `NULL_VALUE` on those columns.
  package var defaultMissingValueInterpretation: Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.MissingValueInterpretation = .unspecified

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Input rows. The `writer_schema` field must be specified at the initial
  /// request and currently, it will be ignored if specified in following
  /// requests. Following requests must have data in the same format as the
  /// initial request.
  package enum OneOf_Rows: Equatable, Sendable {
    /// Rows in proto format.
    case protoRows(Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.ProtoData)
    /// Rows in arrow format. This is an experimental feature only selected for
    /// allowlisted customers.
    case arrowRows(Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.ArrowData)

    fileprivate var isInitialized: Bool {
      guard case .protoRows(let v) = self else {return true}
      return v.isInitialized
    }

  }

  /// An enum to indicate how to interpret missing values of fields that are
  /// present in user schema but missing in rows. A missing value can represent a
  /// NULL or a column default value defined in BigQuery table schema.
  package enum MissingValueInterpretation: SwiftProtobuf.Enum, Swift.CaseIterable {
    package typealias RawValue = Int

    /// Invalid missing value interpretation. Requests with this value will be
    /// rejected.
    case unspecified // = 0

    /// Missing value is interpreted as NULL.
    case nullValue // = 1

    /// Missing value is interpreted as column default value if declared in the
    /// table schema, NULL otherwise.
    case defaultValue // = 2
    case UNRECOGNIZED(Int)

    package init() {
      self = .unspecified
    }

    package init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .nullValue
      case 2: self = .defaultValue
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    package var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .nullValue: return 1
      case .defaultValue: return 2
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    package static let allCases: [Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.MissingValueInterpretation] = [
      .unspecified,
      .nullValue,
      .defaultValue,
    ]

  }

  /// Arrow schema and data.
  /// Arrow format is an experimental feature only selected for allowlisted
  /// customers.
  package struct ArrowData: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Optional. Arrow Schema used to serialize the data.
    package var writerSchema: Google_Cloud_Bigquery_Storage_V1_ArrowSchema {
      get {return _writerSchema ?? Google_Cloud_Bigquery_Storage_V1_ArrowSchema()}
      set {_writerSchema = newValue}
    }
    /// Returns true if `writerSchema` has been explicitly set.
    package var hasWriterSchema: Bool {return self._writerSchema != nil}
    /// Clears the value of `writerSchema`. Subsequent reads from it will return its default value.
    package mutating func clearWriterSchema() {self._writerSchema = nil}

    /// Required. Serialized row data in Arrow format.
    package var rows: Google_Cloud_Bigquery_Storage_V1_ArrowRecordBatch {
      get {return _rows ?? Google_Cloud_Bigquery_Storage_V1_ArrowRecordBatch()}
      set {_rows = newValue}
    }
    /// Returns true if `rows` has been explicitly set.
    package var hasRows: Bool {return self._rows != nil}
    /// Clears the value of `rows`. Subsequent reads from it will return its default value.
    package mutating func clearRows() {self._rows = nil}

    package var unknownFields = SwiftProtobuf.UnknownStorage()

    package init() {}

    fileprivate var _writerSchema: Google_Cloud_Bigquery_Storage_V1_ArrowSchema? = nil
    fileprivate var _rows: Google_Cloud_Bigquery_Storage_V1_ArrowRecordBatch? = nil
  }

  /// ProtoData contains the data rows and schema when constructing append
  /// requests.
  package struct ProtoData: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// The protocol buffer schema used to serialize the data. Provide this value
    /// whenever:
    ///
    /// * You send the first request of an RPC connection.
    ///
    /// * You change the input schema.
    ///
    /// * You specify a new destination table.
    package var writerSchema: Google_Cloud_Bigquery_Storage_V1_ProtoSchema {
      get {return _writerSchema ?? Google_Cloud_Bigquery_Storage_V1_ProtoSchema()}
      set {_writerSchema = newValue}
    }
    /// Returns true if `writerSchema` has been explicitly set.
    package var hasWriterSchema: Bool {return self._writerSchema != nil}
    /// Clears the value of `writerSchema`. Subsequent reads from it will return its default value.
    package mutating func clearWriterSchema() {self._writerSchema = nil}

    /// Serialized row data in protobuf message format.
    /// Currently, the backend expects the serialized rows to adhere to
    /// proto2 semantics when appending rows, particularly with respect to
    /// how default values are encoded.
    package var rows: Google_Cloud_Bigquery_Storage_V1_ProtoRows {
      get {return _rows ?? Google_Cloud_Bigquery_Storage_V1_ProtoRows()}
      set {_rows = newValue}
    }
    /// Returns true if `rows` has been explicitly set.
    package var hasRows: Bool {return self._rows != nil}
    /// Clears the value of `rows`. Subsequent reads from it will return its default value.
    package mutating func clearRows() {self._rows = nil}

    package var unknownFields = SwiftProtobuf.UnknownStorage()

    package init() {}

    fileprivate var _writerSchema: Google_Cloud_Bigquery_Storage_V1_ProtoSchema? = nil
    fileprivate var _rows: Google_Cloud_Bigquery_Storage_V1_ProtoRows? = nil
  }

  package init() {}

  fileprivate var _offset: SwiftProtobuf.Google_Protobuf_Int64Value? = nil
}

/// Response message for `AppendRows`.
package struct Google_Cloud_Bigquery_Storage_V1_AppendRowsResponse: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  package var response: Google_Cloud_Bigquery_Storage_V1_AppendRowsResponse.OneOf_Response? = nil

  /// Result if the append is successful.
  package var appendResult: Google_Cloud_Bigquery_Storage_V1_AppendRowsResponse.AppendResult {
    get {
      if case .appendResult(let v)? = response {return v}
      return Google_Cloud_Bigquery_Storage_V1_AppendRowsResponse.AppendResult()
    }
    set {response = .appendResult(newValue)}
  }

  /// Error returned when problems were encountered.  If present,
  /// it indicates rows were not accepted into the system.
  /// Users can retry or continue with other append requests within the
  /// same connection.
  ///
  /// Additional information about error signalling:
  ///
  /// ALREADY_EXISTS: Happens when an append specified an offset, and the
  /// backend already has received data at this offset.  Typically encountered
  /// in retry scenarios, and can be ignored.
  ///
  /// OUT_OF_RANGE: Returned when the specified offset in the stream is beyond
  /// the current end of the stream.
  ///
  /// INVALID_ARGUMENT: Indicates a malformed request or data.
  ///
  /// ABORTED: Request processing is aborted because of prior failures.  The
  /// request can be retried if previous failure is addressed.
  ///
  /// INTERNAL: Indicates server side error(s) that can be retried.
  package var error: Google_Rpc_Status {
    get {
      if case .error(let v)? = response {return v}
      return Google_Rpc_Status()
    }
    set {response = .error(newValue)}
  }

  /// If backend detects a schema update, pass it to user so that user can
  /// use it to input new type of message. It will be empty when no schema
  /// updates have occurred.
  package var updatedSchema: Google_Cloud_Bigquery_Storage_V1_TableSchema {
    get {return _updatedSchema ?? Google_Cloud_Bigquery_Storage_V1_TableSchema()}
    set {_updatedSchema = newValue}
  }
  /// Returns true if `updatedSchema` has been explicitly set.
  package var hasUpdatedSchema: Bool {return self._updatedSchema != nil}
  /// Clears the value of `updatedSchema`. Subsequent reads from it will return its default value.
  package mutating func clearUpdatedSchema() {self._updatedSchema = nil}

  /// If a request failed due to corrupted rows, no rows in the batch will be
  /// appended. The API will return row level error info, so that the caller can
  /// remove the bad rows and retry the request.
  package var rowErrors: [Google_Cloud_Bigquery_Storage_V1_RowError] = []

  /// The target of the append operation. Matches the write_stream in the
  /// corresponding request.
  package var writeStream: String = String()

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package enum OneOf_Response: Equatable, Sendable {
    /// Result if the append is successful.
    case appendResult(Google_Cloud_Bigquery_Storage_V1_AppendRowsResponse.AppendResult)
    /// Error returned when problems were encountered.  If present,
    /// it indicates rows were not accepted into the system.
    /// Users can retry or continue with other append requests within the
    /// same connection.
    ///
    /// Additional information about error signalling:
    ///
    /// ALREADY_EXISTS: Happens when an append specified an offset, and the
    /// backend already has received data at this offset.  Typically encountered
    /// in retry scenarios, and can be ignored.
    ///
    /// OUT_OF_RANGE: Returned when the specified offset in the stream is beyond
    /// the current end of the stream.
    ///
    /// INVALID_ARGUMENT: Indicates a malformed request or data.
    ///
    /// ABORTED: Request processing is aborted because of prior failures.  The
    /// request can be retried if previous failure is addressed.
    ///
    /// INTERNAL: Indicates server side error(s) that can be retried.
    case error(Google_Rpc_Status)

  }

  /// AppendResult is returned for successful append requests.
  package struct AppendResult: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// The row offset at which the last append occurred. The offset will not be
    /// set if appending using default streams.
    package var offset: SwiftProtobuf.Google_Protobuf_Int64Value {
      get {return _offset ?? SwiftProtobuf.Google_Protobuf_Int64Value()}
      set {_offset = newValue}
    }
    /// Returns true if `offset` has been explicitly set.
    package var hasOffset: Bool {return self._offset != nil}
    /// Clears the value of `offset`. Subsequent reads from it will return its default value.
    package mutating func clearOffset() {self._offset = nil}

    package var unknownFields = SwiftProtobuf.UnknownStorage()

    package init() {}

    fileprivate var _offset: SwiftProtobuf.Google_Protobuf_Int64Value? = nil
  }

  package init() {}

  fileprivate var _updatedSchema: Google_Cloud_Bigquery_Storage_V1_TableSchema? = nil
}

/// Request message for `GetWriteStreamRequest`.
package struct Google_Cloud_Bigquery_Storage_V1_GetWriteStreamRequest: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Name of the stream to get, in the form of
  /// `projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}`.
  package var name: String = String()

  /// Indicates whether to get full or partial view of the WriteStream. If
  /// not set, view returned will be basic.
  package var view: Google_Cloud_Bigquery_Storage_V1_WriteStreamView = .unspecified

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}
}

/// Request message for `BatchCommitWriteStreams`.
package struct Google_Cloud_Bigquery_Storage_V1_BatchCommitWriteStreamsRequest: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Parent table that all the streams should belong to, in the form
  /// of `projects/{project}/datasets/{dataset}/tables/{table}`.
  package var parent: String = String()

  /// Required. The group of streams that will be committed atomically.
  package var writeStreams: [String] = []

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}
}

/// Response message for `BatchCommitWriteStreams`.
package struct Google_Cloud_Bigquery_Storage_V1_BatchCommitWriteStreamsResponse: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The time at which streams were committed in microseconds granularity.
  /// This field will only exist when there are no stream errors.
  /// **Note** if this field is not set, it means the commit was not successful.
  package var commitTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _commitTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_commitTime = newValue}
  }
  /// Returns true if `commitTime` has been explicitly set.
  package var hasCommitTime: Bool {return self._commitTime != nil}
  /// Clears the value of `commitTime`. Subsequent reads from it will return its default value.
  package mutating func clearCommitTime() {self._commitTime = nil}

  /// Stream level error if commit failed. Only streams with error will be in
  /// the list.
  /// If empty, there is no error and all streams are committed successfully.
  /// If non empty, certain streams have errors and ZERO stream is committed due
  /// to atomicity guarantee.
  package var streamErrors: [Google_Cloud_Bigquery_Storage_V1_StorageError] = []

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _commitTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
}

/// Request message for invoking `FinalizeWriteStream`.
package struct Google_Cloud_Bigquery_Storage_V1_FinalizeWriteStreamRequest: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Name of the stream to finalize, in the form of
  /// `projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}`.
  package var name: String = String()

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}
}

/// Response message for `FinalizeWriteStream`.
package struct Google_Cloud_Bigquery_Storage_V1_FinalizeWriteStreamResponse: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Number of rows in the finalized stream.
  package var rowCount: Int64 = 0

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}
}

/// Request message for `FlushRows`.
package struct Google_Cloud_Bigquery_Storage_V1_FlushRowsRequest: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The stream that is the target of the flush operation.
  package var writeStream: String = String()

  /// Ending offset of the flush operation. Rows before this offset(including
  /// this offset) will be flushed.
  package var offset: SwiftProtobuf.Google_Protobuf_Int64Value {
    get {return _offset ?? SwiftProtobuf.Google_Protobuf_Int64Value()}
    set {_offset = newValue}
  }
  /// Returns true if `offset` has been explicitly set.
  package var hasOffset: Bool {return self._offset != nil}
  /// Clears the value of `offset`. Subsequent reads from it will return its default value.
  package mutating func clearOffset() {self._offset = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _offset: SwiftProtobuf.Google_Protobuf_Int64Value? = nil
}

/// Respond message for `FlushRows`.
package struct Google_Cloud_Bigquery_Storage_V1_FlushRowsResponse: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The rows before this offset (including this offset) are flushed.
  package var offset: Int64 = 0

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}
}

/// Structured custom BigQuery Storage error message. The error can be attached
/// as error details in the returned rpc Status. In particular, the use of error
/// codes allows more structured error handling, and reduces the need to evaluate
/// unstructured error text strings.
package struct Google_Cloud_Bigquery_Storage_V1_StorageError: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// BigQuery Storage specific error code.
  package var code: Google_Cloud_Bigquery_Storage_V1_StorageError.StorageErrorCode = .unspecified

  /// Name of the failed entity.
  package var entity: String = String()

  /// Message that describes the error.
  package var errorMessage: String = String()

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Error code for `StorageError`.
  package enum StorageErrorCode: SwiftProtobuf.Enum, Swift.CaseIterable {
    package typealias RawValue = Int

    /// Default error.
    case unspecified // = 0

    /// Table is not found in the system.
    case tableNotFound // = 1

    /// Stream is already committed.
    case streamAlreadyCommitted // = 2

    /// Stream is not found.
    case streamNotFound // = 3

    /// Invalid Stream type.
    /// For example, you try to commit a stream that is not pending.
    case invalidStreamType // = 4

    /// Invalid Stream state.
    /// For example, you try to commit a stream that is not finalized or is
    /// garbaged.
    case invalidStreamState // = 5

    /// Stream is finalized.
    case streamFinalized // = 6

    /// There is a schema mismatch and it is caused by user schema has extra
    /// field than bigquery schema.
    case schemaMismatchExtraFields // = 7

    /// Offset already exists.
    case offsetAlreadyExists // = 8

    /// Offset out of range.
    case offsetOutOfRange // = 9

    /// Customer-managed encryption key (CMEK) not provided for CMEK-enabled
    /// data.
    case cmekNotProvided // = 10

    /// Customer-managed encryption key (CMEK) was incorrectly provided.
    case invalidCmekProvided // = 11

    /// There is an encryption error while using customer-managed encryption key.
    case cmekEncryptionError // = 12

    /// Key Management Service (KMS) service returned an error, which can be
    /// retried.
    case kmsServiceError // = 13

    /// Permission denied while using customer-managed encryption key.
    case kmsPermissionDenied // = 14
    case UNRECOGNIZED(Int)

    package init() {
      self = .unspecified
    }

    package init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .tableNotFound
      case 2: self = .streamAlreadyCommitted
      case 3: self = .streamNotFound
      case 4: self = .invalidStreamType
      case 5: self = .invalidStreamState
      case 6: self = .streamFinalized
      case 7: self = .schemaMismatchExtraFields
      case 8: self = .offsetAlreadyExists
      case 9: self = .offsetOutOfRange
      case 10: self = .cmekNotProvided
      case 11: self = .invalidCmekProvided
      case 12: self = .cmekEncryptionError
      case 13: self = .kmsServiceError
      case 14: self = .kmsPermissionDenied
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    package var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .tableNotFound: return 1
      case .streamAlreadyCommitted: return 2
      case .streamNotFound: return 3
      case .invalidStreamType: return 4
      case .invalidStreamState: return 5
      case .streamFinalized: return 6
      case .schemaMismatchExtraFields: return 7
      case .offsetAlreadyExists: return 8
      case .offsetOutOfRange: return 9
      case .cmekNotProvided: return 10
      case .invalidCmekProvided: return 11
      case .cmekEncryptionError: return 12
      case .kmsServiceError: return 13
      case .kmsPermissionDenied: return 14
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    package static let allCases: [Google_Cloud_Bigquery_Storage_V1_StorageError.StorageErrorCode] = [
      .unspecified,
      .tableNotFound,
      .streamAlreadyCommitted,
      .streamNotFound,
      .invalidStreamType,
      .invalidStreamState,
      .streamFinalized,
      .schemaMismatchExtraFields,
      .offsetAlreadyExists,
      .offsetOutOfRange,
      .cmekNotProvided,
      .invalidCmekProvided,
      .cmekEncryptionError,
      .kmsServiceError,
      .kmsPermissionDenied,
    ]

  }

  package init() {}
}

/// The message that presents row level error info in a request.
package struct Google_Cloud_Bigquery_Storage_V1_RowError: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Index of the malformed row in the request.
  package var index: Int64 = 0

  /// Structured error reason for a row error.
  package var code: Google_Cloud_Bigquery_Storage_V1_RowError.RowErrorCode = .unspecified

  /// Description of the issue encountered when processing the row.
  package var message: String = String()

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Error code for `RowError`.
  package enum RowErrorCode: SwiftProtobuf.Enum, Swift.CaseIterable {
    package typealias RawValue = Int

    /// Default error.
    case unspecified // = 0

    /// One or more fields in the row has errors.
    case fieldsError // = 1
    case UNRECOGNIZED(Int)

    package init() {
      self = .unspecified
    }

    package init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .fieldsError
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    package var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .fieldsError: return 1
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    package static let allCases: [Google_Cloud_Bigquery_Storage_V1_RowError.RowErrorCode] = [
      .unspecified,
      .fieldsError,
    ]

  }

  package init() {}
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.bigquery.storage.v1"

extension Google_Cloud_Bigquery_Storage_V1_CreateReadSessionRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".CreateReadSessionRequest"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "parent"),
    2: .standard(proto: "read_session"),
    3: .standard(proto: "max_stream_count"),
    4: .standard(proto: "preferred_min_stream_count"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.parent) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._readSession) }()
      case 3: try { try decoder.decodeSingularInt32Field(value: &self.maxStreamCount) }()
      case 4: try { try decoder.decodeSingularInt32Field(value: &self.preferredMinStreamCount) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.parent.isEmpty {
      try visitor.visitSingularStringField(value: self.parent, fieldNumber: 1)
    }
    try { if let v = self._readSession {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    if self.maxStreamCount != 0 {
      try visitor.visitSingularInt32Field(value: self.maxStreamCount, fieldNumber: 3)
    }
    if self.preferredMinStreamCount != 0 {
      try visitor.visitSingularInt32Field(value: self.preferredMinStreamCount, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_CreateReadSessionRequest, rhs: Google_Cloud_Bigquery_Storage_V1_CreateReadSessionRequest) -> Bool {
    if lhs.parent != rhs.parent {return false}
    if lhs._readSession != rhs._readSession {return false}
    if lhs.maxStreamCount != rhs.maxStreamCount {return false}
    if lhs.preferredMinStreamCount != rhs.preferredMinStreamCount {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_ReadRowsRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".ReadRowsRequest"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "read_stream"),
    2: .same(proto: "offset"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.readStream) }()
      case 2: try { try decoder.decodeSingularInt64Field(value: &self.offset) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.readStream.isEmpty {
      try visitor.visitSingularStringField(value: self.readStream, fieldNumber: 1)
    }
    if self.offset != 0 {
      try visitor.visitSingularInt64Field(value: self.offset, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_ReadRowsRequest, rhs: Google_Cloud_Bigquery_Storage_V1_ReadRowsRequest) -> Bool {
    if lhs.readStream != rhs.readStream {return false}
    if lhs.offset != rhs.offset {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_ThrottleState: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".ThrottleState"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "throttle_percent"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularInt32Field(value: &self.throttlePercent) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.throttlePercent != 0 {
      try visitor.visitSingularInt32Field(value: self.throttlePercent, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_ThrottleState, rhs: Google_Cloud_Bigquery_Storage_V1_ThrottleState) -> Bool {
    if lhs.throttlePercent != rhs.throttlePercent {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_StreamStats: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".StreamStats"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .same(proto: "progress"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 2: try { try decoder.decodeSingularMessageField(value: &self._progress) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._progress {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_StreamStats, rhs: Google_Cloud_Bigquery_Storage_V1_StreamStats) -> Bool {
    if lhs._progress != rhs._progress {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_StreamStats.Progress: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = Google_Cloud_Bigquery_Storage_V1_StreamStats.protoMessageName + ".Progress"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "at_response_start"),
    2: .standard(proto: "at_response_end"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularDoubleField(value: &self.atResponseStart) }()
      case 2: try { try decoder.decodeSingularDoubleField(value: &self.atResponseEnd) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.atResponseStart.bitPattern != 0 {
      try visitor.visitSingularDoubleField(value: self.atResponseStart, fieldNumber: 1)
    }
    if self.atResponseEnd.bitPattern != 0 {
      try visitor.visitSingularDoubleField(value: self.atResponseEnd, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_StreamStats.Progress, rhs: Google_Cloud_Bigquery_Storage_V1_StreamStats.Progress) -> Bool {
    if lhs.atResponseStart != rhs.atResponseStart {return false}
    if lhs.atResponseEnd != rhs.atResponseEnd {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_ReadRowsResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".ReadRowsResponse"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    3: .standard(proto: "avro_rows"),
    4: .standard(proto: "arrow_record_batch"),
    6: .standard(proto: "row_count"),
    2: .same(proto: "stats"),
    5: .standard(proto: "throttle_state"),
    7: .standard(proto: "avro_schema"),
    8: .standard(proto: "arrow_schema"),
    9: .standard(proto: "uncompressed_byte_size"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 2: try { try decoder.decodeSingularMessageField(value: &self._stats) }()
      case 3: try {
        var v: Google_Cloud_Bigquery_Storage_V1_AvroRows?
        var hadOneofValue = false
        if let current = self.rows {
          hadOneofValue = true
          if case .avroRows(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.rows = .avroRows(v)
        }
      }()
      case 4: try {
        var v: Google_Cloud_Bigquery_Storage_V1_ArrowRecordBatch?
        var hadOneofValue = false
        if let current = self.rows {
          hadOneofValue = true
          if case .arrowRecordBatch(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.rows = .arrowRecordBatch(v)
        }
      }()
      case 5: try { try decoder.decodeSingularMessageField(value: &self._throttleState) }()
      case 6: try { try decoder.decodeSingularInt64Field(value: &self.rowCount) }()
      case 7: try {
        var v: Google_Cloud_Bigquery_Storage_V1_AvroSchema?
        var hadOneofValue = false
        if let current = self.schema {
          hadOneofValue = true
          if case .avroSchema(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.schema = .avroSchema(v)
        }
      }()
      case 8: try {
        var v: Google_Cloud_Bigquery_Storage_V1_ArrowSchema?
        var hadOneofValue = false
        if let current = self.schema {
          hadOneofValue = true
          if case .arrowSchema(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.schema = .arrowSchema(v)
        }
      }()
      case 9: try { try decoder.decodeSingularInt64Field(value: &self._uncompressedByteSize) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._stats {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    switch self.rows {
    case .avroRows?: try {
      guard case .avroRows(let v)? = self.rows else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }()
    case .arrowRecordBatch?: try {
      guard case .arrowRecordBatch(let v)? = self.rows else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }()
    case nil: break
    }
    try { if let v = self._throttleState {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
    } }()
    if self.rowCount != 0 {
      try visitor.visitSingularInt64Field(value: self.rowCount, fieldNumber: 6)
    }
    switch self.schema {
    case .avroSchema?: try {
      guard case .avroSchema(let v)? = self.schema else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
    }()
    case .arrowSchema?: try {
      guard case .arrowSchema(let v)? = self.schema else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
    }()
    case nil: break
    }
    try { if let v = self._uncompressedByteSize {
      try visitor.visitSingularInt64Field(value: v, fieldNumber: 9)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_ReadRowsResponse, rhs: Google_Cloud_Bigquery_Storage_V1_ReadRowsResponse) -> Bool {
    if lhs.rows != rhs.rows {return false}
    if lhs.rowCount != rhs.rowCount {return false}
    if lhs._stats != rhs._stats {return false}
    if lhs._throttleState != rhs._throttleState {return false}
    if lhs.schema != rhs.schema {return false}
    if lhs._uncompressedByteSize != rhs._uncompressedByteSize {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_SplitReadStreamRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".SplitReadStreamRequest"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .same(proto: "fraction"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 2: try { try decoder.decodeSingularDoubleField(value: &self.fraction) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if self.fraction.bitPattern != 0 {
      try visitor.visitSingularDoubleField(value: self.fraction, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_SplitReadStreamRequest, rhs: Google_Cloud_Bigquery_Storage_V1_SplitReadStreamRequest) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs.fraction != rhs.fraction {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_SplitReadStreamResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".SplitReadStreamResponse"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "primary_stream"),
    2: .standard(proto: "remainder_stream"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._primaryStream) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._remainderStream) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._primaryStream {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    try { if let v = self._remainderStream {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_SplitReadStreamResponse, rhs: Google_Cloud_Bigquery_Storage_V1_SplitReadStreamResponse) -> Bool {
    if lhs._primaryStream != rhs._primaryStream {return false}
    if lhs._remainderStream != rhs._remainderStream {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_CreateWriteStreamRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".CreateWriteStreamRequest"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "parent"),
    2: .standard(proto: "write_stream"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.parent) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._writeStream) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.parent.isEmpty {
      try visitor.visitSingularStringField(value: self.parent, fieldNumber: 1)
    }
    try { if let v = self._writeStream {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_CreateWriteStreamRequest, rhs: Google_Cloud_Bigquery_Storage_V1_CreateWriteStreamRequest) -> Bool {
    if lhs.parent != rhs.parent {return false}
    if lhs._writeStream != rhs._writeStream {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".AppendRowsRequest"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "write_stream"),
    2: .same(proto: "offset"),
    4: .standard(proto: "proto_rows"),
    5: .standard(proto: "arrow_rows"),
    6: .standard(proto: "trace_id"),
    7: .standard(proto: "missing_value_interpretations"),
    8: .standard(proto: "default_missing_value_interpretation"),
  ]

  public var isInitialized: Bool {
    if let v = self.rows, !v.isInitialized {return false}
    return true
  }

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.writeStream) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._offset) }()
      case 4: try {
        var v: Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.ProtoData?
        var hadOneofValue = false
        if let current = self.rows {
          hadOneofValue = true
          if case .protoRows(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.rows = .protoRows(v)
        }
      }()
      case 5: try {
        var v: Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.ArrowData?
        var hadOneofValue = false
        if let current = self.rows {
          hadOneofValue = true
          if case .arrowRows(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.rows = .arrowRows(v)
        }
      }()
      case 6: try { try decoder.decodeSingularStringField(value: &self.traceID) }()
      case 7: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufEnumMap<SwiftProtobuf.ProtobufString,Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.MissingValueInterpretation>.self, value: &self.missingValueInterpretations) }()
      case 8: try { try decoder.decodeSingularEnumField(value: &self.defaultMissingValueInterpretation) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.writeStream.isEmpty {
      try visitor.visitSingularStringField(value: self.writeStream, fieldNumber: 1)
    }
    try { if let v = self._offset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    switch self.rows {
    case .protoRows?: try {
      guard case .protoRows(let v)? = self.rows else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }()
    case .arrowRows?: try {
      guard case .arrowRows(let v)? = self.rows else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
    }()
    case nil: break
    }
    if !self.traceID.isEmpty {
      try visitor.visitSingularStringField(value: self.traceID, fieldNumber: 6)
    }
    if !self.missingValueInterpretations.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufEnumMap<SwiftProtobuf.ProtobufString,Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.MissingValueInterpretation>.self, value: self.missingValueInterpretations, fieldNumber: 7)
    }
    if self.defaultMissingValueInterpretation != .unspecified {
      try visitor.visitSingularEnumField(value: self.defaultMissingValueInterpretation, fieldNumber: 8)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest, rhs: Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest) -> Bool {
    if lhs.writeStream != rhs.writeStream {return false}
    if lhs._offset != rhs._offset {return false}
    if lhs.rows != rhs.rows {return false}
    if lhs.traceID != rhs.traceID {return false}
    if lhs.missingValueInterpretations != rhs.missingValueInterpretations {return false}
    if lhs.defaultMissingValueInterpretation != rhs.defaultMissingValueInterpretation {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.MissingValueInterpretation: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "MISSING_VALUE_INTERPRETATION_UNSPECIFIED"),
    1: .same(proto: "NULL_VALUE"),
    2: .same(proto: "DEFAULT_VALUE"),
  ]
}

extension Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.ArrowData: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.protoMessageName + ".ArrowData"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "writer_schema"),
    2: .same(proto: "rows"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._writerSchema) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._rows) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._writerSchema {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    try { if let v = self._rows {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.ArrowData, rhs: Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.ArrowData) -> Bool {
    if lhs._writerSchema != rhs._writerSchema {return false}
    if lhs._rows != rhs._rows {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.ProtoData: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.protoMessageName + ".ProtoData"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "writer_schema"),
    2: .same(proto: "rows"),
  ]

  public var isInitialized: Bool {
    if let v = self._writerSchema, !v.isInitialized {return false}
    return true
  }

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._writerSchema) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._rows) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._writerSchema {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    try { if let v = self._rows {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.ProtoData, rhs: Google_Cloud_Bigquery_Storage_V1_AppendRowsRequest.ProtoData) -> Bool {
    if lhs._writerSchema != rhs._writerSchema {return false}
    if lhs._rows != rhs._rows {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_AppendRowsResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".AppendRowsResponse"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "append_result"),
    2: .same(proto: "error"),
    3: .standard(proto: "updated_schema"),
    4: .standard(proto: "row_errors"),
    5: .standard(proto: "write_stream"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        var v: Google_Cloud_Bigquery_Storage_V1_AppendRowsResponse.AppendResult?
        var hadOneofValue = false
        if let current = self.response {
          hadOneofValue = true
          if case .appendResult(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.response = .appendResult(v)
        }
      }()
      case 2: try {
        var v: Google_Rpc_Status?
        var hadOneofValue = false
        if let current = self.response {
          hadOneofValue = true
          if case .error(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.response = .error(v)
        }
      }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._updatedSchema) }()
      case 4: try { try decoder.decodeRepeatedMessageField(value: &self.rowErrors) }()
      case 5: try { try decoder.decodeSingularStringField(value: &self.writeStream) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    switch self.response {
    case .appendResult?: try {
      guard case .appendResult(let v)? = self.response else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }()
    case .error?: try {
      guard case .error(let v)? = self.response else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    try { if let v = self._updatedSchema {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    } }()
    if !self.rowErrors.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.rowErrors, fieldNumber: 4)
    }
    if !self.writeStream.isEmpty {
      try visitor.visitSingularStringField(value: self.writeStream, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_AppendRowsResponse, rhs: Google_Cloud_Bigquery_Storage_V1_AppendRowsResponse) -> Bool {
    if lhs.response != rhs.response {return false}
    if lhs._updatedSchema != rhs._updatedSchema {return false}
    if lhs.rowErrors != rhs.rowErrors {return false}
    if lhs.writeStream != rhs.writeStream {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_AppendRowsResponse.AppendResult: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = Google_Cloud_Bigquery_Storage_V1_AppendRowsResponse.protoMessageName + ".AppendResult"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "offset"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._offset) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._offset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_AppendRowsResponse.AppendResult, rhs: Google_Cloud_Bigquery_Storage_V1_AppendRowsResponse.AppendResult) -> Bool {
    if lhs._offset != rhs._offset {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_GetWriteStreamRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".GetWriteStreamRequest"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    3: .same(proto: "view"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 3: try { try decoder.decodeSingularEnumField(value: &self.view) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if self.view != .unspecified {
      try visitor.visitSingularEnumField(value: self.view, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_GetWriteStreamRequest, rhs: Google_Cloud_Bigquery_Storage_V1_GetWriteStreamRequest) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs.view != rhs.view {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_BatchCommitWriteStreamsRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".BatchCommitWriteStreamsRequest"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "parent"),
    2: .standard(proto: "write_streams"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.parent) }()
      case 2: try { try decoder.decodeRepeatedStringField(value: &self.writeStreams) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.parent.isEmpty {
      try visitor.visitSingularStringField(value: self.parent, fieldNumber: 1)
    }
    if !self.writeStreams.isEmpty {
      try visitor.visitRepeatedStringField(value: self.writeStreams, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_BatchCommitWriteStreamsRequest, rhs: Google_Cloud_Bigquery_Storage_V1_BatchCommitWriteStreamsRequest) -> Bool {
    if lhs.parent != rhs.parent {return false}
    if lhs.writeStreams != rhs.writeStreams {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_BatchCommitWriteStreamsResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".BatchCommitWriteStreamsResponse"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "commit_time"),
    2: .standard(proto: "stream_errors"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._commitTime) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.streamErrors) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._commitTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    if !self.streamErrors.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.streamErrors, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_BatchCommitWriteStreamsResponse, rhs: Google_Cloud_Bigquery_Storage_V1_BatchCommitWriteStreamsResponse) -> Bool {
    if lhs._commitTime != rhs._commitTime {return false}
    if lhs.streamErrors != rhs.streamErrors {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_FinalizeWriteStreamRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".FinalizeWriteStreamRequest"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_FinalizeWriteStreamRequest, rhs: Google_Cloud_Bigquery_Storage_V1_FinalizeWriteStreamRequest) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_FinalizeWriteStreamResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".FinalizeWriteStreamResponse"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "row_count"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularInt64Field(value: &self.rowCount) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.rowCount != 0 {
      try visitor.visitSingularInt64Field(value: self.rowCount, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_FinalizeWriteStreamResponse, rhs: Google_Cloud_Bigquery_Storage_V1_FinalizeWriteStreamResponse) -> Bool {
    if lhs.rowCount != rhs.rowCount {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_FlushRowsRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".FlushRowsRequest"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "write_stream"),
    2: .same(proto: "offset"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.writeStream) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._offset) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.writeStream.isEmpty {
      try visitor.visitSingularStringField(value: self.writeStream, fieldNumber: 1)
    }
    try { if let v = self._offset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_FlushRowsRequest, rhs: Google_Cloud_Bigquery_Storage_V1_FlushRowsRequest) -> Bool {
    if lhs.writeStream != rhs.writeStream {return false}
    if lhs._offset != rhs._offset {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_FlushRowsResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".FlushRowsResponse"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "offset"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularInt64Field(value: &self.offset) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.offset != 0 {
      try visitor.visitSingularInt64Field(value: self.offset, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_FlushRowsResponse, rhs: Google_Cloud_Bigquery_Storage_V1_FlushRowsResponse) -> Bool {
    if lhs.offset != rhs.offset {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_StorageError: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".StorageError"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "code"),
    2: .same(proto: "entity"),
    3: .standard(proto: "error_message"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.code) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.entity) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.errorMessage) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.code != .unspecified {
      try visitor.visitSingularEnumField(value: self.code, fieldNumber: 1)
    }
    if !self.entity.isEmpty {
      try visitor.visitSingularStringField(value: self.entity, fieldNumber: 2)
    }
    if !self.errorMessage.isEmpty {
      try visitor.visitSingularStringField(value: self.errorMessage, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_StorageError, rhs: Google_Cloud_Bigquery_Storage_V1_StorageError) -> Bool {
    if lhs.code != rhs.code {return false}
    if lhs.entity != rhs.entity {return false}
    if lhs.errorMessage != rhs.errorMessage {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_StorageError.StorageErrorCode: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "STORAGE_ERROR_CODE_UNSPECIFIED"),
    1: .same(proto: "TABLE_NOT_FOUND"),
    2: .same(proto: "STREAM_ALREADY_COMMITTED"),
    3: .same(proto: "STREAM_NOT_FOUND"),
    4: .same(proto: "INVALID_STREAM_TYPE"),
    5: .same(proto: "INVALID_STREAM_STATE"),
    6: .same(proto: "STREAM_FINALIZED"),
    7: .same(proto: "SCHEMA_MISMATCH_EXTRA_FIELDS"),
    8: .same(proto: "OFFSET_ALREADY_EXISTS"),
    9: .same(proto: "OFFSET_OUT_OF_RANGE"),
    10: .same(proto: "CMEK_NOT_PROVIDED"),
    11: .same(proto: "INVALID_CMEK_PROVIDED"),
    12: .same(proto: "CMEK_ENCRYPTION_ERROR"),
    13: .same(proto: "KMS_SERVICE_ERROR"),
    14: .same(proto: "KMS_PERMISSION_DENIED"),
  ]
}

extension Google_Cloud_Bigquery_Storage_V1_RowError: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".RowError"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "index"),
    2: .same(proto: "code"),
    3: .same(proto: "message"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularInt64Field(value: &self.index) }()
      case 2: try { try decoder.decodeSingularEnumField(value: &self.code) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.message) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.index != 0 {
      try visitor.visitSingularInt64Field(value: self.index, fieldNumber: 1)
    }
    if self.code != .unspecified {
      try visitor.visitSingularEnumField(value: self.code, fieldNumber: 2)
    }
    if !self.message.isEmpty {
      try visitor.visitSingularStringField(value: self.message, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_RowError, rhs: Google_Cloud_Bigquery_Storage_V1_RowError) -> Bool {
    if lhs.index != rhs.index {return false}
    if lhs.code != rhs.code {return false}
    if lhs.message != rhs.message {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_RowError.RowErrorCode: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "ROW_ERROR_CODE_UNSPECIFIED"),
    1: .same(proto: "FIELDS_ERROR"),
  ]
}
