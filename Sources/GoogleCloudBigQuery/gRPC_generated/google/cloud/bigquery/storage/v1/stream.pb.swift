// DO NOT EDIT.
// swift-format-ignore-file
// swiftlint:disable all
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/bigquery/storage/v1/stream.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// Data format for input or output data.
package enum Google_Cloud_Bigquery_Storage_V1_DataFormat: SwiftProtobuf.Enum, Swift.CaseIterable {
  package typealias RawValue = Int

  /// Data format is unspecified.
  case unspecified // = 0

  /// Avro is a standard open source row based file format.
  /// See https://avro.apache.org/ for more details.
  case avro // = 1

  /// Arrow is a standard open source column-based message format.
  /// See https://arrow.apache.org/ for more details.
  case arrow // = 2
  case UNRECOGNIZED(Int)

  package init() {
    self = .unspecified
  }

  package init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .avro
    case 2: self = .arrow
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  package var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .avro: return 1
    case .arrow: return 2
    case .UNRECOGNIZED(let i): return i
    }
  }

  // The compiler won't synthesize support with the UNRECOGNIZED case.
  package static let allCases: [Google_Cloud_Bigquery_Storage_V1_DataFormat] = [
    .unspecified,
    .avro,
    .arrow,
  ]

}

/// WriteStreamView is a view enum that controls what details about a write
/// stream should be returned.
package enum Google_Cloud_Bigquery_Storage_V1_WriteStreamView: SwiftProtobuf.Enum, Swift.CaseIterable {
  package typealias RawValue = Int

  /// The default / unset value.
  case unspecified // = 0

  /// The BASIC projection returns basic metadata about a write stream.  The
  /// basic view does not include schema information.  This is the default view
  /// returned by GetWriteStream.
  case basic // = 1

  /// The FULL projection returns all available write stream metadata, including
  /// the schema.  CreateWriteStream returns the full projection of write stream
  /// metadata.
  case full // = 2
  case UNRECOGNIZED(Int)

  package init() {
    self = .unspecified
  }

  package init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .basic
    case 2: self = .full
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  package var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .basic: return 1
    case .full: return 2
    case .UNRECOGNIZED(let i): return i
    }
  }

  // The compiler won't synthesize support with the UNRECOGNIZED case.
  package static let allCases: [Google_Cloud_Bigquery_Storage_V1_WriteStreamView] = [
    .unspecified,
    .basic,
    .full,
  ]

}

/// Information about the ReadSession.
package struct Google_Cloud_Bigquery_Storage_V1_ReadSession: @unchecked Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. Unique identifier for the session, in the form
  /// `projects/{project_id}/locations/{location}/sessions/{session_id}`.
  package var name: String {
    get {return _storage._name}
    set {_uniqueStorage()._name = newValue}
  }

  /// Output only. Time at which the session becomes invalid. After this time,
  /// subsequent requests to read this Session will return errors. The
  /// expire_time is automatically assigned and currently cannot be specified or
  /// updated.
  package var expireTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._expireTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._expireTime = newValue}
  }
  /// Returns true if `expireTime` has been explicitly set.
  package var hasExpireTime: Bool {return _storage._expireTime != nil}
  /// Clears the value of `expireTime`. Subsequent reads from it will return its default value.
  package mutating func clearExpireTime() {_uniqueStorage()._expireTime = nil}

  /// Immutable. Data format of the output data. DATA_FORMAT_UNSPECIFIED not
  /// supported.
  package var dataFormat: Google_Cloud_Bigquery_Storage_V1_DataFormat {
    get {return _storage._dataFormat}
    set {_uniqueStorage()._dataFormat = newValue}
  }

  /// The schema for the read. If read_options.selected_fields is set, the
  /// schema may be different from the table schema as it will only contain
  /// the selected fields.
  package var schema: OneOf_Schema? {
    get {return _storage._schema}
    set {_uniqueStorage()._schema = newValue}
  }

  /// Output only. Avro schema.
  package var avroSchema: Google_Cloud_Bigquery_Storage_V1_AvroSchema {
    get {
      if case .avroSchema(let v)? = _storage._schema {return v}
      return Google_Cloud_Bigquery_Storage_V1_AvroSchema()
    }
    set {_uniqueStorage()._schema = .avroSchema(newValue)}
  }

  /// Output only. Arrow schema.
  package var arrowSchema: Google_Cloud_Bigquery_Storage_V1_ArrowSchema {
    get {
      if case .arrowSchema(let v)? = _storage._schema {return v}
      return Google_Cloud_Bigquery_Storage_V1_ArrowSchema()
    }
    set {_uniqueStorage()._schema = .arrowSchema(newValue)}
  }

  /// Immutable. Table that this ReadSession is reading from, in the form
  /// `projects/{project_id}/datasets/{dataset_id}/tables/{table_id}`
  package var table: String {
    get {return _storage._table}
    set {_uniqueStorage()._table = newValue}
  }

  /// Optional. Any modifiers which are applied when reading from the specified
  /// table.
  package var tableModifiers: Google_Cloud_Bigquery_Storage_V1_ReadSession.TableModifiers {
    get {return _storage._tableModifiers ?? Google_Cloud_Bigquery_Storage_V1_ReadSession.TableModifiers()}
    set {_uniqueStorage()._tableModifiers = newValue}
  }
  /// Returns true if `tableModifiers` has been explicitly set.
  package var hasTableModifiers: Bool {return _storage._tableModifiers != nil}
  /// Clears the value of `tableModifiers`. Subsequent reads from it will return its default value.
  package mutating func clearTableModifiers() {_uniqueStorage()._tableModifiers = nil}

  /// Optional. Read options for this session (e.g. column selection, filters).
  package var readOptions: Google_Cloud_Bigquery_Storage_V1_ReadSession.TableReadOptions {
    get {return _storage._readOptions ?? Google_Cloud_Bigquery_Storage_V1_ReadSession.TableReadOptions()}
    set {_uniqueStorage()._readOptions = newValue}
  }
  /// Returns true if `readOptions` has been explicitly set.
  package var hasReadOptions: Bool {return _storage._readOptions != nil}
  /// Clears the value of `readOptions`. Subsequent reads from it will return its default value.
  package mutating func clearReadOptions() {_uniqueStorage()._readOptions = nil}

  /// Output only. A list of streams created with the session.
  ///
  /// At least one stream is created with the session. In the future, larger
  /// request_stream_count values *may* result in this list being unpopulated,
  /// in that case, the user will need to use a List method to get the streams
  /// instead, which is not yet available.
  package var streams: [Google_Cloud_Bigquery_Storage_V1_ReadStream] {
    get {return _storage._streams}
    set {_uniqueStorage()._streams = newValue}
  }

  /// Output only. An estimate on the number of bytes this session will scan when
  /// all streams are completely consumed. This estimate is based on
  /// metadata from the table which might be incomplete or stale.
  package var estimatedTotalBytesScanned: Int64 {
    get {return _storage._estimatedTotalBytesScanned}
    set {_uniqueStorage()._estimatedTotalBytesScanned = newValue}
  }

  /// Output only. A pre-projected estimate of the total physical size of files
  /// (in bytes) that this session will scan when all streams are consumed. This
  /// estimate is independent of the selected columns and can be based on
  /// incomplete or stale metadata from the table.  This field is only set for
  /// BigLake tables.
  package var estimatedTotalPhysicalFileSize: Int64 {
    get {return _storage._estimatedTotalPhysicalFileSize}
    set {_uniqueStorage()._estimatedTotalPhysicalFileSize = newValue}
  }

  /// Output only. An estimate on the number of rows present in this session's
  /// streams. This estimate is based on metadata from the table which might be
  /// incomplete or stale.
  package var estimatedRowCount: Int64 {
    get {return _storage._estimatedRowCount}
    set {_uniqueStorage()._estimatedRowCount = newValue}
  }

  /// Optional. ID set by client to annotate a session identity.  This does not
  /// need to be strictly unique, but instead the same ID should be used to group
  /// logically connected sessions (e.g. All using the same ID for all sessions
  /// needed to complete a Spark SQL query is reasonable).
  ///
  /// Maximum length is 256 bytes.
  package var traceID: String {
    get {return _storage._traceID}
    set {_uniqueStorage()._traceID = newValue}
  }

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The schema for the read. If read_options.selected_fields is set, the
  /// schema may be different from the table schema as it will only contain
  /// the selected fields.
  package enum OneOf_Schema: Equatable, Sendable {
    /// Output only. Avro schema.
    case avroSchema(Google_Cloud_Bigquery_Storage_V1_AvroSchema)
    /// Output only. Arrow schema.
    case arrowSchema(Google_Cloud_Bigquery_Storage_V1_ArrowSchema)

  }

  /// Additional attributes when reading a table.
  package struct TableModifiers: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// The snapshot time of the table. If not set, interpreted as now.
    package var snapshotTime: SwiftProtobuf.Google_Protobuf_Timestamp {
      get {return _snapshotTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
      set {_snapshotTime = newValue}
    }
    /// Returns true if `snapshotTime` has been explicitly set.
    package var hasSnapshotTime: Bool {return self._snapshotTime != nil}
    /// Clears the value of `snapshotTime`. Subsequent reads from it will return its default value.
    package mutating func clearSnapshotTime() {self._snapshotTime = nil}

    package var unknownFields = SwiftProtobuf.UnknownStorage()

    package init() {}

    fileprivate var _snapshotTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
  }

  /// Options dictating how we read a table.
  package struct TableReadOptions: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Optional. The names of the fields in the table to be returned. If no
    /// field names are specified, then all fields in the table are returned.
    ///
    /// Nested fields -- the child elements of a STRUCT field -- can be selected
    /// individually using their fully-qualified names, and will be returned as
    /// record fields containing only the selected nested fields. If a STRUCT
    /// field is specified in the selected fields list, all of the child elements
    /// will be returned.
    ///
    /// As an example, consider a table with the following schema:
    ///
    ///   {
    ///       "name": "struct_field",
    ///       "type": "RECORD",
    ///       "mode": "NULLABLE",
    ///       "fields": [
    ///           {
    ///               "name": "string_field1",
    ///               "type": "STRING",
    /// .              "mode": "NULLABLE"
    ///           },
    ///           {
    ///               "name": "string_field2",
    ///               "type": "STRING",
    ///               "mode": "NULLABLE"
    ///           }
    ///       ]
    ///   }
    ///
    /// Specifying "struct_field" in the selected fields list will result in a
    /// read session schema with the following logical structure:
    ///
    ///   struct_field {
    ///       string_field1
    ///       string_field2
    ///   }
    ///
    /// Specifying "struct_field.string_field1" in the selected fields list will
    /// result in a read session schema with the following logical structure:
    ///
    ///   struct_field {
    ///       string_field1
    ///   }
    ///
    /// The order of the fields in the read session schema is derived from the
    /// table schema and does not correspond to the order in which the fields are
    /// specified in this list.
    package var selectedFields: [String] = []

    /// SQL text filtering statement, similar to a WHERE clause in a query.
    /// Aggregates are not supported.
    ///
    /// Examples: "int_field > 5"
    ///           "date_field = CAST('2014-9-27' as DATE)"
    ///           "nullable_field is not NULL"
    ///           "st_equals(geo_field, st_geofromtext("POINT(2, 2)"))"
    ///           "numeric_field BETWEEN 1.0 AND 5.0"
    ///
    /// Restricted to a maximum length for 1 MB.
    package var rowRestriction: String = String()

    package var outputFormatSerializationOptions: Google_Cloud_Bigquery_Storage_V1_ReadSession.TableReadOptions.OneOf_OutputFormatSerializationOptions? = nil

    /// Optional. Options specific to the Apache Arrow output format.
    package var arrowSerializationOptions: Google_Cloud_Bigquery_Storage_V1_ArrowSerializationOptions {
      get {
        if case .arrowSerializationOptions(let v)? = outputFormatSerializationOptions {return v}
        return Google_Cloud_Bigquery_Storage_V1_ArrowSerializationOptions()
      }
      set {outputFormatSerializationOptions = .arrowSerializationOptions(newValue)}
    }

    /// Optional. Options specific to the Apache Avro output format
    package var avroSerializationOptions: Google_Cloud_Bigquery_Storage_V1_AvroSerializationOptions {
      get {
        if case .avroSerializationOptions(let v)? = outputFormatSerializationOptions {return v}
        return Google_Cloud_Bigquery_Storage_V1_AvroSerializationOptions()
      }
      set {outputFormatSerializationOptions = .avroSerializationOptions(newValue)}
    }

    /// Optional. Specifies a table sampling percentage. Specifically, the query
    /// planner will use TABLESAMPLE SYSTEM (sample_percentage PERCENT). The
    /// sampling percentage is applied at the data block granularity. It will
    /// randomly choose for each data block whether to read the rows in that data
    /// block. For more details, see
    /// https://cloud.google.com/bigquery/docs/table-sampling)
    package var samplePercentage: Double {
      get {return _samplePercentage ?? 0}
      set {_samplePercentage = newValue}
    }
    /// Returns true if `samplePercentage` has been explicitly set.
    package var hasSamplePercentage: Bool {return self._samplePercentage != nil}
    /// Clears the value of `samplePercentage`. Subsequent reads from it will return its default value.
    package mutating func clearSamplePercentage() {self._samplePercentage = nil}

    /// Optional. Set response_compression_codec when creating a read session to
    /// enable application-level compression of ReadRows responses.
    package var responseCompressionCodec: Google_Cloud_Bigquery_Storage_V1_ReadSession.TableReadOptions.ResponseCompressionCodec {
      get {return _responseCompressionCodec ?? .unspecified}
      set {_responseCompressionCodec = newValue}
    }
    /// Returns true if `responseCompressionCodec` has been explicitly set.
    package var hasResponseCompressionCodec: Bool {return self._responseCompressionCodec != nil}
    /// Clears the value of `responseCompressionCodec`. Subsequent reads from it will return its default value.
    package mutating func clearResponseCompressionCodec() {self._responseCompressionCodec = nil}

    package var unknownFields = SwiftProtobuf.UnknownStorage()

    package enum OneOf_OutputFormatSerializationOptions: Equatable, Sendable {
      /// Optional. Options specific to the Apache Arrow output format.
      case arrowSerializationOptions(Google_Cloud_Bigquery_Storage_V1_ArrowSerializationOptions)
      /// Optional. Options specific to the Apache Avro output format
      case avroSerializationOptions(Google_Cloud_Bigquery_Storage_V1_AvroSerializationOptions)

    }

    /// Specifies which compression codec to attempt on the entire serialized
    /// response payload (either Arrow record batch or Avro rows). This is
    /// not to be confused with the Apache Arrow native compression codecs
    /// specified in ArrowSerializationOptions. For performance reasons, when
    /// creating a read session requesting Arrow responses, setting both native
    /// Arrow compression and application-level response compression will not be
    /// allowed - choose, at most, one kind of compression.
    package enum ResponseCompressionCodec: SwiftProtobuf.Enum, Swift.CaseIterable {
      package typealias RawValue = Int

      /// Default is no compression.
      case unspecified // = 0

      /// Use raw LZ4 compression.
      case lz4 // = 2
      case UNRECOGNIZED(Int)

      package init() {
        self = .unspecified
      }

      package init?(rawValue: Int) {
        switch rawValue {
        case 0: self = .unspecified
        case 2: self = .lz4
        default: self = .UNRECOGNIZED(rawValue)
        }
      }

      package var rawValue: Int {
        switch self {
        case .unspecified: return 0
        case .lz4: return 2
        case .UNRECOGNIZED(let i): return i
        }
      }

      // The compiler won't synthesize support with the UNRECOGNIZED case.
      package static let allCases: [Google_Cloud_Bigquery_Storage_V1_ReadSession.TableReadOptions.ResponseCompressionCodec] = [
        .unspecified,
        .lz4,
      ]

    }

    package init() {}

    fileprivate var _samplePercentage: Double? = nil
    fileprivate var _responseCompressionCodec: Google_Cloud_Bigquery_Storage_V1_ReadSession.TableReadOptions.ResponseCompressionCodec? = nil
  }

  package init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// Information about a single stream that gets data out of the storage system.
/// Most of the information about `ReadStream` instances is aggregated, making
/// `ReadStream` lightweight.
package struct Google_Cloud_Bigquery_Storage_V1_ReadStream: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. Name of the stream, in the form
  /// `projects/{project_id}/locations/{location}/sessions/{session_id}/streams/{stream_id}`.
  package var name: String = String()

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}
}

/// Information about a single stream that gets data inside the storage system.
package struct Google_Cloud_Bigquery_Storage_V1_WriteStream: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. Name of the stream, in the form
  /// `projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}`.
  package var name: String = String()

  /// Immutable. Type of the stream.
  package var type: Google_Cloud_Bigquery_Storage_V1_WriteStream.TypeEnum = .unspecified

  /// Output only. Create time of the stream. For the _default stream, this is
  /// the creation_time of the table.
  package var createTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _createTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_createTime = newValue}
  }
  /// Returns true if `createTime` has been explicitly set.
  package var hasCreateTime: Bool {return self._createTime != nil}
  /// Clears the value of `createTime`. Subsequent reads from it will return its default value.
  package mutating func clearCreateTime() {self._createTime = nil}

  /// Output only. Commit time of the stream.
  /// If a stream is of `COMMITTED` type, then it will have a commit_time same as
  /// `create_time`. If the stream is of `PENDING` type, empty commit_time
  /// means it is not committed.
  package var commitTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _commitTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_commitTime = newValue}
  }
  /// Returns true if `commitTime` has been explicitly set.
  package var hasCommitTime: Bool {return self._commitTime != nil}
  /// Clears the value of `commitTime`. Subsequent reads from it will return its default value.
  package mutating func clearCommitTime() {self._commitTime = nil}

  /// Output only. The schema of the destination table. It is only returned in
  /// `CreateWriteStream` response. Caller should generate data that's
  /// compatible with this schema to send in initial `AppendRowsRequest`.
  /// The table schema could go out of date during the life time of the stream.
  package var tableSchema: Google_Cloud_Bigquery_Storage_V1_TableSchema {
    get {return _tableSchema ?? Google_Cloud_Bigquery_Storage_V1_TableSchema()}
    set {_tableSchema = newValue}
  }
  /// Returns true if `tableSchema` has been explicitly set.
  package var hasTableSchema: Bool {return self._tableSchema != nil}
  /// Clears the value of `tableSchema`. Subsequent reads from it will return its default value.
  package mutating func clearTableSchema() {self._tableSchema = nil}

  /// Immutable. Mode of the stream.
  package var writeMode: Google_Cloud_Bigquery_Storage_V1_WriteStream.WriteMode = .unspecified

  /// Immutable. The geographic location where the stream's dataset resides. See
  /// https://cloud.google.com/bigquery/docs/locations for supported
  /// locations.
  package var location: String = String()

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Type enum of the stream.
  package enum TypeEnum: SwiftProtobuf.Enum, Swift.CaseIterable {
    package typealias RawValue = Int

    /// Unknown type.
    case unspecified // = 0

    /// Data will commit automatically and appear as soon as the write is
    /// acknowledged.
    case committed // = 1

    /// Data is invisible until the stream is committed.
    case pending // = 2

    /// Data is only visible up to the offset to which it was flushed.
    case buffered // = 3
    case UNRECOGNIZED(Int)

    package init() {
      self = .unspecified
    }

    package init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .committed
      case 2: self = .pending
      case 3: self = .buffered
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    package var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .committed: return 1
      case .pending: return 2
      case .buffered: return 3
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    package static let allCases: [Google_Cloud_Bigquery_Storage_V1_WriteStream.TypeEnum] = [
      .unspecified,
      .committed,
      .pending,
      .buffered,
    ]

  }

  /// Mode enum of the stream.
  package enum WriteMode: SwiftProtobuf.Enum, Swift.CaseIterable {
    package typealias RawValue = Int

    /// Unknown type.
    case unspecified // = 0

    /// Insert new records into the table.
    /// It is the default value if customers do not specify it.
    case insert // = 1
    case UNRECOGNIZED(Int)

    package init() {
      self = .unspecified
    }

    package init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .insert
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    package var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .insert: return 1
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    package static let allCases: [Google_Cloud_Bigquery_Storage_V1_WriteStream.WriteMode] = [
      .unspecified,
      .insert,
    ]

  }

  package init() {}

  fileprivate var _createTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
  fileprivate var _commitTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
  fileprivate var _tableSchema: Google_Cloud_Bigquery_Storage_V1_TableSchema? = nil
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.bigquery.storage.v1"

extension Google_Cloud_Bigquery_Storage_V1_DataFormat: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "DATA_FORMAT_UNSPECIFIED"),
    1: .same(proto: "AVRO"),
    2: .same(proto: "ARROW"),
  ]
}

extension Google_Cloud_Bigquery_Storage_V1_WriteStreamView: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "WRITE_STREAM_VIEW_UNSPECIFIED"),
    1: .same(proto: "BASIC"),
    2: .same(proto: "FULL"),
  ]
}

extension Google_Cloud_Bigquery_Storage_V1_ReadSession: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".ReadSession"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .standard(proto: "expire_time"),
    3: .standard(proto: "data_format"),
    4: .standard(proto: "avro_schema"),
    5: .standard(proto: "arrow_schema"),
    6: .same(proto: "table"),
    7: .standard(proto: "table_modifiers"),
    8: .standard(proto: "read_options"),
    10: .same(proto: "streams"),
    12: .standard(proto: "estimated_total_bytes_scanned"),
    15: .standard(proto: "estimated_total_physical_file_size"),
    14: .standard(proto: "estimated_row_count"),
    13: .standard(proto: "trace_id"),
  ]

  fileprivate class _StorageClass {
    var _name: String = String()
    var _expireTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _dataFormat: Google_Cloud_Bigquery_Storage_V1_DataFormat = .unspecified
    var _schema: Google_Cloud_Bigquery_Storage_V1_ReadSession.OneOf_Schema?
    var _table: String = String()
    var _tableModifiers: Google_Cloud_Bigquery_Storage_V1_ReadSession.TableModifiers? = nil
    var _readOptions: Google_Cloud_Bigquery_Storage_V1_ReadSession.TableReadOptions? = nil
    var _streams: [Google_Cloud_Bigquery_Storage_V1_ReadStream] = []
    var _estimatedTotalBytesScanned: Int64 = 0
    var _estimatedTotalPhysicalFileSize: Int64 = 0
    var _estimatedRowCount: Int64 = 0
    var _traceID: String = String()

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _name = source._name
      _expireTime = source._expireTime
      _dataFormat = source._dataFormat
      _schema = source._schema
      _table = source._table
      _tableModifiers = source._tableModifiers
      _readOptions = source._readOptions
      _streams = source._streams
      _estimatedTotalBytesScanned = source._estimatedTotalBytesScanned
      _estimatedTotalPhysicalFileSize = source._estimatedTotalPhysicalFileSize
      _estimatedRowCount = source._estimatedRowCount
      _traceID = source._traceID
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularStringField(value: &_storage._name) }()
        case 2: try { try decoder.decodeSingularMessageField(value: &_storage._expireTime) }()
        case 3: try { try decoder.decodeSingularEnumField(value: &_storage._dataFormat) }()
        case 4: try {
          var v: Google_Cloud_Bigquery_Storage_V1_AvroSchema?
          var hadOneofValue = false
          if let current = _storage._schema {
            hadOneofValue = true
            if case .avroSchema(let m) = current {v = m}
          }
          try decoder.decodeSingularMessageField(value: &v)
          if let v = v {
            if hadOneofValue {try decoder.handleConflictingOneOf()}
            _storage._schema = .avroSchema(v)
          }
        }()
        case 5: try {
          var v: Google_Cloud_Bigquery_Storage_V1_ArrowSchema?
          var hadOneofValue = false
          if let current = _storage._schema {
            hadOneofValue = true
            if case .arrowSchema(let m) = current {v = m}
          }
          try decoder.decodeSingularMessageField(value: &v)
          if let v = v {
            if hadOneofValue {try decoder.handleConflictingOneOf()}
            _storage._schema = .arrowSchema(v)
          }
        }()
        case 6: try { try decoder.decodeSingularStringField(value: &_storage._table) }()
        case 7: try { try decoder.decodeSingularMessageField(value: &_storage._tableModifiers) }()
        case 8: try { try decoder.decodeSingularMessageField(value: &_storage._readOptions) }()
        case 10: try { try decoder.decodeRepeatedMessageField(value: &_storage._streams) }()
        case 12: try { try decoder.decodeSingularInt64Field(value: &_storage._estimatedTotalBytesScanned) }()
        case 13: try { try decoder.decodeSingularStringField(value: &_storage._traceID) }()
        case 14: try { try decoder.decodeSingularInt64Field(value: &_storage._estimatedRowCount) }()
        case 15: try { try decoder.decodeSingularInt64Field(value: &_storage._estimatedTotalPhysicalFileSize) }()
        default: break
        }
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      if !_storage._name.isEmpty {
        try visitor.visitSingularStringField(value: _storage._name, fieldNumber: 1)
      }
      try { if let v = _storage._expireTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      } }()
      if _storage._dataFormat != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._dataFormat, fieldNumber: 3)
      }
      switch _storage._schema {
      case .avroSchema?: try {
        guard case .avroSchema(let v)? = _storage._schema else { preconditionFailure() }
        try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
      }()
      case .arrowSchema?: try {
        guard case .arrowSchema(let v)? = _storage._schema else { preconditionFailure() }
        try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
      }()
      case nil: break
      }
      if !_storage._table.isEmpty {
        try visitor.visitSingularStringField(value: _storage._table, fieldNumber: 6)
      }
      try { if let v = _storage._tableModifiers {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
      } }()
      try { if let v = _storage._readOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
      } }()
      if !_storage._streams.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._streams, fieldNumber: 10)
      }
      if _storage._estimatedTotalBytesScanned != 0 {
        try visitor.visitSingularInt64Field(value: _storage._estimatedTotalBytesScanned, fieldNumber: 12)
      }
      if !_storage._traceID.isEmpty {
        try visitor.visitSingularStringField(value: _storage._traceID, fieldNumber: 13)
      }
      if _storage._estimatedRowCount != 0 {
        try visitor.visitSingularInt64Field(value: _storage._estimatedRowCount, fieldNumber: 14)
      }
      if _storage._estimatedTotalPhysicalFileSize != 0 {
        try visitor.visitSingularInt64Field(value: _storage._estimatedTotalPhysicalFileSize, fieldNumber: 15)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_ReadSession, rhs: Google_Cloud_Bigquery_Storage_V1_ReadSession) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._name != rhs_storage._name {return false}
        if _storage._expireTime != rhs_storage._expireTime {return false}
        if _storage._dataFormat != rhs_storage._dataFormat {return false}
        if _storage._schema != rhs_storage._schema {return false}
        if _storage._table != rhs_storage._table {return false}
        if _storage._tableModifiers != rhs_storage._tableModifiers {return false}
        if _storage._readOptions != rhs_storage._readOptions {return false}
        if _storage._streams != rhs_storage._streams {return false}
        if _storage._estimatedTotalBytesScanned != rhs_storage._estimatedTotalBytesScanned {return false}
        if _storage._estimatedTotalPhysicalFileSize != rhs_storage._estimatedTotalPhysicalFileSize {return false}
        if _storage._estimatedRowCount != rhs_storage._estimatedRowCount {return false}
        if _storage._traceID != rhs_storage._traceID {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_ReadSession.TableModifiers: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = Google_Cloud_Bigquery_Storage_V1_ReadSession.protoMessageName + ".TableModifiers"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "snapshot_time"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._snapshotTime) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._snapshotTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_ReadSession.TableModifiers, rhs: Google_Cloud_Bigquery_Storage_V1_ReadSession.TableModifiers) -> Bool {
    if lhs._snapshotTime != rhs._snapshotTime {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_ReadSession.TableReadOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = Google_Cloud_Bigquery_Storage_V1_ReadSession.protoMessageName + ".TableReadOptions"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "selected_fields"),
    2: .standard(proto: "row_restriction"),
    3: .standard(proto: "arrow_serialization_options"),
    4: .standard(proto: "avro_serialization_options"),
    5: .standard(proto: "sample_percentage"),
    6: .standard(proto: "response_compression_codec"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedStringField(value: &self.selectedFields) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.rowRestriction) }()
      case 3: try {
        var v: Google_Cloud_Bigquery_Storage_V1_ArrowSerializationOptions?
        var hadOneofValue = false
        if let current = self.outputFormatSerializationOptions {
          hadOneofValue = true
          if case .arrowSerializationOptions(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.outputFormatSerializationOptions = .arrowSerializationOptions(v)
        }
      }()
      case 4: try {
        var v: Google_Cloud_Bigquery_Storage_V1_AvroSerializationOptions?
        var hadOneofValue = false
        if let current = self.outputFormatSerializationOptions {
          hadOneofValue = true
          if case .avroSerializationOptions(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.outputFormatSerializationOptions = .avroSerializationOptions(v)
        }
      }()
      case 5: try { try decoder.decodeSingularDoubleField(value: &self._samplePercentage) }()
      case 6: try { try decoder.decodeSingularEnumField(value: &self._responseCompressionCodec) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.selectedFields.isEmpty {
      try visitor.visitRepeatedStringField(value: self.selectedFields, fieldNumber: 1)
    }
    if !self.rowRestriction.isEmpty {
      try visitor.visitSingularStringField(value: self.rowRestriction, fieldNumber: 2)
    }
    switch self.outputFormatSerializationOptions {
    case .arrowSerializationOptions?: try {
      guard case .arrowSerializationOptions(let v)? = self.outputFormatSerializationOptions else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }()
    case .avroSerializationOptions?: try {
      guard case .avroSerializationOptions(let v)? = self.outputFormatSerializationOptions else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }()
    case nil: break
    }
    try { if let v = self._samplePercentage {
      try visitor.visitSingularDoubleField(value: v, fieldNumber: 5)
    } }()
    try { if let v = self._responseCompressionCodec {
      try visitor.visitSingularEnumField(value: v, fieldNumber: 6)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_ReadSession.TableReadOptions, rhs: Google_Cloud_Bigquery_Storage_V1_ReadSession.TableReadOptions) -> Bool {
    if lhs.selectedFields != rhs.selectedFields {return false}
    if lhs.rowRestriction != rhs.rowRestriction {return false}
    if lhs.outputFormatSerializationOptions != rhs.outputFormatSerializationOptions {return false}
    if lhs._samplePercentage != rhs._samplePercentage {return false}
    if lhs._responseCompressionCodec != rhs._responseCompressionCodec {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_ReadSession.TableReadOptions.ResponseCompressionCodec: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "RESPONSE_COMPRESSION_CODEC_UNSPECIFIED"),
    2: .same(proto: "RESPONSE_COMPRESSION_CODEC_LZ4"),
  ]
}

extension Google_Cloud_Bigquery_Storage_V1_ReadStream: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".ReadStream"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_ReadStream, rhs: Google_Cloud_Bigquery_Storage_V1_ReadStream) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_WriteStream: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".WriteStream"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .same(proto: "type"),
    3: .standard(proto: "create_time"),
    4: .standard(proto: "commit_time"),
    5: .standard(proto: "table_schema"),
    7: .standard(proto: "write_mode"),
    8: .same(proto: "location"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 2: try { try decoder.decodeSingularEnumField(value: &self.type) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._createTime) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._commitTime) }()
      case 5: try { try decoder.decodeSingularMessageField(value: &self._tableSchema) }()
      case 7: try { try decoder.decodeSingularEnumField(value: &self.writeMode) }()
      case 8: try { try decoder.decodeSingularStringField(value: &self.location) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if self.type != .unspecified {
      try visitor.visitSingularEnumField(value: self.type, fieldNumber: 2)
    }
    try { if let v = self._createTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    } }()
    try { if let v = self._commitTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    } }()
    try { if let v = self._tableSchema {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
    } }()
    if self.writeMode != .unspecified {
      try visitor.visitSingularEnumField(value: self.writeMode, fieldNumber: 7)
    }
    if !self.location.isEmpty {
      try visitor.visitSingularStringField(value: self.location, fieldNumber: 8)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_Storage_V1_WriteStream, rhs: Google_Cloud_Bigquery_Storage_V1_WriteStream) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs.type != rhs.type {return false}
    if lhs._createTime != rhs._createTime {return false}
    if lhs._commitTime != rhs._commitTime {return false}
    if lhs._tableSchema != rhs._tableSchema {return false}
    if lhs.writeMode != rhs.writeMode {return false}
    if lhs.location != rhs.location {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_Storage_V1_WriteStream.TypeEnum: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "TYPE_UNSPECIFIED"),
    1: .same(proto: "COMMITTED"),
    2: .same(proto: "PENDING"),
    3: .same(proto: "BUFFERED"),
  ]
}

extension Google_Cloud_Bigquery_Storage_V1_WriteStream.WriteMode: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "WRITE_MODE_UNSPECIFIED"),
    1: .same(proto: "INSERT"),
  ]
}
