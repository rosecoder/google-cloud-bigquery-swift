// DO NOT EDIT.
// swift-format-ignore-file
// swiftlint:disable all
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/bigquery/v2/external_data_config.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// Options for external data sources.
package struct Google_Cloud_Bigquery_V2_AvroOptions: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. If sourceFormat is set to "AVRO", indicates whether to interpret
  /// logical types as the corresponding BigQuery data type (for example,
  /// TIMESTAMP), instead of using the raw type (for example, INTEGER).
  package var useAvroLogicalTypes: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _useAvroLogicalTypes ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_useAvroLogicalTypes = newValue}
  }
  /// Returns true if `useAvroLogicalTypes` has been explicitly set.
  package var hasUseAvroLogicalTypes: Bool {return self._useAvroLogicalTypes != nil}
  /// Clears the value of `useAvroLogicalTypes`. Subsequent reads from it will return its default value.
  package mutating func clearUseAvroLogicalTypes() {self._useAvroLogicalTypes = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _useAvroLogicalTypes: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
}

/// Parquet Options for load and make external tables.
package struct Google_Cloud_Bigquery_V2_ParquetOptions: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. Indicates whether to infer Parquet ENUM logical type as STRING
  /// instead of BYTES by default.
  package var enumAsString: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _enumAsString ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_enumAsString = newValue}
  }
  /// Returns true if `enumAsString` has been explicitly set.
  package var hasEnumAsString: Bool {return self._enumAsString != nil}
  /// Clears the value of `enumAsString`. Subsequent reads from it will return its default value.
  package mutating func clearEnumAsString() {self._enumAsString = nil}

  /// Optional. Indicates whether to use schema inference specifically for
  /// Parquet LIST logical type.
  package var enableListInference: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _enableListInference ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_enableListInference = newValue}
  }
  /// Returns true if `enableListInference` has been explicitly set.
  package var hasEnableListInference: Bool {return self._enableListInference != nil}
  /// Clears the value of `enableListInference`. Subsequent reads from it will return its default value.
  package mutating func clearEnableListInference() {self._enableListInference = nil}

  /// Optional. Indicates how to represent a Parquet map if present.
  package var mapTargetType: Google_Cloud_Bigquery_V2_MapTargetType = .unspecified

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _enumAsString: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
  fileprivate var _enableListInference: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
}

/// Information related to a CSV data source.
package struct Google_Cloud_Bigquery_V2_CsvOptions: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. The separator character for fields in a CSV file. The separator
  /// is interpreted as a single byte. For files encoded in ISO-8859-1, any
  /// single character can be used as a separator. For files encoded in UTF-8,
  /// characters represented in decimal range 1-127 (U+0001-U+007F) can be used
  /// without any modification. UTF-8 characters encoded with multiple bytes
  /// (i.e. U+0080 and above) will have only the first byte used for separating
  /// fields. The remaining bytes will be treated as a part of the field.
  /// BigQuery also supports the escape sequence "\t" (U+0009) to specify a tab
  /// separator. The default value is comma (",", U+002C).
  package var fieldDelimiter: String = String()

  /// Optional. The number of rows at the top of a CSV file that BigQuery will
  /// skip when reading the data. The default value is 0. This property is
  /// useful if you have header rows in the file that should be skipped.
  /// When autodetect is on, the behavior is the following:
  ///
  /// * skipLeadingRows unspecified - Autodetect tries to detect headers in the
  ///   first row. If they are not detected, the row is read as data. Otherwise
  ///   data is read starting from the second row.
  /// * skipLeadingRows is 0 - Instructs autodetect that there are no headers and
  ///   data should be read starting from the first row.
  /// * skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect
  ///   headers in row N. If headers are not detected, row N is just skipped.
  ///   Otherwise row N is used to extract column names for the detected schema.
  package var skipLeadingRows: SwiftProtobuf.Google_Protobuf_Int64Value {
    get {return _skipLeadingRows ?? SwiftProtobuf.Google_Protobuf_Int64Value()}
    set {_skipLeadingRows = newValue}
  }
  /// Returns true if `skipLeadingRows` has been explicitly set.
  package var hasSkipLeadingRows: Bool {return self._skipLeadingRows != nil}
  /// Clears the value of `skipLeadingRows`. Subsequent reads from it will return its default value.
  package mutating func clearSkipLeadingRows() {self._skipLeadingRows = nil}

  /// Optional. The value that is used to quote data sections in a CSV file.
  /// BigQuery converts the string to ISO-8859-1 encoding, and then uses the
  /// first byte of the encoded string to split the data in its raw, binary
  /// state.
  /// The default value is a double-quote (").
  /// If your data does not contain quoted sections,
  /// set the property value to an empty string.
  /// If your data contains quoted newline characters, you must also set the
  /// allowQuotedNewlines property to true.
  /// To include the specific quote character within a quoted value, precede it
  /// with an additional matching quote character. For example, if you want to
  /// escape the default character  ' " ', use ' "" '.
  package var quote: SwiftProtobuf.Google_Protobuf_StringValue {
    get {return _quote ?? SwiftProtobuf.Google_Protobuf_StringValue()}
    set {_quote = newValue}
  }
  /// Returns true if `quote` has been explicitly set.
  package var hasQuote: Bool {return self._quote != nil}
  /// Clears the value of `quote`. Subsequent reads from it will return its default value.
  package mutating func clearQuote() {self._quote = nil}

  /// Optional. Indicates if BigQuery should allow quoted data sections that
  /// contain newline characters in a CSV file. The default value is false.
  package var allowQuotedNewlines: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _allowQuotedNewlines ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_allowQuotedNewlines = newValue}
  }
  /// Returns true if `allowQuotedNewlines` has been explicitly set.
  package var hasAllowQuotedNewlines: Bool {return self._allowQuotedNewlines != nil}
  /// Clears the value of `allowQuotedNewlines`. Subsequent reads from it will return its default value.
  package mutating func clearAllowQuotedNewlines() {self._allowQuotedNewlines = nil}

  /// Optional. Indicates if BigQuery should accept rows that are missing
  /// trailing optional columns. If true, BigQuery treats missing trailing
  /// columns as null values.
  /// If false, records with missing trailing columns are treated as bad records,
  /// and if there are too many bad records, an invalid error is returned in the
  /// job result. The default value is false.
  package var allowJaggedRows: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _allowJaggedRows ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_allowJaggedRows = newValue}
  }
  /// Returns true if `allowJaggedRows` has been explicitly set.
  package var hasAllowJaggedRows: Bool {return self._allowJaggedRows != nil}
  /// Clears the value of `allowJaggedRows`. Subsequent reads from it will return its default value.
  package mutating func clearAllowJaggedRows() {self._allowJaggedRows = nil}

  /// Optional. The character encoding of the data.
  /// The supported values are UTF-8, ISO-8859-1, UTF-16BE, UTF-16LE, UTF-32BE,
  /// and UTF-32LE.  The default value is UTF-8.
  /// BigQuery decodes the data after the raw, binary data has been split using
  /// the values of the quote and fieldDelimiter properties.
  package var encoding: String = String()

  /// Optional. Indicates if the embedded ASCII control characters (the first 32
  /// characters in the ASCII-table, from '\x00' to '\x1F') are preserved.
  package var preserveAsciiControlCharacters: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _preserveAsciiControlCharacters ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_preserveAsciiControlCharacters = newValue}
  }
  /// Returns true if `preserveAsciiControlCharacters` has been explicitly set.
  package var hasPreserveAsciiControlCharacters: Bool {return self._preserveAsciiControlCharacters != nil}
  /// Clears the value of `preserveAsciiControlCharacters`. Subsequent reads from it will return its default value.
  package mutating func clearPreserveAsciiControlCharacters() {self._preserveAsciiControlCharacters = nil}

  /// Optional. Specifies a string that represents a null value in a CSV file.
  /// For example, if you specify "\N", BigQuery interprets "\N" as a null value
  /// when querying a CSV file.
  /// The default value is the empty string. If you set this property to a custom
  /// value, BigQuery throws an error if an empty string is present for all data
  /// types except for STRING and BYTE. For STRING and BYTE columns, BigQuery
  /// interprets the empty string as an empty value.
  package var nullMarker: SwiftProtobuf.Google_Protobuf_StringValue {
    get {return _nullMarker ?? SwiftProtobuf.Google_Protobuf_StringValue()}
    set {_nullMarker = newValue}
  }
  /// Returns true if `nullMarker` has been explicitly set.
  package var hasNullMarker: Bool {return self._nullMarker != nil}
  /// Clears the value of `nullMarker`. Subsequent reads from it will return its default value.
  package mutating func clearNullMarker() {self._nullMarker = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _skipLeadingRows: SwiftProtobuf.Google_Protobuf_Int64Value? = nil
  fileprivate var _quote: SwiftProtobuf.Google_Protobuf_StringValue? = nil
  fileprivate var _allowQuotedNewlines: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
  fileprivate var _allowJaggedRows: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
  fileprivate var _preserveAsciiControlCharacters: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
  fileprivate var _nullMarker: SwiftProtobuf.Google_Protobuf_StringValue? = nil
}

/// Json Options for load and make external tables.
package struct Google_Cloud_Bigquery_V2_JsonOptions: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. The character encoding of the data.
  /// The supported values are UTF-8, UTF-16BE, UTF-16LE, UTF-32BE,
  /// and UTF-32LE.  The default value is UTF-8.
  package var encoding: String = String()

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}
}

/// Information related to a Bigtable column.
package struct Google_Cloud_Bigquery_V2_BigtableColumn: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// [Required] Qualifier of the column.
  /// Columns in the parent column family that has this exact qualifier are
  /// exposed as `<family field name>.<column field name>` field.
  /// If the qualifier is valid UTF-8 string, it can be specified in the
  /// qualifier_string field.  Otherwise, a base-64 encoded value must be set to
  /// qualifier_encoded.
  /// The column field name is the same as the column qualifier. However, if the
  /// qualifier is not a valid BigQuery field identifier i.e. does not match
  /// [a-zA-Z][a-zA-Z0-9_]*, a valid identifier must be provided as field_name.
  package var qualifierEncoded: SwiftProtobuf.Google_Protobuf_BytesValue {
    get {return _qualifierEncoded ?? SwiftProtobuf.Google_Protobuf_BytesValue()}
    set {_qualifierEncoded = newValue}
  }
  /// Returns true if `qualifierEncoded` has been explicitly set.
  package var hasQualifierEncoded: Bool {return self._qualifierEncoded != nil}
  /// Clears the value of `qualifierEncoded`. Subsequent reads from it will return its default value.
  package mutating func clearQualifierEncoded() {self._qualifierEncoded = nil}

  /// Qualifier string.
  package var qualifierString: SwiftProtobuf.Google_Protobuf_StringValue {
    get {return _qualifierString ?? SwiftProtobuf.Google_Protobuf_StringValue()}
    set {_qualifierString = newValue}
  }
  /// Returns true if `qualifierString` has been explicitly set.
  package var hasQualifierString: Bool {return self._qualifierString != nil}
  /// Clears the value of `qualifierString`. Subsequent reads from it will return its default value.
  package mutating func clearQualifierString() {self._qualifierString = nil}

  /// Optional. If the qualifier is not a valid BigQuery field identifier i.e.
  /// does not match [a-zA-Z][a-zA-Z0-9_]*,  a valid identifier must be provided
  /// as the column field name and is used as field name in queries.
  package var fieldName: String = String()

  /// Optional. The type to convert the value in cells of this column.
  /// The values are expected to be encoded using HBase Bytes.toBytes function
  /// when using the BINARY encoding value.
  /// Following BigQuery types are allowed (case-sensitive):
  ///
  /// * BYTES
  /// * STRING
  /// * INTEGER
  /// * FLOAT
  /// * BOOLEAN
  /// * JSON
  ///
  /// Default type is BYTES.
  /// 'type' can also be set at the column family level. However, the setting at
  /// this level takes precedence if 'type' is set at both levels.
  package var type: String = String()

  /// Optional. The encoding of the values when the type is not STRING.
  /// Acceptable encoding values are:
  ///   TEXT - indicates values are alphanumeric text strings.
  ///   BINARY - indicates values are encoded using HBase Bytes.toBytes family of
  ///            functions.
  /// 'encoding' can also be set at the column family level. However, the setting
  /// at this level takes precedence if 'encoding' is set at both levels.
  package var encoding: String = String()

  /// Optional. If this is set, only the latest version of value in this column
  ///             are exposed.
  /// 'onlyReadLatest' can also be set at the column family level. However, the
  /// setting at this level takes precedence if 'onlyReadLatest' is set at both
  /// levels.
  package var onlyReadLatest: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _onlyReadLatest ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_onlyReadLatest = newValue}
  }
  /// Returns true if `onlyReadLatest` has been explicitly set.
  package var hasOnlyReadLatest: Bool {return self._onlyReadLatest != nil}
  /// Clears the value of `onlyReadLatest`. Subsequent reads from it will return its default value.
  package mutating func clearOnlyReadLatest() {self._onlyReadLatest = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _qualifierEncoded: SwiftProtobuf.Google_Protobuf_BytesValue? = nil
  fileprivate var _qualifierString: SwiftProtobuf.Google_Protobuf_StringValue? = nil
  fileprivate var _onlyReadLatest: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
}

/// Information related to a Bigtable column family.
package struct Google_Cloud_Bigquery_V2_BigtableColumnFamily: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Identifier of the column family.
  package var familyID: String = String()

  /// Optional. The type to convert the value in cells of this column family.
  /// The values are expected to be encoded using HBase Bytes.toBytes function
  /// when using the BINARY encoding value.
  /// Following BigQuery types are allowed (case-sensitive):
  ///
  /// * BYTES
  /// * STRING
  /// * INTEGER
  /// * FLOAT
  /// * BOOLEAN
  /// * JSON
  ///
  /// Default type is BYTES.
  /// This can be overridden for a specific column by listing that column in
  /// 'columns' and specifying a type for it.
  package var type: String = String()

  /// Optional. The encoding of the values when the type is not STRING.
  /// Acceptable encoding values are:
  ///   TEXT - indicates values are alphanumeric text strings.
  ///   BINARY - indicates values are encoded using HBase Bytes.toBytes family of
  ///            functions.
  /// This can be overridden for a specific column by listing that column in
  /// 'columns' and specifying an encoding for it.
  package var encoding: String = String()

  /// Optional. Lists of columns that should be exposed as individual fields as
  /// opposed to a list of (column name, value) pairs.
  /// All columns whose qualifier matches a qualifier in this list can be
  /// accessed as `<family field name>.<column field name>`.
  /// Other columns can be accessed as a list through
  /// the `<family field name>.Column` field.
  package var columns: [Google_Cloud_Bigquery_V2_BigtableColumn] = []

  /// Optional. If this is set only the latest version of value are exposed for
  /// all columns in this column family.
  /// This can be overridden for a specific column by listing that column in
  /// 'columns' and specifying a different setting
  /// for that column.
  package var onlyReadLatest: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _onlyReadLatest ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_onlyReadLatest = newValue}
  }
  /// Returns true if `onlyReadLatest` has been explicitly set.
  package var hasOnlyReadLatest: Bool {return self._onlyReadLatest != nil}
  /// Clears the value of `onlyReadLatest`. Subsequent reads from it will return its default value.
  package mutating func clearOnlyReadLatest() {self._onlyReadLatest = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _onlyReadLatest: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
}

/// Options specific to Google Cloud Bigtable data sources.
package struct Google_Cloud_Bigquery_V2_BigtableOptions: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. List of column families to expose in the table schema along with
  /// their types.
  /// This list restricts the column families that can be referenced in queries
  /// and specifies their value types.
  /// You can use this list to do type conversions - see the 'type' field for
  /// more details.
  /// If you leave this list empty, all column families are present in the table
  /// schema and their values are read as BYTES.
  /// During a query only the column families referenced in that query are read
  /// from Bigtable.
  package var columnFamilies: [Google_Cloud_Bigquery_V2_BigtableColumnFamily] = []

  /// Optional. If field is true, then the column families that are not
  /// specified in columnFamilies list are not exposed in the table schema.
  /// Otherwise, they are read with BYTES type values.
  /// The default value is false.
  package var ignoreUnspecifiedColumnFamilies: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _ignoreUnspecifiedColumnFamilies ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_ignoreUnspecifiedColumnFamilies = newValue}
  }
  /// Returns true if `ignoreUnspecifiedColumnFamilies` has been explicitly set.
  package var hasIgnoreUnspecifiedColumnFamilies: Bool {return self._ignoreUnspecifiedColumnFamilies != nil}
  /// Clears the value of `ignoreUnspecifiedColumnFamilies`. Subsequent reads from it will return its default value.
  package mutating func clearIgnoreUnspecifiedColumnFamilies() {self._ignoreUnspecifiedColumnFamilies = nil}

  /// Optional. If field is true, then the rowkey column families will be read
  /// and converted to string. Otherwise they are read with BYTES type values and
  /// users need to manually cast them with CAST if necessary.
  /// The default value is false.
  package var readRowkeyAsString: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _readRowkeyAsString ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_readRowkeyAsString = newValue}
  }
  /// Returns true if `readRowkeyAsString` has been explicitly set.
  package var hasReadRowkeyAsString: Bool {return self._readRowkeyAsString != nil}
  /// Clears the value of `readRowkeyAsString`. Subsequent reads from it will return its default value.
  package mutating func clearReadRowkeyAsString() {self._readRowkeyAsString = nil}

  /// Optional. If field is true, then each column family will be read as a
  /// single JSON column. Otherwise they are read as a repeated cell structure
  /// containing timestamp/value tuples. The default value is false.
  package var outputColumnFamiliesAsJson: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _outputColumnFamiliesAsJson ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_outputColumnFamiliesAsJson = newValue}
  }
  /// Returns true if `outputColumnFamiliesAsJson` has been explicitly set.
  package var hasOutputColumnFamiliesAsJson: Bool {return self._outputColumnFamiliesAsJson != nil}
  /// Clears the value of `outputColumnFamiliesAsJson`. Subsequent reads from it will return its default value.
  package mutating func clearOutputColumnFamiliesAsJson() {self._outputColumnFamiliesAsJson = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _ignoreUnspecifiedColumnFamilies: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
  fileprivate var _readRowkeyAsString: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
  fileprivate var _outputColumnFamiliesAsJson: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
}

/// Options specific to Google Sheets data sources.
package struct Google_Cloud_Bigquery_V2_GoogleSheetsOptions: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. The number of rows at the top of a sheet that BigQuery will skip
  /// when reading the data. The default value is 0. This property is useful if
  /// you have header rows that should be skipped. When autodetect is on,
  /// the behavior is the following:
  /// * skipLeadingRows unspecified - Autodetect tries to detect headers in the
  ///   first row. If they are not detected, the row is read as data. Otherwise
  ///   data is read starting from the second row.
  /// * skipLeadingRows is 0 - Instructs autodetect that there are no headers and
  ///   data should be read starting from the first row.
  /// * skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect
  ///   headers in row N. If headers are not detected, row N is just skipped.
  ///   Otherwise row N is used to extract column names for the detected schema.
  package var skipLeadingRows: SwiftProtobuf.Google_Protobuf_Int64Value {
    get {return _skipLeadingRows ?? SwiftProtobuf.Google_Protobuf_Int64Value()}
    set {_skipLeadingRows = newValue}
  }
  /// Returns true if `skipLeadingRows` has been explicitly set.
  package var hasSkipLeadingRows: Bool {return self._skipLeadingRows != nil}
  /// Clears the value of `skipLeadingRows`. Subsequent reads from it will return its default value.
  package mutating func clearSkipLeadingRows() {self._skipLeadingRows = nil}

  /// Optional. Range of a sheet to query from. Only used when non-empty.
  /// Typical format: sheet_name!top_left_cell_id:bottom_right_cell_id
  /// For example: sheet1!A1:B20
  package var range: String = String()

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _skipLeadingRows: SwiftProtobuf.Google_Protobuf_Int64Value? = nil
}

package struct Google_Cloud_Bigquery_V2_ExternalDataConfiguration: @unchecked Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// [Required] The fully-qualified URIs that point to your data in Google
  /// Cloud. For Google Cloud Storage URIs:
  ///   Each URI can contain one '*' wildcard character and it must come after
  ///   the 'bucket' name.
  ///   Size limits related to load jobs apply to external data sources.
  /// For Google Cloud Bigtable URIs:
  ///   Exactly one URI can be specified and it has be a fully specified and
  ///   valid HTTPS URL for a Google Cloud Bigtable table.
  /// For Google Cloud Datastore backups, exactly one URI can be specified. Also,
  /// the '*' wildcard character is not allowed.
  package var sourceUris: [String] {
    get {return _storage._sourceUris}
    set {_uniqueStorage()._sourceUris = newValue}
  }

  /// Optional. Specifies how source URIs are interpreted for constructing the
  /// file set to load.  By default source URIs are expanded against the
  /// underlying storage.  Other options include specifying manifest files. Only
  /// applicable to object storage systems.
  package var fileSetSpecType: Google_Cloud_Bigquery_V2_FileSetSpecType {
    get {return _storage._fileSetSpecType}
    set {_uniqueStorage()._fileSetSpecType = newValue}
  }

  /// Optional. The schema for the data.
  /// Schema is required for CSV and JSON formats if autodetect is not on.
  /// Schema is disallowed for Google Cloud Bigtable, Cloud Datastore backups,
  /// Avro, ORC and Parquet formats.
  package var schema: Google_Cloud_Bigquery_V2_TableSchema {
    get {return _storage._schema ?? Google_Cloud_Bigquery_V2_TableSchema()}
    set {_uniqueStorage()._schema = newValue}
  }
  /// Returns true if `schema` has been explicitly set.
  package var hasSchema: Bool {return _storage._schema != nil}
  /// Clears the value of `schema`. Subsequent reads from it will return its default value.
  package mutating func clearSchema() {_uniqueStorage()._schema = nil}

  /// [Required] The data format.
  /// For CSV files, specify "CSV".
  /// For Google sheets, specify "GOOGLE_SHEETS".
  /// For newline-delimited JSON, specify "NEWLINE_DELIMITED_JSON".
  /// For Avro files, specify "AVRO".
  /// For Google Cloud Datastore backups, specify "DATASTORE_BACKUP".
  /// For Apache Iceberg tables, specify "ICEBERG".
  /// For ORC files, specify "ORC".
  /// For Parquet files, specify "PARQUET".
  /// [Beta] For Google Cloud Bigtable, specify "BIGTABLE".
  package var sourceFormat: String {
    get {return _storage._sourceFormat}
    set {_uniqueStorage()._sourceFormat = newValue}
  }

  /// Optional. The maximum number of bad records that BigQuery can ignore when
  /// reading data. If the number of bad records exceeds this value, an invalid
  /// error is returned in the job result. The default value is 0, which requires
  /// that all records are valid. This setting is ignored for Google Cloud
  /// Bigtable, Google Cloud Datastore backups, Avro, ORC and Parquet formats.
  package var maxBadRecords: SwiftProtobuf.Google_Protobuf_Int32Value {
    get {return _storage._maxBadRecords ?? SwiftProtobuf.Google_Protobuf_Int32Value()}
    set {_uniqueStorage()._maxBadRecords = newValue}
  }
  /// Returns true if `maxBadRecords` has been explicitly set.
  package var hasMaxBadRecords: Bool {return _storage._maxBadRecords != nil}
  /// Clears the value of `maxBadRecords`. Subsequent reads from it will return its default value.
  package mutating func clearMaxBadRecords() {_uniqueStorage()._maxBadRecords = nil}

  /// Try to detect schema and format options automatically.
  /// Any option specified explicitly will be honored.
  package var autodetect: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._autodetect ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._autodetect = newValue}
  }
  /// Returns true if `autodetect` has been explicitly set.
  package var hasAutodetect: Bool {return _storage._autodetect != nil}
  /// Clears the value of `autodetect`. Subsequent reads from it will return its default value.
  package mutating func clearAutodetect() {_uniqueStorage()._autodetect = nil}

  /// Optional. Indicates if BigQuery should allow extra values that are not
  /// represented in the table schema.
  /// If true, the extra values are ignored.
  /// If false, records with extra columns are treated as bad records, and if
  /// there are too many bad records, an invalid error is returned in the job
  /// result.
  /// The default value is false.
  /// The sourceFormat property determines what BigQuery treats as an extra
  /// value:
  ///   CSV: Trailing columns
  ///   JSON: Named values that don't match any column names
  ///   Google Cloud Bigtable: This setting is ignored.
  ///   Google Cloud Datastore backups: This setting is ignored.
  ///   Avro: This setting is ignored.
  ///   ORC: This setting is ignored.
  ///   Parquet: This setting is ignored.
  package var ignoreUnknownValues: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._ignoreUnknownValues ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._ignoreUnknownValues = newValue}
  }
  /// Returns true if `ignoreUnknownValues` has been explicitly set.
  package var hasIgnoreUnknownValues: Bool {return _storage._ignoreUnknownValues != nil}
  /// Clears the value of `ignoreUnknownValues`. Subsequent reads from it will return its default value.
  package mutating func clearIgnoreUnknownValues() {_uniqueStorage()._ignoreUnknownValues = nil}

  /// Optional. The compression type of the data source.
  /// Possible values include GZIP and NONE. The default value is NONE.
  /// This setting is ignored for Google Cloud Bigtable, Google Cloud Datastore
  /// backups, Avro, ORC and Parquet
  /// formats. An empty string is an invalid value.
  package var compression: String {
    get {return _storage._compression}
    set {_uniqueStorage()._compression = newValue}
  }

  /// Optional. Additional properties to set if sourceFormat is set to CSV.
  package var csvOptions: Google_Cloud_Bigquery_V2_CsvOptions {
    get {return _storage._csvOptions ?? Google_Cloud_Bigquery_V2_CsvOptions()}
    set {_uniqueStorage()._csvOptions = newValue}
  }
  /// Returns true if `csvOptions` has been explicitly set.
  package var hasCsvOptions: Bool {return _storage._csvOptions != nil}
  /// Clears the value of `csvOptions`. Subsequent reads from it will return its default value.
  package mutating func clearCsvOptions() {_uniqueStorage()._csvOptions = nil}

  /// Optional. Additional properties to set if sourceFormat is set to JSON.
  package var jsonOptions: Google_Cloud_Bigquery_V2_JsonOptions {
    get {return _storage._jsonOptions ?? Google_Cloud_Bigquery_V2_JsonOptions()}
    set {_uniqueStorage()._jsonOptions = newValue}
  }
  /// Returns true if `jsonOptions` has been explicitly set.
  package var hasJsonOptions: Bool {return _storage._jsonOptions != nil}
  /// Clears the value of `jsonOptions`. Subsequent reads from it will return its default value.
  package mutating func clearJsonOptions() {_uniqueStorage()._jsonOptions = nil}

  /// Optional. Additional options if sourceFormat is set to BIGTABLE.
  package var bigtableOptions: Google_Cloud_Bigquery_V2_BigtableOptions {
    get {return _storage._bigtableOptions ?? Google_Cloud_Bigquery_V2_BigtableOptions()}
    set {_uniqueStorage()._bigtableOptions = newValue}
  }
  /// Returns true if `bigtableOptions` has been explicitly set.
  package var hasBigtableOptions: Bool {return _storage._bigtableOptions != nil}
  /// Clears the value of `bigtableOptions`. Subsequent reads from it will return its default value.
  package mutating func clearBigtableOptions() {_uniqueStorage()._bigtableOptions = nil}

  /// Optional. Additional options if sourceFormat is set to GOOGLE_SHEETS.
  package var googleSheetsOptions: Google_Cloud_Bigquery_V2_GoogleSheetsOptions {
    get {return _storage._googleSheetsOptions ?? Google_Cloud_Bigquery_V2_GoogleSheetsOptions()}
    set {_uniqueStorage()._googleSheetsOptions = newValue}
  }
  /// Returns true if `googleSheetsOptions` has been explicitly set.
  package var hasGoogleSheetsOptions: Bool {return _storage._googleSheetsOptions != nil}
  /// Clears the value of `googleSheetsOptions`. Subsequent reads from it will return its default value.
  package mutating func clearGoogleSheetsOptions() {_uniqueStorage()._googleSheetsOptions = nil}

  /// Optional. When set, configures hive partitioning support. Not all storage
  /// formats support hive partitioning -- requesting hive partitioning on an
  /// unsupported format will lead to an error, as will providing an invalid
  /// specification.
  package var hivePartitioningOptions: Google_Cloud_Bigquery_V2_HivePartitioningOptions {
    get {return _storage._hivePartitioningOptions ?? Google_Cloud_Bigquery_V2_HivePartitioningOptions()}
    set {_uniqueStorage()._hivePartitioningOptions = newValue}
  }
  /// Returns true if `hivePartitioningOptions` has been explicitly set.
  package var hasHivePartitioningOptions: Bool {return _storage._hivePartitioningOptions != nil}
  /// Clears the value of `hivePartitioningOptions`. Subsequent reads from it will return its default value.
  package mutating func clearHivePartitioningOptions() {_uniqueStorage()._hivePartitioningOptions = nil}

  /// Optional. The connection specifying the credentials to be used to read
  /// external storage, such as Azure Blob, Cloud Storage, or S3. The
  /// connection_id can have the form
  /// `{project_id}.{location_id};{connection_id}` or
  /// `projects/{project_id}/locations/{location_id}/connections/{connection_id}`.
  package var connectionID: String {
    get {return _storage._connectionID}
    set {_uniqueStorage()._connectionID = newValue}
  }

  /// Defines the list of possible SQL data types to which the source decimal
  /// values are converted. This list and the precision and the scale parameters
  /// of the decimal field determine the target type. In the order of NUMERIC,
  /// BIGNUMERIC, and STRING, a
  /// type is picked if it is in the specified list and if it supports the
  /// precision and the scale. STRING supports all precision and scale values.
  /// If none of the listed types supports the precision and the scale, the type
  /// supporting the widest range in the specified list is picked, and if a value
  /// exceeds the supported range when reading the data, an error will be thrown.
  ///
  /// Example: Suppose the value of this field is ["NUMERIC", "BIGNUMERIC"].
  /// If (precision,scale) is:
  ///
  /// * (38,9) -> NUMERIC;
  /// * (39,9) -> BIGNUMERIC (NUMERIC cannot hold 30 integer digits);
  /// * (38,10) -> BIGNUMERIC (NUMERIC cannot hold 10 fractional digits);
  /// * (76,38) -> BIGNUMERIC;
  /// * (77,38) -> BIGNUMERIC (error if value exeeds supported range).
  ///
  /// This field cannot contain duplicate types. The order of the types in this
  /// field is ignored. For example, ["BIGNUMERIC", "NUMERIC"] is the same as
  /// ["NUMERIC", "BIGNUMERIC"] and NUMERIC always takes precedence over
  /// BIGNUMERIC.
  ///
  /// Defaults to ["NUMERIC", "STRING"] for ORC and ["NUMERIC"] for the other
  /// file formats.
  package var decimalTargetTypes: [Google_Cloud_Bigquery_V2_DecimalTargetType] {
    get {return _storage._decimalTargetTypes}
    set {_uniqueStorage()._decimalTargetTypes = newValue}
  }

  /// Optional. Additional properties to set if sourceFormat is set to AVRO.
  package var avroOptions: Google_Cloud_Bigquery_V2_AvroOptions {
    get {return _storage._avroOptions ?? Google_Cloud_Bigquery_V2_AvroOptions()}
    set {_uniqueStorage()._avroOptions = newValue}
  }
  /// Returns true if `avroOptions` has been explicitly set.
  package var hasAvroOptions: Bool {return _storage._avroOptions != nil}
  /// Clears the value of `avroOptions`. Subsequent reads from it will return its default value.
  package mutating func clearAvroOptions() {_uniqueStorage()._avroOptions = nil}

  /// Optional. Load option to be used together with source_format
  /// newline-delimited JSON to indicate that a variant of JSON is being loaded.
  /// To load newline-delimited GeoJSON, specify GEOJSON (and source_format must
  /// be set to NEWLINE_DELIMITED_JSON).
  package var jsonExtension: Google_Cloud_Bigquery_V2_JsonExtension {
    get {return _storage._jsonExtension}
    set {_uniqueStorage()._jsonExtension = newValue}
  }

  /// Optional. Additional properties to set if sourceFormat is set to PARQUET.
  package var parquetOptions: Google_Cloud_Bigquery_V2_ParquetOptions {
    get {return _storage._parquetOptions ?? Google_Cloud_Bigquery_V2_ParquetOptions()}
    set {_uniqueStorage()._parquetOptions = newValue}
  }
  /// Returns true if `parquetOptions` has been explicitly set.
  package var hasParquetOptions: Bool {return _storage._parquetOptions != nil}
  /// Clears the value of `parquetOptions`. Subsequent reads from it will return its default value.
  package mutating func clearParquetOptions() {_uniqueStorage()._parquetOptions = nil}

  /// Optional. ObjectMetadata is used to create Object Tables. Object Tables
  /// contain a listing of objects (with their metadata) found at the
  /// source_uris. If ObjectMetadata is set, source_format should be omitted.
  ///
  /// Currently SIMPLE is the only supported Object Metadata type.
  package var objectMetadata: Google_Cloud_Bigquery_V2_ExternalDataConfiguration.ObjectMetadata {
    get {return _storage._objectMetadata ?? .unspecified}
    set {_uniqueStorage()._objectMetadata = newValue}
  }
  /// Returns true if `objectMetadata` has been explicitly set.
  package var hasObjectMetadata: Bool {return _storage._objectMetadata != nil}
  /// Clears the value of `objectMetadata`. Subsequent reads from it will return its default value.
  package mutating func clearObjectMetadata() {_uniqueStorage()._objectMetadata = nil}

  /// Optional. When creating an external table, the user can provide a reference
  /// file with the table schema. This is enabled for the following formats:
  /// AVRO, PARQUET, ORC.
  package var referenceFileSchemaUri: SwiftProtobuf.Google_Protobuf_StringValue {
    get {return _storage._referenceFileSchemaUri ?? SwiftProtobuf.Google_Protobuf_StringValue()}
    set {_uniqueStorage()._referenceFileSchemaUri = newValue}
  }
  /// Returns true if `referenceFileSchemaUri` has been explicitly set.
  package var hasReferenceFileSchemaUri: Bool {return _storage._referenceFileSchemaUri != nil}
  /// Clears the value of `referenceFileSchemaUri`. Subsequent reads from it will return its default value.
  package mutating func clearReferenceFileSchemaUri() {_uniqueStorage()._referenceFileSchemaUri = nil}

  /// Optional. Metadata Cache Mode for the table. Set this to enable caching of
  /// metadata from external data source.
  package var metadataCacheMode: Google_Cloud_Bigquery_V2_ExternalDataConfiguration.MetadataCacheMode {
    get {return _storage._metadataCacheMode}
    set {_uniqueStorage()._metadataCacheMode = newValue}
  }

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Supported Object Metadata Types.
  package enum ObjectMetadata: SwiftProtobuf.Enum, Swift.CaseIterable {
    package typealias RawValue = Int

    /// Unspecified by default.
    case unspecified // = 0

    /// A synonym for `SIMPLE`.
    case directory // = 1

    /// Directory listing of objects.
    case simple // = 2
    case UNRECOGNIZED(Int)

    package init() {
      self = .unspecified
    }

    package init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .directory
      case 2: self = .simple
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    package var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .directory: return 1
      case .simple: return 2
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    package static let allCases: [Google_Cloud_Bigquery_V2_ExternalDataConfiguration.ObjectMetadata] = [
      .unspecified,
      .directory,
      .simple,
    ]

  }

  /// MetadataCacheMode identifies if the table should use metadata caching for
  /// files from external source (eg Google Cloud Storage).
  package enum MetadataCacheMode: SwiftProtobuf.Enum, Swift.CaseIterable {
    package typealias RawValue = Int

    /// Unspecified metadata cache mode.
    case unspecified // = 0

    /// Set this mode to trigger automatic background refresh of metadata cache
    /// from the external source. Queries will use the latest available cache
    /// version within the table's maxStaleness interval.
    case automatic // = 1

    /// Set this mode to enable triggering manual refresh of the metadata cache
    /// from external source. Queries will use the latest manually triggered
    /// cache version within the table's maxStaleness interval.
    case manual // = 2
    case UNRECOGNIZED(Int)

    package init() {
      self = .unspecified
    }

    package init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .automatic
      case 2: self = .manual
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    package var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .automatic: return 1
      case .manual: return 2
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    package static let allCases: [Google_Cloud_Bigquery_V2_ExternalDataConfiguration.MetadataCacheMode] = [
      .unspecified,
      .automatic,
      .manual,
    ]

  }

  package init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.bigquery.v2"

extension Google_Cloud_Bigquery_V2_AvroOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".AvroOptions"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "use_avro_logical_types"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._useAvroLogicalTypes) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._useAvroLogicalTypes {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_AvroOptions, rhs: Google_Cloud_Bigquery_V2_AvroOptions) -> Bool {
    if lhs._useAvroLogicalTypes != rhs._useAvroLogicalTypes {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_ParquetOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".ParquetOptions"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "enum_as_string"),
    2: .standard(proto: "enable_list_inference"),
    3: .standard(proto: "map_target_type"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._enumAsString) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._enableListInference) }()
      case 3: try { try decoder.decodeSingularEnumField(value: &self.mapTargetType) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._enumAsString {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    try { if let v = self._enableListInference {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    if self.mapTargetType != .unspecified {
      try visitor.visitSingularEnumField(value: self.mapTargetType, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_ParquetOptions, rhs: Google_Cloud_Bigquery_V2_ParquetOptions) -> Bool {
    if lhs._enumAsString != rhs._enumAsString {return false}
    if lhs._enableListInference != rhs._enableListInference {return false}
    if lhs.mapTargetType != rhs.mapTargetType {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_CsvOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".CsvOptions"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "field_delimiter"),
    2: .standard(proto: "skip_leading_rows"),
    3: .same(proto: "quote"),
    4: .standard(proto: "allow_quoted_newlines"),
    5: .standard(proto: "allow_jagged_rows"),
    6: .same(proto: "encoding"),
    7: .standard(proto: "preserve_ascii_control_characters"),
    8: .standard(proto: "null_marker"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.fieldDelimiter) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._skipLeadingRows) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._quote) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._allowQuotedNewlines) }()
      case 5: try { try decoder.decodeSingularMessageField(value: &self._allowJaggedRows) }()
      case 6: try { try decoder.decodeSingularStringField(value: &self.encoding) }()
      case 7: try { try decoder.decodeSingularMessageField(value: &self._preserveAsciiControlCharacters) }()
      case 8: try { try decoder.decodeSingularMessageField(value: &self._nullMarker) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.fieldDelimiter.isEmpty {
      try visitor.visitSingularStringField(value: self.fieldDelimiter, fieldNumber: 1)
    }
    try { if let v = self._skipLeadingRows {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    try { if let v = self._quote {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    } }()
    try { if let v = self._allowQuotedNewlines {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    } }()
    try { if let v = self._allowJaggedRows {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
    } }()
    if !self.encoding.isEmpty {
      try visitor.visitSingularStringField(value: self.encoding, fieldNumber: 6)
    }
    try { if let v = self._preserveAsciiControlCharacters {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
    } }()
    try { if let v = self._nullMarker {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_CsvOptions, rhs: Google_Cloud_Bigquery_V2_CsvOptions) -> Bool {
    if lhs.fieldDelimiter != rhs.fieldDelimiter {return false}
    if lhs._skipLeadingRows != rhs._skipLeadingRows {return false}
    if lhs._quote != rhs._quote {return false}
    if lhs._allowQuotedNewlines != rhs._allowQuotedNewlines {return false}
    if lhs._allowJaggedRows != rhs._allowJaggedRows {return false}
    if lhs.encoding != rhs.encoding {return false}
    if lhs._preserveAsciiControlCharacters != rhs._preserveAsciiControlCharacters {return false}
    if lhs._nullMarker != rhs._nullMarker {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_JsonOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".JsonOptions"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "encoding"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.encoding) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.encoding.isEmpty {
      try visitor.visitSingularStringField(value: self.encoding, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_JsonOptions, rhs: Google_Cloud_Bigquery_V2_JsonOptions) -> Bool {
    if lhs.encoding != rhs.encoding {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_BigtableColumn: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".BigtableColumn"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "qualifier_encoded"),
    2: .standard(proto: "qualifier_string"),
    3: .standard(proto: "field_name"),
    4: .same(proto: "type"),
    5: .same(proto: "encoding"),
    6: .standard(proto: "only_read_latest"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._qualifierEncoded) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._qualifierString) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.fieldName) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.type) }()
      case 5: try { try decoder.decodeSingularStringField(value: &self.encoding) }()
      case 6: try { try decoder.decodeSingularMessageField(value: &self._onlyReadLatest) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._qualifierEncoded {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    try { if let v = self._qualifierString {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    if !self.fieldName.isEmpty {
      try visitor.visitSingularStringField(value: self.fieldName, fieldNumber: 3)
    }
    if !self.type.isEmpty {
      try visitor.visitSingularStringField(value: self.type, fieldNumber: 4)
    }
    if !self.encoding.isEmpty {
      try visitor.visitSingularStringField(value: self.encoding, fieldNumber: 5)
    }
    try { if let v = self._onlyReadLatest {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_BigtableColumn, rhs: Google_Cloud_Bigquery_V2_BigtableColumn) -> Bool {
    if lhs._qualifierEncoded != rhs._qualifierEncoded {return false}
    if lhs._qualifierString != rhs._qualifierString {return false}
    if lhs.fieldName != rhs.fieldName {return false}
    if lhs.type != rhs.type {return false}
    if lhs.encoding != rhs.encoding {return false}
    if lhs._onlyReadLatest != rhs._onlyReadLatest {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_BigtableColumnFamily: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".BigtableColumnFamily"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "family_id"),
    2: .same(proto: "type"),
    3: .same(proto: "encoding"),
    4: .same(proto: "columns"),
    5: .standard(proto: "only_read_latest"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.familyID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.type) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.encoding) }()
      case 4: try { try decoder.decodeRepeatedMessageField(value: &self.columns) }()
      case 5: try { try decoder.decodeSingularMessageField(value: &self._onlyReadLatest) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.familyID.isEmpty {
      try visitor.visitSingularStringField(value: self.familyID, fieldNumber: 1)
    }
    if !self.type.isEmpty {
      try visitor.visitSingularStringField(value: self.type, fieldNumber: 2)
    }
    if !self.encoding.isEmpty {
      try visitor.visitSingularStringField(value: self.encoding, fieldNumber: 3)
    }
    if !self.columns.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.columns, fieldNumber: 4)
    }
    try { if let v = self._onlyReadLatest {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_BigtableColumnFamily, rhs: Google_Cloud_Bigquery_V2_BigtableColumnFamily) -> Bool {
    if lhs.familyID != rhs.familyID {return false}
    if lhs.type != rhs.type {return false}
    if lhs.encoding != rhs.encoding {return false}
    if lhs.columns != rhs.columns {return false}
    if lhs._onlyReadLatest != rhs._onlyReadLatest {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_BigtableOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".BigtableOptions"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "column_families"),
    2: .standard(proto: "ignore_unspecified_column_families"),
    3: .standard(proto: "read_rowkey_as_string"),
    4: .standard(proto: "output_column_families_as_json"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.columnFamilies) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._ignoreUnspecifiedColumnFamilies) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._readRowkeyAsString) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._outputColumnFamiliesAsJson) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.columnFamilies.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.columnFamilies, fieldNumber: 1)
    }
    try { if let v = self._ignoreUnspecifiedColumnFamilies {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    try { if let v = self._readRowkeyAsString {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    } }()
    try { if let v = self._outputColumnFamiliesAsJson {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_BigtableOptions, rhs: Google_Cloud_Bigquery_V2_BigtableOptions) -> Bool {
    if lhs.columnFamilies != rhs.columnFamilies {return false}
    if lhs._ignoreUnspecifiedColumnFamilies != rhs._ignoreUnspecifiedColumnFamilies {return false}
    if lhs._readRowkeyAsString != rhs._readRowkeyAsString {return false}
    if lhs._outputColumnFamiliesAsJson != rhs._outputColumnFamiliesAsJson {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_GoogleSheetsOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".GoogleSheetsOptions"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "skip_leading_rows"),
    2: .same(proto: "range"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._skipLeadingRows) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.range) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._skipLeadingRows {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    if !self.range.isEmpty {
      try visitor.visitSingularStringField(value: self.range, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_GoogleSheetsOptions, rhs: Google_Cloud_Bigquery_V2_GoogleSheetsOptions) -> Bool {
    if lhs._skipLeadingRows != rhs._skipLeadingRows {return false}
    if lhs.range != rhs.range {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_ExternalDataConfiguration: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".ExternalDataConfiguration"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "source_uris"),
    25: .standard(proto: "file_set_spec_type"),
    2: .same(proto: "schema"),
    3: .standard(proto: "source_format"),
    4: .standard(proto: "max_bad_records"),
    5: .same(proto: "autodetect"),
    6: .standard(proto: "ignore_unknown_values"),
    7: .same(proto: "compression"),
    8: .standard(proto: "csv_options"),
    26: .standard(proto: "json_options"),
    9: .standard(proto: "bigtable_options"),
    10: .standard(proto: "google_sheets_options"),
    13: .standard(proto: "hive_partitioning_options"),
    14: .standard(proto: "connection_id"),
    16: .standard(proto: "decimal_target_types"),
    17: .standard(proto: "avro_options"),
    18: .standard(proto: "json_extension"),
    19: .standard(proto: "parquet_options"),
    22: .standard(proto: "object_metadata"),
    23: .standard(proto: "reference_file_schema_uri"),
    24: .standard(proto: "metadata_cache_mode"),
  ]

  fileprivate class _StorageClass {
    var _sourceUris: [String] = []
    var _fileSetSpecType: Google_Cloud_Bigquery_V2_FileSetSpecType = .fileSystemMatch
    var _schema: Google_Cloud_Bigquery_V2_TableSchema? = nil
    var _sourceFormat: String = String()
    var _maxBadRecords: SwiftProtobuf.Google_Protobuf_Int32Value? = nil
    var _autodetect: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
    var _ignoreUnknownValues: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
    var _compression: String = String()
    var _csvOptions: Google_Cloud_Bigquery_V2_CsvOptions? = nil
    var _jsonOptions: Google_Cloud_Bigquery_V2_JsonOptions? = nil
    var _bigtableOptions: Google_Cloud_Bigquery_V2_BigtableOptions? = nil
    var _googleSheetsOptions: Google_Cloud_Bigquery_V2_GoogleSheetsOptions? = nil
    var _hivePartitioningOptions: Google_Cloud_Bigquery_V2_HivePartitioningOptions? = nil
    var _connectionID: String = String()
    var _decimalTargetTypes: [Google_Cloud_Bigquery_V2_DecimalTargetType] = []
    var _avroOptions: Google_Cloud_Bigquery_V2_AvroOptions? = nil
    var _jsonExtension: Google_Cloud_Bigquery_V2_JsonExtension = .unspecified
    var _parquetOptions: Google_Cloud_Bigquery_V2_ParquetOptions? = nil
    var _objectMetadata: Google_Cloud_Bigquery_V2_ExternalDataConfiguration.ObjectMetadata? = nil
    var _referenceFileSchemaUri: SwiftProtobuf.Google_Protobuf_StringValue? = nil
    var _metadataCacheMode: Google_Cloud_Bigquery_V2_ExternalDataConfiguration.MetadataCacheMode = .unspecified

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _sourceUris = source._sourceUris
      _fileSetSpecType = source._fileSetSpecType
      _schema = source._schema
      _sourceFormat = source._sourceFormat
      _maxBadRecords = source._maxBadRecords
      _autodetect = source._autodetect
      _ignoreUnknownValues = source._ignoreUnknownValues
      _compression = source._compression
      _csvOptions = source._csvOptions
      _jsonOptions = source._jsonOptions
      _bigtableOptions = source._bigtableOptions
      _googleSheetsOptions = source._googleSheetsOptions
      _hivePartitioningOptions = source._hivePartitioningOptions
      _connectionID = source._connectionID
      _decimalTargetTypes = source._decimalTargetTypes
      _avroOptions = source._avroOptions
      _jsonExtension = source._jsonExtension
      _parquetOptions = source._parquetOptions
      _objectMetadata = source._objectMetadata
      _referenceFileSchemaUri = source._referenceFileSchemaUri
      _metadataCacheMode = source._metadataCacheMode
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeRepeatedStringField(value: &_storage._sourceUris) }()
        case 2: try { try decoder.decodeSingularMessageField(value: &_storage._schema) }()
        case 3: try { try decoder.decodeSingularStringField(value: &_storage._sourceFormat) }()
        case 4: try { try decoder.decodeSingularMessageField(value: &_storage._maxBadRecords) }()
        case 5: try { try decoder.decodeSingularMessageField(value: &_storage._autodetect) }()
        case 6: try { try decoder.decodeSingularMessageField(value: &_storage._ignoreUnknownValues) }()
        case 7: try { try decoder.decodeSingularStringField(value: &_storage._compression) }()
        case 8: try { try decoder.decodeSingularMessageField(value: &_storage._csvOptions) }()
        case 9: try { try decoder.decodeSingularMessageField(value: &_storage._bigtableOptions) }()
        case 10: try { try decoder.decodeSingularMessageField(value: &_storage._googleSheetsOptions) }()
        case 13: try { try decoder.decodeSingularMessageField(value: &_storage._hivePartitioningOptions) }()
        case 14: try { try decoder.decodeSingularStringField(value: &_storage._connectionID) }()
        case 16: try { try decoder.decodeRepeatedEnumField(value: &_storage._decimalTargetTypes) }()
        case 17: try { try decoder.decodeSingularMessageField(value: &_storage._avroOptions) }()
        case 18: try { try decoder.decodeSingularEnumField(value: &_storage._jsonExtension) }()
        case 19: try { try decoder.decodeSingularMessageField(value: &_storage._parquetOptions) }()
        case 22: try { try decoder.decodeSingularEnumField(value: &_storage._objectMetadata) }()
        case 23: try { try decoder.decodeSingularMessageField(value: &_storage._referenceFileSchemaUri) }()
        case 24: try { try decoder.decodeSingularEnumField(value: &_storage._metadataCacheMode) }()
        case 25: try { try decoder.decodeSingularEnumField(value: &_storage._fileSetSpecType) }()
        case 26: try { try decoder.decodeSingularMessageField(value: &_storage._jsonOptions) }()
        default: break
        }
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      if !_storage._sourceUris.isEmpty {
        try visitor.visitRepeatedStringField(value: _storage._sourceUris, fieldNumber: 1)
      }
      try { if let v = _storage._schema {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      } }()
      if !_storage._sourceFormat.isEmpty {
        try visitor.visitSingularStringField(value: _storage._sourceFormat, fieldNumber: 3)
      }
      try { if let v = _storage._maxBadRecords {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
      } }()
      try { if let v = _storage._autodetect {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
      } }()
      try { if let v = _storage._ignoreUnknownValues {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
      } }()
      if !_storage._compression.isEmpty {
        try visitor.visitSingularStringField(value: _storage._compression, fieldNumber: 7)
      }
      try { if let v = _storage._csvOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
      } }()
      try { if let v = _storage._bigtableOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
      } }()
      try { if let v = _storage._googleSheetsOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 10)
      } }()
      try { if let v = _storage._hivePartitioningOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 13)
      } }()
      if !_storage._connectionID.isEmpty {
        try visitor.visitSingularStringField(value: _storage._connectionID, fieldNumber: 14)
      }
      if !_storage._decimalTargetTypes.isEmpty {
        try visitor.visitPackedEnumField(value: _storage._decimalTargetTypes, fieldNumber: 16)
      }
      try { if let v = _storage._avroOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 17)
      } }()
      if _storage._jsonExtension != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._jsonExtension, fieldNumber: 18)
      }
      try { if let v = _storage._parquetOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 19)
      } }()
      try { if let v = _storage._objectMetadata {
        try visitor.visitSingularEnumField(value: v, fieldNumber: 22)
      } }()
      try { if let v = _storage._referenceFileSchemaUri {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 23)
      } }()
      if _storage._metadataCacheMode != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._metadataCacheMode, fieldNumber: 24)
      }
      if _storage._fileSetSpecType != .fileSystemMatch {
        try visitor.visitSingularEnumField(value: _storage._fileSetSpecType, fieldNumber: 25)
      }
      try { if let v = _storage._jsonOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 26)
      } }()
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_ExternalDataConfiguration, rhs: Google_Cloud_Bigquery_V2_ExternalDataConfiguration) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._sourceUris != rhs_storage._sourceUris {return false}
        if _storage._fileSetSpecType != rhs_storage._fileSetSpecType {return false}
        if _storage._schema != rhs_storage._schema {return false}
        if _storage._sourceFormat != rhs_storage._sourceFormat {return false}
        if _storage._maxBadRecords != rhs_storage._maxBadRecords {return false}
        if _storage._autodetect != rhs_storage._autodetect {return false}
        if _storage._ignoreUnknownValues != rhs_storage._ignoreUnknownValues {return false}
        if _storage._compression != rhs_storage._compression {return false}
        if _storage._csvOptions != rhs_storage._csvOptions {return false}
        if _storage._jsonOptions != rhs_storage._jsonOptions {return false}
        if _storage._bigtableOptions != rhs_storage._bigtableOptions {return false}
        if _storage._googleSheetsOptions != rhs_storage._googleSheetsOptions {return false}
        if _storage._hivePartitioningOptions != rhs_storage._hivePartitioningOptions {return false}
        if _storage._connectionID != rhs_storage._connectionID {return false}
        if _storage._decimalTargetTypes != rhs_storage._decimalTargetTypes {return false}
        if _storage._avroOptions != rhs_storage._avroOptions {return false}
        if _storage._jsonExtension != rhs_storage._jsonExtension {return false}
        if _storage._parquetOptions != rhs_storage._parquetOptions {return false}
        if _storage._objectMetadata != rhs_storage._objectMetadata {return false}
        if _storage._referenceFileSchemaUri != rhs_storage._referenceFileSchemaUri {return false}
        if _storage._metadataCacheMode != rhs_storage._metadataCacheMode {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_ExternalDataConfiguration.ObjectMetadata: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "OBJECT_METADATA_UNSPECIFIED"),
    1: .same(proto: "DIRECTORY"),
    2: .same(proto: "SIMPLE"),
  ]
}

extension Google_Cloud_Bigquery_V2_ExternalDataConfiguration.MetadataCacheMode: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "METADATA_CACHE_MODE_UNSPECIFIED"),
    1: .same(proto: "AUTOMATIC"),
    2: .same(proto: "MANUAL"),
  ]
}
