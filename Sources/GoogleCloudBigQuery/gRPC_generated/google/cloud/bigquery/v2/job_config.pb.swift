// DO NOT EDIT.
// swift-format-ignore-file
// swiftlint:disable all
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/bigquery/v2/job_config.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// Properties for the destination table.
package struct Google_Cloud_Bigquery_V2_DestinationTableProperties: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. Friendly name for the destination table. If the table already
  /// exists, it should be same as the existing friendly name.
  package var friendlyName: SwiftProtobuf.Google_Protobuf_StringValue {
    get {return _friendlyName ?? SwiftProtobuf.Google_Protobuf_StringValue()}
    set {_friendlyName = newValue}
  }
  /// Returns true if `friendlyName` has been explicitly set.
  package var hasFriendlyName: Bool {return self._friendlyName != nil}
  /// Clears the value of `friendlyName`. Subsequent reads from it will return its default value.
  package mutating func clearFriendlyName() {self._friendlyName = nil}

  /// Optional. The description for the destination table.
  /// This will only be used if the destination table is newly created.
  /// If the table already exists and a value different than the current
  /// description is provided, the job will fail.
  package var description_p: SwiftProtobuf.Google_Protobuf_StringValue {
    get {return _description_p ?? SwiftProtobuf.Google_Protobuf_StringValue()}
    set {_description_p = newValue}
  }
  /// Returns true if `description_p` has been explicitly set.
  package var hasDescription_p: Bool {return self._description_p != nil}
  /// Clears the value of `description_p`. Subsequent reads from it will return its default value.
  package mutating func clearDescription_p() {self._description_p = nil}

  /// Optional. The labels associated with this table. You can use these to
  /// organize and group your tables. This will only be used if the destination
  /// table is newly created. If the table already exists and labels are
  /// different than the current labels are provided, the job will fail.
  package var labels: Dictionary<String,String> = [:]

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _friendlyName: SwiftProtobuf.Google_Protobuf_StringValue? = nil
  fileprivate var _description_p: SwiftProtobuf.Google_Protobuf_StringValue? = nil
}

/// A connection-level property to customize query behavior. Under JDBC, these
/// correspond directly to connection properties passed to the DriverManager.
/// Under ODBC, these correspond to properties in the connection string.
///
/// Currently supported connection properties:
///
/// * **dataset_project_id**: represents the default project for datasets that
/// are used in the query. Setting the
/// system variable `@@dataset_project_id` achieves the same behavior.  For
/// more information about system variables, see:
/// https://cloud.google.com/bigquery/docs/reference/system-variables
///
/// * **time_zone**: represents the default timezone used to run the query.
///
/// * **session_id**: associates the query with a given session.
///
/// * **query_label**: associates the query with a given job label. If set,
/// all subsequent queries in a script or session will have this label. For the
/// format in which a you can specify a query label, see labels
/// in the JobConfiguration resource type:
/// https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfiguration
///
/// * **service_account**: indicates the service account to use to run a
/// continuous query. If set, the query job uses the service account to access
/// Google Cloud resources. Service account access is bounded by the IAM
/// permissions that you have granted to the service account.
///
/// Additional properties are allowed, but ignored. Specifying multiple
/// connection properties with the same key returns an error.
package struct Google_Cloud_Bigquery_V2_ConnectionProperty: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The key of the property to set.
  package var key: String = String()

  /// The value of the property to set.
  package var value: String = String()

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}
}

/// JobConfigurationQuery configures a BigQuery query job.
package struct Google_Cloud_Bigquery_V2_JobConfigurationQuery: @unchecked Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// [Required] SQL query text to execute. The useLegacySql field can be used
  /// to indicate whether the query uses legacy SQL or GoogleSQL.
  package var query: String {
    get {return _storage._query}
    set {_uniqueStorage()._query = newValue}
  }

  /// Optional. Describes the table where the query results should be stored.
  /// This property must be set for large results that exceed the maximum
  /// response size.  For queries that produce anonymous (cached) results, this
  /// field will be populated by BigQuery.
  package var destinationTable: Google_Cloud_Bigquery_V2_TableReference {
    get {return _storage._destinationTable ?? Google_Cloud_Bigquery_V2_TableReference()}
    set {_uniqueStorage()._destinationTable = newValue}
  }
  /// Returns true if `destinationTable` has been explicitly set.
  package var hasDestinationTable: Bool {return _storage._destinationTable != nil}
  /// Clears the value of `destinationTable`. Subsequent reads from it will return its default value.
  package mutating func clearDestinationTable() {_uniqueStorage()._destinationTable = nil}

  /// Optional. You can specify external table definitions, which operate as
  /// ephemeral tables that can be queried.  These definitions are configured
  /// using a JSON map, where the string key represents the table identifier, and
  /// the value is the corresponding external data configuration object.
  package var externalTableDefinitions: Dictionary<String,Google_Cloud_Bigquery_V2_ExternalDataConfiguration> {
    get {return _storage._externalTableDefinitions}
    set {_uniqueStorage()._externalTableDefinitions = newValue}
  }

  /// Describes user-defined function resources used in the query.
  package var userDefinedFunctionResources: [Google_Cloud_Bigquery_V2_UserDefinedFunctionResource] {
    get {return _storage._userDefinedFunctionResources}
    set {_uniqueStorage()._userDefinedFunctionResources = newValue}
  }

  /// Optional. Specifies whether the job is allowed to create new tables.
  /// The following values are supported:
  ///
  /// * CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the
  /// table.
  /// * CREATE_NEVER: The table must already exist. If it does not,
  /// a 'notFound' error is returned in the job result.
  ///
  /// The default value is CREATE_IF_NEEDED.
  /// Creation, truncation and append actions occur as one atomic update
  /// upon job completion.
  package var createDisposition: String {
    get {return _storage._createDisposition}
    set {_uniqueStorage()._createDisposition = newValue}
  }

  /// Optional. Specifies the action that occurs if the destination table
  /// already exists. The following values are supported:
  ///
  /// * WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the
  /// data, removes the constraints, and uses the schema from the query result.
  /// * WRITE_APPEND: If the table already exists, BigQuery appends the data to
  /// the table.
  /// * WRITE_EMPTY: If the table already exists and contains data, a 'duplicate'
  /// error is returned in the job result.
  ///
  /// The default value is WRITE_EMPTY. Each action is atomic and only occurs if
  /// BigQuery is able to complete the job successfully. Creation, truncation and
  /// append actions occur as one atomic update upon job completion.
  package var writeDisposition: String {
    get {return _storage._writeDisposition}
    set {_uniqueStorage()._writeDisposition = newValue}
  }

  /// Optional. Specifies the default dataset to use for unqualified
  /// table names in the query. This setting does not alter behavior of
  /// unqualified dataset names. Setting the system variable
  /// `@@dataset_id` achieves the same behavior.  See
  /// https://cloud.google.com/bigquery/docs/reference/system-variables for more
  /// information on system variables.
  package var defaultDataset: Google_Cloud_Bigquery_V2_DatasetReference {
    get {return _storage._defaultDataset ?? Google_Cloud_Bigquery_V2_DatasetReference()}
    set {_uniqueStorage()._defaultDataset = newValue}
  }
  /// Returns true if `defaultDataset` has been explicitly set.
  package var hasDefaultDataset: Bool {return _storage._defaultDataset != nil}
  /// Clears the value of `defaultDataset`. Subsequent reads from it will return its default value.
  package mutating func clearDefaultDataset() {_uniqueStorage()._defaultDataset = nil}

  /// Optional. Specifies a priority for the query. Possible values include
  /// INTERACTIVE and BATCH. The default value is INTERACTIVE.
  package var priority: String {
    get {return _storage._priority}
    set {_uniqueStorage()._priority = newValue}
  }

  /// Optional. If true and query uses legacy SQL dialect, allows the query
  /// to produce arbitrarily large result tables at a slight cost in performance.
  /// Requires destinationTable to be set.
  /// For GoogleSQL queries, this flag is ignored and large results are
  /// always allowed.  However, you must still set destinationTable when result
  /// size exceeds the allowed maximum response size.
  package var allowLargeResults: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._allowLargeResults ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._allowLargeResults = newValue}
  }
  /// Returns true if `allowLargeResults` has been explicitly set.
  package var hasAllowLargeResults: Bool {return _storage._allowLargeResults != nil}
  /// Clears the value of `allowLargeResults`. Subsequent reads from it will return its default value.
  package mutating func clearAllowLargeResults() {_uniqueStorage()._allowLargeResults = nil}

  /// Optional. Whether to look for the result in the query cache. The query
  /// cache is a best-effort cache that will be flushed whenever tables in the
  /// query are modified. Moreover, the query cache is only available when a
  /// query does not have a destination table specified. The default value is
  /// true.
  package var useQueryCache: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._useQueryCache ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._useQueryCache = newValue}
  }
  /// Returns true if `useQueryCache` has been explicitly set.
  package var hasUseQueryCache: Bool {return _storage._useQueryCache != nil}
  /// Clears the value of `useQueryCache`. Subsequent reads from it will return its default value.
  package mutating func clearUseQueryCache() {_uniqueStorage()._useQueryCache = nil}

  /// Optional. If true and query uses legacy SQL dialect, flattens all nested
  /// and repeated fields in the query results.
  /// allowLargeResults must be true if this is set to false.
  /// For GoogleSQL queries, this flag is ignored and results are never
  /// flattened.
  package var flattenResults: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._flattenResults ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._flattenResults = newValue}
  }
  /// Returns true if `flattenResults` has been explicitly set.
  package var hasFlattenResults: Bool {return _storage._flattenResults != nil}
  /// Clears the value of `flattenResults`. Subsequent reads from it will return its default value.
  package mutating func clearFlattenResults() {_uniqueStorage()._flattenResults = nil}

  /// Limits the bytes billed for this job. Queries that will have
  /// bytes billed beyond this limit will fail (without incurring a charge).
  /// If unspecified, this will be set to your project default.
  package var maximumBytesBilled: SwiftProtobuf.Google_Protobuf_Int64Value {
    get {return _storage._maximumBytesBilled ?? SwiftProtobuf.Google_Protobuf_Int64Value()}
    set {_uniqueStorage()._maximumBytesBilled = newValue}
  }
  /// Returns true if `maximumBytesBilled` has been explicitly set.
  package var hasMaximumBytesBilled: Bool {return _storage._maximumBytesBilled != nil}
  /// Clears the value of `maximumBytesBilled`. Subsequent reads from it will return its default value.
  package mutating func clearMaximumBytesBilled() {_uniqueStorage()._maximumBytesBilled = nil}

  /// Optional. Specifies whether to use BigQuery's legacy SQL dialect for this
  /// query. The default value is true. If set to false, the query will use
  /// BigQuery's GoogleSQL:
  /// https://cloud.google.com/bigquery/sql-reference/
  ///
  /// When useLegacySql is set to false, the value of flattenResults is ignored;
  /// query will be run as if flattenResults is false.
  package var useLegacySql: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._useLegacySql ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._useLegacySql = newValue}
  }
  /// Returns true if `useLegacySql` has been explicitly set.
  package var hasUseLegacySql: Bool {return _storage._useLegacySql != nil}
  /// Clears the value of `useLegacySql`. Subsequent reads from it will return its default value.
  package mutating func clearUseLegacySql() {_uniqueStorage()._useLegacySql = nil}

  /// GoogleSQL only. Set to POSITIONAL to use positional (?) query parameters
  /// or to NAMED to use named (@myparam) query parameters in this query.
  package var parameterMode: String {
    get {return _storage._parameterMode}
    set {_uniqueStorage()._parameterMode = newValue}
  }

  /// Query parameters for GoogleSQL queries.
  package var queryParameters: [Google_Cloud_Bigquery_V2_QueryParameter] {
    get {return _storage._queryParameters}
    set {_uniqueStorage()._queryParameters = newValue}
  }

  /// Output only. System variables for GoogleSQL queries. A system variable is
  /// output if the variable is settable and its value differs from the system
  /// default.
  /// "@@" prefix is not included in the name of the System variables.
  package var systemVariables: Google_Cloud_Bigquery_V2_SystemVariables {
    get {return _storage._systemVariables ?? Google_Cloud_Bigquery_V2_SystemVariables()}
    set {_uniqueStorage()._systemVariables = newValue}
  }
  /// Returns true if `systemVariables` has been explicitly set.
  package var hasSystemVariables: Bool {return _storage._systemVariables != nil}
  /// Clears the value of `systemVariables`. Subsequent reads from it will return its default value.
  package mutating func clearSystemVariables() {_uniqueStorage()._systemVariables = nil}

  /// Allows the schema of the destination table to be updated as a side effect
  /// of the query job. Schema update options are supported in two cases:
  /// when writeDisposition is WRITE_APPEND;
  /// when writeDisposition is WRITE_TRUNCATE and the destination table is a
  /// partition of a table, specified by partition decorators. For normal tables,
  /// WRITE_TRUNCATE will always overwrite the schema.
  /// One or more of the following values are specified:
  ///
  /// * ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
  /// * ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original
  /// schema to nullable.
  package var schemaUpdateOptions: [String] {
    get {return _storage._schemaUpdateOptions}
    set {_uniqueStorage()._schemaUpdateOptions = newValue}
  }

  /// Time-based partitioning specification for the destination table. Only one
  /// of timePartitioning and rangePartitioning should be specified.
  package var timePartitioning: Google_Cloud_Bigquery_V2_TimePartitioning {
    get {return _storage._timePartitioning ?? Google_Cloud_Bigquery_V2_TimePartitioning()}
    set {_uniqueStorage()._timePartitioning = newValue}
  }
  /// Returns true if `timePartitioning` has been explicitly set.
  package var hasTimePartitioning: Bool {return _storage._timePartitioning != nil}
  /// Clears the value of `timePartitioning`. Subsequent reads from it will return its default value.
  package mutating func clearTimePartitioning() {_uniqueStorage()._timePartitioning = nil}

  /// Range partitioning specification for the destination table.
  /// Only one of timePartitioning and rangePartitioning should be specified.
  package var rangePartitioning: Google_Cloud_Bigquery_V2_RangePartitioning {
    get {return _storage._rangePartitioning ?? Google_Cloud_Bigquery_V2_RangePartitioning()}
    set {_uniqueStorage()._rangePartitioning = newValue}
  }
  /// Returns true if `rangePartitioning` has been explicitly set.
  package var hasRangePartitioning: Bool {return _storage._rangePartitioning != nil}
  /// Clears the value of `rangePartitioning`. Subsequent reads from it will return its default value.
  package mutating func clearRangePartitioning() {_uniqueStorage()._rangePartitioning = nil}

  /// Clustering specification for the destination table.
  package var clustering: Google_Cloud_Bigquery_V2_Clustering {
    get {return _storage._clustering ?? Google_Cloud_Bigquery_V2_Clustering()}
    set {_uniqueStorage()._clustering = newValue}
  }
  /// Returns true if `clustering` has been explicitly set.
  package var hasClustering: Bool {return _storage._clustering != nil}
  /// Clears the value of `clustering`. Subsequent reads from it will return its default value.
  package mutating func clearClustering() {_uniqueStorage()._clustering = nil}

  /// Custom encryption configuration (e.g., Cloud KMS keys)
  package var destinationEncryptionConfiguration: Google_Cloud_Bigquery_V2_EncryptionConfiguration {
    get {return _storage._destinationEncryptionConfiguration ?? Google_Cloud_Bigquery_V2_EncryptionConfiguration()}
    set {_uniqueStorage()._destinationEncryptionConfiguration = newValue}
  }
  /// Returns true if `destinationEncryptionConfiguration` has been explicitly set.
  package var hasDestinationEncryptionConfiguration: Bool {return _storage._destinationEncryptionConfiguration != nil}
  /// Clears the value of `destinationEncryptionConfiguration`. Subsequent reads from it will return its default value.
  package mutating func clearDestinationEncryptionConfiguration() {_uniqueStorage()._destinationEncryptionConfiguration = nil}

  /// Options controlling the execution of scripts.
  package var scriptOptions: Google_Cloud_Bigquery_V2_ScriptOptions {
    get {return _storage._scriptOptions ?? Google_Cloud_Bigquery_V2_ScriptOptions()}
    set {_uniqueStorage()._scriptOptions = newValue}
  }
  /// Returns true if `scriptOptions` has been explicitly set.
  package var hasScriptOptions: Bool {return _storage._scriptOptions != nil}
  /// Clears the value of `scriptOptions`. Subsequent reads from it will return its default value.
  package mutating func clearScriptOptions() {_uniqueStorage()._scriptOptions = nil}

  /// Connection properties which can modify the query behavior.
  package var connectionProperties: [Google_Cloud_Bigquery_V2_ConnectionProperty] {
    get {return _storage._connectionProperties}
    set {_uniqueStorage()._connectionProperties = newValue}
  }

  /// If this property is true, the job creates a new session using a randomly
  /// generated session_id.  To continue using a created session with
  /// subsequent queries, pass the existing session identifier as a
  /// `ConnectionProperty` value.  The session identifier is returned as part of
  /// the `SessionInfo` message within the query statistics.
  ///
  /// The new session's location will be set to `Job.JobReference.location` if it
  /// is present, otherwise it's set to the default location based on existing
  /// routing logic.
  package var createSession: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._createSession ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._createSession = newValue}
  }
  /// Returns true if `createSession` has been explicitly set.
  package var hasCreateSession: Bool {return _storage._createSession != nil}
  /// Clears the value of `createSession`. Subsequent reads from it will return its default value.
  package mutating func clearCreateSession() {_uniqueStorage()._createSession = nil}

  /// Optional. Whether to run the query as continuous or a regular query.
  /// Continuous query is currently in experimental stage and not ready for
  /// general usage.
  package var continuous: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._continuous ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._continuous = newValue}
  }
  /// Returns true if `continuous` has been explicitly set.
  package var hasContinuous: Bool {return _storage._continuous != nil}
  /// Clears the value of `continuous`. Subsequent reads from it will return its default value.
  package mutating func clearContinuous() {_uniqueStorage()._continuous = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// Options related to script execution.
package struct Google_Cloud_Bigquery_V2_ScriptOptions: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Timeout period for each statement in a script.
  package var statementTimeoutMs: SwiftProtobuf.Google_Protobuf_Int64Value {
    get {return _statementTimeoutMs ?? SwiftProtobuf.Google_Protobuf_Int64Value()}
    set {_statementTimeoutMs = newValue}
  }
  /// Returns true if `statementTimeoutMs` has been explicitly set.
  package var hasStatementTimeoutMs: Bool {return self._statementTimeoutMs != nil}
  /// Clears the value of `statementTimeoutMs`. Subsequent reads from it will return its default value.
  package mutating func clearStatementTimeoutMs() {self._statementTimeoutMs = nil}

  /// Limit on the number of bytes billed per statement. Exceeding this budget
  /// results in an error.
  package var statementByteBudget: SwiftProtobuf.Google_Protobuf_Int64Value {
    get {return _statementByteBudget ?? SwiftProtobuf.Google_Protobuf_Int64Value()}
    set {_statementByteBudget = newValue}
  }
  /// Returns true if `statementByteBudget` has been explicitly set.
  package var hasStatementByteBudget: Bool {return self._statementByteBudget != nil}
  /// Clears the value of `statementByteBudget`. Subsequent reads from it will return its default value.
  package mutating func clearStatementByteBudget() {self._statementByteBudget = nil}

  /// Determines which statement in the script represents the "key result",
  /// used to populate the schema and query results of the script job.
  /// Default is LAST.
  package var keyResultStatement: Google_Cloud_Bigquery_V2_ScriptOptions.KeyResultStatementKind = .unspecified

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  /// KeyResultStatementKind controls how the key result is determined.
  package enum KeyResultStatementKind: SwiftProtobuf.Enum, Swift.CaseIterable {
    package typealias RawValue = Int

    /// Default value.
    case unspecified // = 0

    /// The last result determines the key result.
    case last // = 1

    /// The first SELECT statement determines the key result.
    case firstSelect // = 2
    case UNRECOGNIZED(Int)

    package init() {
      self = .unspecified
    }

    package init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .last
      case 2: self = .firstSelect
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    package var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .last: return 1
      case .firstSelect: return 2
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    package static let allCases: [Google_Cloud_Bigquery_V2_ScriptOptions.KeyResultStatementKind] = [
      .unspecified,
      .last,
      .firstSelect,
    ]

  }

  package init() {}

  fileprivate var _statementTimeoutMs: SwiftProtobuf.Google_Protobuf_Int64Value? = nil
  fileprivate var _statementByteBudget: SwiftProtobuf.Google_Protobuf_Int64Value? = nil
}

/// JobConfigurationLoad contains the configuration properties for loading data
/// into a destination table.
package struct Google_Cloud_Bigquery_V2_JobConfigurationLoad: @unchecked Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// [Required] The fully-qualified URIs that point to your data in Google
  /// Cloud.
  /// For Google Cloud Storage URIs:
  ///   Each URI can contain one '*' wildcard character and it must come after
  ///   the 'bucket' name. Size limits related to load jobs apply to external
  ///   data sources.
  /// For Google Cloud Bigtable URIs:
  ///   Exactly one URI can be specified and it has be a fully specified and
  ///   valid HTTPS URL for a Google Cloud Bigtable table.
  /// For Google Cloud Datastore backups:
  ///  Exactly one URI can be specified. Also, the '*' wildcard character is not
  ///  allowed.
  package var sourceUris: [String] {
    get {return _storage._sourceUris}
    set {_uniqueStorage()._sourceUris = newValue}
  }

  /// Optional. Specifies how source URIs are interpreted for constructing the
  /// file set to load. By default, source URIs are expanded against the
  /// underlying storage. You can also specify manifest files to control how the
  /// file set is constructed. This option is only applicable to object storage
  /// systems.
  package var fileSetSpecType: Google_Cloud_Bigquery_V2_FileSetSpecType {
    get {return _storage._fileSetSpecType}
    set {_uniqueStorage()._fileSetSpecType = newValue}
  }

  /// Optional. The schema for the destination table. The schema can be
  /// omitted if the destination table already exists, or if you're loading data
  /// from Google Cloud Datastore.
  package var schema: Google_Cloud_Bigquery_V2_TableSchema {
    get {return _storage._schema ?? Google_Cloud_Bigquery_V2_TableSchema()}
    set {_uniqueStorage()._schema = newValue}
  }
  /// Returns true if `schema` has been explicitly set.
  package var hasSchema: Bool {return _storage._schema != nil}
  /// Clears the value of `schema`. Subsequent reads from it will return its default value.
  package mutating func clearSchema() {_uniqueStorage()._schema = nil}

  /// [Required] The destination table to load the data into.
  package var destinationTable: Google_Cloud_Bigquery_V2_TableReference {
    get {return _storage._destinationTable ?? Google_Cloud_Bigquery_V2_TableReference()}
    set {_uniqueStorage()._destinationTable = newValue}
  }
  /// Returns true if `destinationTable` has been explicitly set.
  package var hasDestinationTable: Bool {return _storage._destinationTable != nil}
  /// Clears the value of `destinationTable`. Subsequent reads from it will return its default value.
  package mutating func clearDestinationTable() {_uniqueStorage()._destinationTable = nil}

  /// Optional. [Experimental] Properties with which to create the destination
  /// table if it is new.
  package var destinationTableProperties: Google_Cloud_Bigquery_V2_DestinationTableProperties {
    get {return _storage._destinationTableProperties ?? Google_Cloud_Bigquery_V2_DestinationTableProperties()}
    set {_uniqueStorage()._destinationTableProperties = newValue}
  }
  /// Returns true if `destinationTableProperties` has been explicitly set.
  package var hasDestinationTableProperties: Bool {return _storage._destinationTableProperties != nil}
  /// Clears the value of `destinationTableProperties`. Subsequent reads from it will return its default value.
  package mutating func clearDestinationTableProperties() {_uniqueStorage()._destinationTableProperties = nil}

  /// Optional. Specifies whether the job is allowed to create new tables.
  /// The following values are supported:
  ///
  /// * CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the
  /// table.
  /// * CREATE_NEVER: The table must already exist. If it does not,
  /// a 'notFound' error is returned in the job result.
  /// The default value is CREATE_IF_NEEDED.
  /// Creation, truncation and append actions occur as one atomic update
  /// upon job completion.
  package var createDisposition: String {
    get {return _storage._createDisposition}
    set {_uniqueStorage()._createDisposition = newValue}
  }

  /// Optional. Specifies the action that occurs if the destination table
  /// already exists. The following values are supported:
  ///
  /// * WRITE_TRUNCATE:  If the table already exists, BigQuery overwrites the
  /// data, removes the constraints and uses the schema from the load job.
  /// * WRITE_APPEND: If the table already exists, BigQuery appends the data to
  /// the table.
  /// * WRITE_EMPTY: If the table already exists and contains data, a 'duplicate'
  /// error is returned in the job result.
  ///
  /// The default value is WRITE_APPEND.
  /// Each action is atomic and only occurs if BigQuery is able to complete the
  /// job successfully.
  /// Creation, truncation and append actions occur as one atomic update
  /// upon job completion.
  package var writeDisposition: String {
    get {return _storage._writeDisposition}
    set {_uniqueStorage()._writeDisposition = newValue}
  }

  /// Optional. Specifies a string that represents a null value in a CSV file.
  /// For example, if you specify "\N", BigQuery interprets "\N" as a null value
  /// when loading a CSV file.
  /// The default value is the empty string. If you set this property to a custom
  /// value, BigQuery throws an error if an empty string is present for all data
  /// types except for STRING and BYTE. For STRING and BYTE columns, BigQuery
  /// interprets the empty string as an empty value.
  package var nullMarker: SwiftProtobuf.Google_Protobuf_StringValue {
    get {return _storage._nullMarker ?? SwiftProtobuf.Google_Protobuf_StringValue()}
    set {_uniqueStorage()._nullMarker = newValue}
  }
  /// Returns true if `nullMarker` has been explicitly set.
  package var hasNullMarker: Bool {return _storage._nullMarker != nil}
  /// Clears the value of `nullMarker`. Subsequent reads from it will return its default value.
  package mutating func clearNullMarker() {_uniqueStorage()._nullMarker = nil}

  /// Optional. The separator character for fields in a CSV file. The separator
  /// is interpreted as a single byte. For files encoded in ISO-8859-1, any
  /// single character can be used as a separator. For files encoded in UTF-8,
  /// characters represented in decimal range 1-127 (U+0001-U+007F) can be used
  /// without any modification. UTF-8 characters encoded with multiple bytes
  /// (i.e. U+0080 and above) will have only the first byte used for separating
  /// fields. The remaining bytes will be treated as a part of the field.
  /// BigQuery also supports the escape sequence "\t" (U+0009) to specify a tab
  /// separator. The default value is comma (",", U+002C).
  package var fieldDelimiter: String {
    get {return _storage._fieldDelimiter}
    set {_uniqueStorage()._fieldDelimiter = newValue}
  }

  /// Optional. The number of rows at the top of a CSV file that BigQuery will
  /// skip when loading the data. The default value is 0. This property is useful
  /// if you have header rows in the file that should be skipped. When autodetect
  /// is on, the behavior is the following:
  ///
  /// * skipLeadingRows unspecified - Autodetect tries to detect headers in the
  ///   first row. If they are not detected, the row is read as data. Otherwise
  ///   data is read starting from the second row.
  /// * skipLeadingRows is 0 - Instructs autodetect that there are no headers and
  ///   data should be read starting from the first row.
  /// * skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect
  ///   headers in row N. If headers are not detected, row N is just skipped.
  ///   Otherwise row N is used to extract column names for the detected schema.
  package var skipLeadingRows: SwiftProtobuf.Google_Protobuf_Int32Value {
    get {return _storage._skipLeadingRows ?? SwiftProtobuf.Google_Protobuf_Int32Value()}
    set {_uniqueStorage()._skipLeadingRows = newValue}
  }
  /// Returns true if `skipLeadingRows` has been explicitly set.
  package var hasSkipLeadingRows: Bool {return _storage._skipLeadingRows != nil}
  /// Clears the value of `skipLeadingRows`. Subsequent reads from it will return its default value.
  package mutating func clearSkipLeadingRows() {_uniqueStorage()._skipLeadingRows = nil}

  /// Optional. The character encoding of the data.
  /// The supported values are UTF-8, ISO-8859-1, UTF-16BE, UTF-16LE, UTF-32BE,
  /// and UTF-32LE. The default value is UTF-8. BigQuery decodes the data after
  /// the raw, binary data has been split using the values of the `quote` and
  /// `fieldDelimiter` properties.
  ///
  /// If you don't specify an encoding, or if you specify a UTF-8 encoding when
  /// the CSV file is not UTF-8 encoded, BigQuery attempts to convert the data to
  /// UTF-8. Generally, your data loads successfully, but it may not match
  /// byte-for-byte what you expect. To avoid this, specify the correct encoding
  /// by using the `--encoding` flag.
  ///
  /// If BigQuery can't convert a character other than the ASCII `0` character,
  /// BigQuery converts the character to the standard Unicode replacement
  /// character: &#65533;.
  package var encoding: String {
    get {return _storage._encoding}
    set {_uniqueStorage()._encoding = newValue}
  }

  /// Optional. The value that is used to quote data sections in a CSV file.
  /// BigQuery converts the string to ISO-8859-1 encoding, and then uses the
  /// first byte of the encoded string to split the data in its raw, binary
  /// state.
  /// The default value is a double-quote ('"').
  /// If your data does not contain quoted sections, set the property value to an
  /// empty string.
  /// If your data contains quoted newline characters, you must also set the
  /// allowQuotedNewlines property to true.
  /// To include the specific quote character within a quoted value, precede it
  /// with an additional matching quote character. For example, if you want to
  /// escape the default character  ' " ', use ' "" '.
  /// @default "
  package var quote: SwiftProtobuf.Google_Protobuf_StringValue {
    get {return _storage._quote ?? SwiftProtobuf.Google_Protobuf_StringValue()}
    set {_uniqueStorage()._quote = newValue}
  }
  /// Returns true if `quote` has been explicitly set.
  package var hasQuote: Bool {return _storage._quote != nil}
  /// Clears the value of `quote`. Subsequent reads from it will return its default value.
  package mutating func clearQuote() {_uniqueStorage()._quote = nil}

  /// Optional. The maximum number of bad records that BigQuery can ignore when
  /// running the job. If the number of bad records exceeds this value, an
  /// invalid error is returned in the job result.
  /// The default value is 0, which requires that all records are valid.
  /// This is only supported for CSV and NEWLINE_DELIMITED_JSON file formats.
  package var maxBadRecords: SwiftProtobuf.Google_Protobuf_Int32Value {
    get {return _storage._maxBadRecords ?? SwiftProtobuf.Google_Protobuf_Int32Value()}
    set {_uniqueStorage()._maxBadRecords = newValue}
  }
  /// Returns true if `maxBadRecords` has been explicitly set.
  package var hasMaxBadRecords: Bool {return _storage._maxBadRecords != nil}
  /// Clears the value of `maxBadRecords`. Subsequent reads from it will return its default value.
  package mutating func clearMaxBadRecords() {_uniqueStorage()._maxBadRecords = nil}

  /// Indicates if BigQuery should allow quoted data sections that contain
  /// newline characters in a CSV file. The default value is false.
  package var allowQuotedNewlines: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._allowQuotedNewlines ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._allowQuotedNewlines = newValue}
  }
  /// Returns true if `allowQuotedNewlines` has been explicitly set.
  package var hasAllowQuotedNewlines: Bool {return _storage._allowQuotedNewlines != nil}
  /// Clears the value of `allowQuotedNewlines`. Subsequent reads from it will return its default value.
  package mutating func clearAllowQuotedNewlines() {_uniqueStorage()._allowQuotedNewlines = nil}

  /// Optional. The format of the data files.
  /// For CSV files, specify "CSV". For datastore backups,
  /// specify "DATASTORE_BACKUP". For newline-delimited JSON,
  /// specify "NEWLINE_DELIMITED_JSON". For Avro, specify "AVRO".
  /// For parquet, specify "PARQUET". For orc, specify "ORC".
  /// The default value is CSV.
  package var sourceFormat: String {
    get {return _storage._sourceFormat}
    set {_uniqueStorage()._sourceFormat = newValue}
  }

  /// Optional. Accept rows that are missing trailing optional columns.
  /// The missing values are treated as nulls.
  /// If false, records with missing trailing columns are treated as bad records,
  /// and if there are too many bad records, an invalid error is returned in the
  /// job result.
  /// The default value is false.
  /// Only applicable to CSV, ignored for other formats.
  package var allowJaggedRows: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._allowJaggedRows ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._allowJaggedRows = newValue}
  }
  /// Returns true if `allowJaggedRows` has been explicitly set.
  package var hasAllowJaggedRows: Bool {return _storage._allowJaggedRows != nil}
  /// Clears the value of `allowJaggedRows`. Subsequent reads from it will return its default value.
  package mutating func clearAllowJaggedRows() {_uniqueStorage()._allowJaggedRows = nil}

  /// Optional. Indicates if BigQuery should allow extra values that are not
  /// represented in the table schema.
  /// If true, the extra values are ignored.
  /// If false, records with extra columns are treated as bad records, and if
  /// there are too many bad records, an invalid error is returned in the job
  /// result. The default value is false.
  /// The sourceFormat property determines what BigQuery treats as an extra
  /// value:
  ///   CSV: Trailing columns
  ///   JSON: Named values that don't match any column names in the table schema
  ///   Avro, Parquet, ORC: Fields in the file schema that don't exist in the
  ///   table schema.
  package var ignoreUnknownValues: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._ignoreUnknownValues ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._ignoreUnknownValues = newValue}
  }
  /// Returns true if `ignoreUnknownValues` has been explicitly set.
  package var hasIgnoreUnknownValues: Bool {return _storage._ignoreUnknownValues != nil}
  /// Clears the value of `ignoreUnknownValues`. Subsequent reads from it will return its default value.
  package mutating func clearIgnoreUnknownValues() {_uniqueStorage()._ignoreUnknownValues = nil}

  /// If sourceFormat is set to "DATASTORE_BACKUP", indicates which entity
  /// properties to load into BigQuery from a Cloud Datastore backup. Property
  /// names are case sensitive and must be top-level properties. If no properties
  /// are specified, BigQuery loads all properties. If any named property isn't
  /// found in the Cloud Datastore backup, an invalid error is returned in the
  /// job result.
  package var projectionFields: [String] {
    get {return _storage._projectionFields}
    set {_uniqueStorage()._projectionFields = newValue}
  }

  /// Optional. Indicates if we should automatically infer the options and
  /// schema for CSV and JSON sources.
  package var autodetect: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._autodetect ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._autodetect = newValue}
  }
  /// Returns true if `autodetect` has been explicitly set.
  package var hasAutodetect: Bool {return _storage._autodetect != nil}
  /// Clears the value of `autodetect`. Subsequent reads from it will return its default value.
  package mutating func clearAutodetect() {_uniqueStorage()._autodetect = nil}

  /// Allows the schema of the destination table to be updated as a side effect
  /// of the load job if a schema is autodetected or supplied in the job
  /// configuration.
  /// Schema update options are supported in two cases:
  /// when writeDisposition is WRITE_APPEND;
  /// when writeDisposition is WRITE_TRUNCATE and the destination table is a
  /// partition of a table, specified by partition decorators. For normal tables,
  /// WRITE_TRUNCATE will always overwrite the schema.
  /// One or more of the following values are specified:
  ///
  /// * ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
  /// * ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original
  /// schema to nullable.
  package var schemaUpdateOptions: [String] {
    get {return _storage._schemaUpdateOptions}
    set {_uniqueStorage()._schemaUpdateOptions = newValue}
  }

  /// Time-based partitioning specification for the destination table. Only one
  /// of timePartitioning and rangePartitioning should be specified.
  package var timePartitioning: Google_Cloud_Bigquery_V2_TimePartitioning {
    get {return _storage._timePartitioning ?? Google_Cloud_Bigquery_V2_TimePartitioning()}
    set {_uniqueStorage()._timePartitioning = newValue}
  }
  /// Returns true if `timePartitioning` has been explicitly set.
  package var hasTimePartitioning: Bool {return _storage._timePartitioning != nil}
  /// Clears the value of `timePartitioning`. Subsequent reads from it will return its default value.
  package mutating func clearTimePartitioning() {_uniqueStorage()._timePartitioning = nil}

  /// Range partitioning specification for the destination table.
  /// Only one of timePartitioning and rangePartitioning should be specified.
  package var rangePartitioning: Google_Cloud_Bigquery_V2_RangePartitioning {
    get {return _storage._rangePartitioning ?? Google_Cloud_Bigquery_V2_RangePartitioning()}
    set {_uniqueStorage()._rangePartitioning = newValue}
  }
  /// Returns true if `rangePartitioning` has been explicitly set.
  package var hasRangePartitioning: Bool {return _storage._rangePartitioning != nil}
  /// Clears the value of `rangePartitioning`. Subsequent reads from it will return its default value.
  package mutating func clearRangePartitioning() {_uniqueStorage()._rangePartitioning = nil}

  /// Clustering specification for the destination table.
  package var clustering: Google_Cloud_Bigquery_V2_Clustering {
    get {return _storage._clustering ?? Google_Cloud_Bigquery_V2_Clustering()}
    set {_uniqueStorage()._clustering = newValue}
  }
  /// Returns true if `clustering` has been explicitly set.
  package var hasClustering: Bool {return _storage._clustering != nil}
  /// Clears the value of `clustering`. Subsequent reads from it will return its default value.
  package mutating func clearClustering() {_uniqueStorage()._clustering = nil}

  /// Custom encryption configuration (e.g., Cloud KMS keys)
  package var destinationEncryptionConfiguration: Google_Cloud_Bigquery_V2_EncryptionConfiguration {
    get {return _storage._destinationEncryptionConfiguration ?? Google_Cloud_Bigquery_V2_EncryptionConfiguration()}
    set {_uniqueStorage()._destinationEncryptionConfiguration = newValue}
  }
  /// Returns true if `destinationEncryptionConfiguration` has been explicitly set.
  package var hasDestinationEncryptionConfiguration: Bool {return _storage._destinationEncryptionConfiguration != nil}
  /// Clears the value of `destinationEncryptionConfiguration`. Subsequent reads from it will return its default value.
  package mutating func clearDestinationEncryptionConfiguration() {_uniqueStorage()._destinationEncryptionConfiguration = nil}

  /// Optional. If sourceFormat is set to "AVRO", indicates whether to interpret
  /// logical types as the corresponding BigQuery data type (for example,
  /// TIMESTAMP), instead of using the raw type (for example, INTEGER).
  package var useAvroLogicalTypes: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._useAvroLogicalTypes ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._useAvroLogicalTypes = newValue}
  }
  /// Returns true if `useAvroLogicalTypes` has been explicitly set.
  package var hasUseAvroLogicalTypes: Bool {return _storage._useAvroLogicalTypes != nil}
  /// Clears the value of `useAvroLogicalTypes`. Subsequent reads from it will return its default value.
  package mutating func clearUseAvroLogicalTypes() {_uniqueStorage()._useAvroLogicalTypes = nil}

  /// Optional. The user can provide a reference file with the reader schema.
  /// This file is only loaded if it is part of source URIs, but is not loaded
  /// otherwise. It is enabled for the following formats: AVRO, PARQUET, ORC.
  package var referenceFileSchemaUri: SwiftProtobuf.Google_Protobuf_StringValue {
    get {return _storage._referenceFileSchemaUri ?? SwiftProtobuf.Google_Protobuf_StringValue()}
    set {_uniqueStorage()._referenceFileSchemaUri = newValue}
  }
  /// Returns true if `referenceFileSchemaUri` has been explicitly set.
  package var hasReferenceFileSchemaUri: Bool {return _storage._referenceFileSchemaUri != nil}
  /// Clears the value of `referenceFileSchemaUri`. Subsequent reads from it will return its default value.
  package mutating func clearReferenceFileSchemaUri() {_uniqueStorage()._referenceFileSchemaUri = nil}

  /// Optional. When set, configures hive partitioning support.
  /// Not all storage formats support hive partitioning -- requesting hive
  /// partitioning on an unsupported format will lead to an error, as will
  /// providing an invalid specification.
  package var hivePartitioningOptions: Google_Cloud_Bigquery_V2_HivePartitioningOptions {
    get {return _storage._hivePartitioningOptions ?? Google_Cloud_Bigquery_V2_HivePartitioningOptions()}
    set {_uniqueStorage()._hivePartitioningOptions = newValue}
  }
  /// Returns true if `hivePartitioningOptions` has been explicitly set.
  package var hasHivePartitioningOptions: Bool {return _storage._hivePartitioningOptions != nil}
  /// Clears the value of `hivePartitioningOptions`. Subsequent reads from it will return its default value.
  package mutating func clearHivePartitioningOptions() {_uniqueStorage()._hivePartitioningOptions = nil}

  /// Defines the list of possible SQL data types to which the source decimal
  /// values are converted. This list and the precision and the scale parameters
  /// of the decimal field determine the target type. In the order of NUMERIC,
  /// BIGNUMERIC, and STRING, a
  /// type is picked if it is in the specified list and if it supports the
  /// precision and the scale. STRING supports all precision and scale values.
  /// If none of the listed types supports the precision and the scale, the type
  /// supporting the widest range in the specified list is picked, and if a value
  /// exceeds the supported range when reading the data, an error will be thrown.
  ///
  /// Example: Suppose the value of this field is ["NUMERIC", "BIGNUMERIC"].
  /// If (precision,scale) is:
  ///
  /// * (38,9) -> NUMERIC;
  /// * (39,9) -> BIGNUMERIC (NUMERIC cannot hold 30 integer digits);
  /// * (38,10) -> BIGNUMERIC (NUMERIC cannot hold 10 fractional digits);
  /// * (76,38) -> BIGNUMERIC;
  /// * (77,38) -> BIGNUMERIC (error if value exeeds supported range).
  ///
  /// This field cannot contain duplicate types. The order of the types in this
  /// field is ignored. For example, ["BIGNUMERIC", "NUMERIC"] is the same as
  /// ["NUMERIC", "BIGNUMERIC"] and NUMERIC always takes precedence over
  /// BIGNUMERIC.
  ///
  /// Defaults to ["NUMERIC", "STRING"] for ORC and ["NUMERIC"] for the other
  /// file formats.
  package var decimalTargetTypes: [Google_Cloud_Bigquery_V2_DecimalTargetType] {
    get {return _storage._decimalTargetTypes}
    set {_uniqueStorage()._decimalTargetTypes = newValue}
  }

  /// Optional. Load option to be used together with source_format
  /// newline-delimited JSON to indicate that a variant of JSON is being loaded.
  /// To load newline-delimited GeoJSON, specify GEOJSON (and source_format must
  /// be set to NEWLINE_DELIMITED_JSON).
  package var jsonExtension: Google_Cloud_Bigquery_V2_JsonExtension {
    get {return _storage._jsonExtension}
    set {_uniqueStorage()._jsonExtension = newValue}
  }

  /// Optional. Additional properties to set if sourceFormat is set to PARQUET.
  package var parquetOptions: Google_Cloud_Bigquery_V2_ParquetOptions {
    get {return _storage._parquetOptions ?? Google_Cloud_Bigquery_V2_ParquetOptions()}
    set {_uniqueStorage()._parquetOptions = newValue}
  }
  /// Returns true if `parquetOptions` has been explicitly set.
  package var hasParquetOptions: Bool {return _storage._parquetOptions != nil}
  /// Clears the value of `parquetOptions`. Subsequent reads from it will return its default value.
  package mutating func clearParquetOptions() {_uniqueStorage()._parquetOptions = nil}

  /// Optional. When sourceFormat is set to "CSV", this indicates whether the
  /// embedded ASCII control characters (the first 32 characters in the
  /// ASCII-table, from
  /// '\x00' to '\x1F') are preserved.
  package var preserveAsciiControlCharacters: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._preserveAsciiControlCharacters ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._preserveAsciiControlCharacters = newValue}
  }
  /// Returns true if `preserveAsciiControlCharacters` has been explicitly set.
  package var hasPreserveAsciiControlCharacters: Bool {return _storage._preserveAsciiControlCharacters != nil}
  /// Clears the value of `preserveAsciiControlCharacters`. Subsequent reads from it will return its default value.
  package mutating func clearPreserveAsciiControlCharacters() {_uniqueStorage()._preserveAsciiControlCharacters = nil}

  /// Optional. Connection properties which can modify the load job behavior.
  /// Currently, only the 'session_id' connection property is supported, and is
  /// used to resolve _SESSION appearing as the dataset id.
  package var connectionProperties: [Google_Cloud_Bigquery_V2_ConnectionProperty] {
    get {return _storage._connectionProperties}
    set {_uniqueStorage()._connectionProperties = newValue}
  }

  /// Optional. If this property is true, the job creates a new session using a
  /// randomly generated session_id.  To continue using a created session with
  /// subsequent queries, pass the existing session identifier as a
  /// `ConnectionProperty` value.  The session identifier is returned as part of
  /// the `SessionInfo` message within the query statistics.
  ///
  /// The new session's location will be set to `Job.JobReference.location` if it
  /// is present, otherwise it's set to the default location based on existing
  /// routing logic.
  package var createSession: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._createSession ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._createSession = newValue}
  }
  /// Returns true if `createSession` has been explicitly set.
  package var hasCreateSession: Bool {return _storage._createSession != nil}
  /// Clears the value of `createSession`. Subsequent reads from it will return its default value.
  package mutating func clearCreateSession() {_uniqueStorage()._createSession = nil}

  /// Optional. Character map supported for column names in CSV/Parquet loads.
  /// Defaults to STRICT and can be overridden by Project Config Service. Using
  /// this option with unsupporting load formats will result in an error.
  package var columnNameCharacterMap: Google_Cloud_Bigquery_V2_JobConfigurationLoad.ColumnNameCharacterMap {
    get {return _storage._columnNameCharacterMap}
    set {_uniqueStorage()._columnNameCharacterMap = newValue}
  }

  /// Optional. [Experimental] Configures the load job to copy files directly to
  /// the destination BigLake managed table, bypassing file content reading and
  /// rewriting.
  ///
  /// Copying files only is supported when all the following are true:
  ///
  /// * `source_uris` are located in the same Cloud Storage location as the
  ///   destination table's `storage_uri` location.
  /// * `source_format` is `PARQUET`.
  /// * `destination_table` is an existing BigLake managed table. The table's
  ///   schema does not have flexible column names. The table's columns do not
  ///   have type parameters other than precision and scale.
  /// * No options other than the above are specified.
  package var copyFilesOnly: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._copyFilesOnly ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._copyFilesOnly = newValue}
  }
  /// Returns true if `copyFilesOnly` has been explicitly set.
  package var hasCopyFilesOnly: Bool {return _storage._copyFilesOnly != nil}
  /// Clears the value of `copyFilesOnly`. Subsequent reads from it will return its default value.
  package mutating func clearCopyFilesOnly() {_uniqueStorage()._copyFilesOnly = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Indicates the character map used for column names.
  package enum ColumnNameCharacterMap: SwiftProtobuf.Enum, Swift.CaseIterable {
    package typealias RawValue = Int

    /// Unspecified column name character map.
    case unspecified // = 0

    /// Support flexible column name and reject invalid column names.
    case strict // = 1

    /// Support alphanumeric + underscore characters and names must start with a
    /// letter or underscore. Invalid column names will be normalized.
    case v1 // = 2

    /// Support flexible column name. Invalid column names will be normalized.
    case v2 // = 3
    case UNRECOGNIZED(Int)

    package init() {
      self = .unspecified
    }

    package init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .strict
      case 2: self = .v1
      case 3: self = .v2
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    package var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .strict: return 1
      case .v1: return 2
      case .v2: return 3
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    package static let allCases: [Google_Cloud_Bigquery_V2_JobConfigurationLoad.ColumnNameCharacterMap] = [
      .unspecified,
      .strict,
      .v1,
      .v2,
    ]

  }

  package init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// JobConfigurationTableCopy configures a job that copies data from one table
/// to another.
/// For more information on copying tables, see [Copy a
///  table](https://cloud.google.com/bigquery/docs/managing-tables#copy-table).
package struct Google_Cloud_Bigquery_V2_JobConfigurationTableCopy: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// [Pick one] Source table to copy.
  package var sourceTable: Google_Cloud_Bigquery_V2_TableReference {
    get {return _sourceTable ?? Google_Cloud_Bigquery_V2_TableReference()}
    set {_sourceTable = newValue}
  }
  /// Returns true if `sourceTable` has been explicitly set.
  package var hasSourceTable: Bool {return self._sourceTable != nil}
  /// Clears the value of `sourceTable`. Subsequent reads from it will return its default value.
  package mutating func clearSourceTable() {self._sourceTable = nil}

  /// [Pick one] Source tables to copy.
  package var sourceTables: [Google_Cloud_Bigquery_V2_TableReference] = []

  /// [Required] The destination table.
  package var destinationTable: Google_Cloud_Bigquery_V2_TableReference {
    get {return _destinationTable ?? Google_Cloud_Bigquery_V2_TableReference()}
    set {_destinationTable = newValue}
  }
  /// Returns true if `destinationTable` has been explicitly set.
  package var hasDestinationTable: Bool {return self._destinationTable != nil}
  /// Clears the value of `destinationTable`. Subsequent reads from it will return its default value.
  package mutating func clearDestinationTable() {self._destinationTable = nil}

  /// Optional. Specifies whether the job is allowed to create new tables.
  /// The following values are supported:
  ///
  /// * CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the
  /// table.
  /// * CREATE_NEVER: The table must already exist. If it does not,
  /// a 'notFound' error is returned in the job result.
  ///
  /// The default value is CREATE_IF_NEEDED.
  /// Creation, truncation and append actions occur as one atomic update
  /// upon job completion.
  package var createDisposition: String = String()

  /// Optional. Specifies the action that occurs if the destination table
  /// already exists. The following values are supported:
  ///
  /// * WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the
  /// table data and uses the schema and table constraints from the source table.
  /// * WRITE_APPEND: If the table already exists, BigQuery appends the data to
  /// the table.
  /// * WRITE_EMPTY: If the table already exists and contains data, a 'duplicate'
  /// error is returned in the job result.
  ///
  /// The default value is WRITE_EMPTY. Each action is atomic and only occurs if
  /// BigQuery is able to complete the job successfully. Creation, truncation and
  /// append actions occur as one atomic update upon job completion.
  package var writeDisposition: String = String()

  /// Custom encryption configuration (e.g., Cloud KMS keys).
  package var destinationEncryptionConfiguration: Google_Cloud_Bigquery_V2_EncryptionConfiguration {
    get {return _destinationEncryptionConfiguration ?? Google_Cloud_Bigquery_V2_EncryptionConfiguration()}
    set {_destinationEncryptionConfiguration = newValue}
  }
  /// Returns true if `destinationEncryptionConfiguration` has been explicitly set.
  package var hasDestinationEncryptionConfiguration: Bool {return self._destinationEncryptionConfiguration != nil}
  /// Clears the value of `destinationEncryptionConfiguration`. Subsequent reads from it will return its default value.
  package mutating func clearDestinationEncryptionConfiguration() {self._destinationEncryptionConfiguration = nil}

  /// Optional. Supported operation types in table copy job.
  package var operationType: Google_Cloud_Bigquery_V2_JobConfigurationTableCopy.OperationType = .unspecified

  /// Optional. The time when the destination table expires. Expired tables will
  /// be deleted and their storage reclaimed.
  package var destinationExpirationTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _destinationExpirationTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_destinationExpirationTime = newValue}
  }
  /// Returns true if `destinationExpirationTime` has been explicitly set.
  package var hasDestinationExpirationTime: Bool {return self._destinationExpirationTime != nil}
  /// Clears the value of `destinationExpirationTime`. Subsequent reads from it will return its default value.
  package mutating func clearDestinationExpirationTime() {self._destinationExpirationTime = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Indicates different operation types supported in table copy job.
  package enum OperationType: SwiftProtobuf.Enum, Swift.CaseIterable {
    package typealias RawValue = Int

    /// Unspecified operation type.
    case unspecified // = 0

    /// The source and destination table have the same table type.
    case copy // = 1

    /// The source table type is TABLE and
    /// the destination table type is SNAPSHOT.
    case snapshot // = 2

    /// The source table type is SNAPSHOT and
    /// the destination table type is TABLE.
    case restore // = 3

    /// The source and destination table have the same table type,
    /// but only bill for unique data.
    case clone // = 4
    case UNRECOGNIZED(Int)

    package init() {
      self = .unspecified
    }

    package init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .copy
      case 2: self = .snapshot
      case 3: self = .restore
      case 4: self = .clone
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    package var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .copy: return 1
      case .snapshot: return 2
      case .restore: return 3
      case .clone: return 4
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    package static let allCases: [Google_Cloud_Bigquery_V2_JobConfigurationTableCopy.OperationType] = [
      .unspecified,
      .copy,
      .snapshot,
      .restore,
      .clone,
    ]

  }

  package init() {}

  fileprivate var _sourceTable: Google_Cloud_Bigquery_V2_TableReference? = nil
  fileprivate var _destinationTable: Google_Cloud_Bigquery_V2_TableReference? = nil
  fileprivate var _destinationEncryptionConfiguration: Google_Cloud_Bigquery_V2_EncryptionConfiguration? = nil
  fileprivate var _destinationExpirationTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
}

/// JobConfigurationExtract configures a job that exports data from a BigQuery
/// table into Google Cloud Storage.
package struct Google_Cloud_Bigquery_V2_JobConfigurationExtract: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Source reference for the export.
  package var source: Google_Cloud_Bigquery_V2_JobConfigurationExtract.OneOf_Source? = nil

  /// A reference to the table being exported.
  package var sourceTable: Google_Cloud_Bigquery_V2_TableReference {
    get {
      if case .sourceTable(let v)? = source {return v}
      return Google_Cloud_Bigquery_V2_TableReference()
    }
    set {source = .sourceTable(newValue)}
  }

  /// A reference to the model being exported.
  package var sourceModel: Google_Cloud_Bigquery_V2_ModelReference {
    get {
      if case .sourceModel(let v)? = source {return v}
      return Google_Cloud_Bigquery_V2_ModelReference()
    }
    set {source = .sourceModel(newValue)}
  }

  /// [Pick one] A list of fully-qualified Google Cloud Storage URIs where the
  /// extracted table should be written.
  package var destinationUris: [String] = []

  /// Optional. Whether to print out a header row in the results.
  /// Default is true. Not applicable when extracting models.
  package var printHeader: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _printHeader ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_printHeader = newValue}
  }
  /// Returns true if `printHeader` has been explicitly set.
  package var hasPrintHeader: Bool {return self._printHeader != nil}
  /// Clears the value of `printHeader`. Subsequent reads from it will return its default value.
  package mutating func clearPrintHeader() {self._printHeader = nil}

  /// Optional. When extracting data in CSV format, this defines the
  /// delimiter to use between fields in the exported data.
  /// Default is ','. Not applicable when extracting models.
  package var fieldDelimiter: String = String()

  /// Optional. The exported file format. Possible values include CSV,
  /// NEWLINE_DELIMITED_JSON, PARQUET, or AVRO for tables and ML_TF_SAVED_MODEL
  /// or ML_XGBOOST_BOOSTER for models. The default value for tables is CSV.
  /// Tables with nested or repeated fields cannot be exported as CSV. The
  /// default value for models is ML_TF_SAVED_MODEL.
  package var destinationFormat: String = String()

  /// Optional. The compression type to use for exported files. Possible values
  /// include DEFLATE, GZIP, NONE, SNAPPY, and ZSTD. The default value is NONE.
  /// Not all compression formats are support for all file formats. DEFLATE is
  /// only supported for Avro. ZSTD is only supported for Parquet. Not applicable
  /// when extracting models.
  package var compression: String = String()

  /// Whether to use logical types when extracting to AVRO format. Not applicable
  /// when extracting models.
  package var useAvroLogicalTypes: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _useAvroLogicalTypes ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_useAvroLogicalTypes = newValue}
  }
  /// Returns true if `useAvroLogicalTypes` has been explicitly set.
  package var hasUseAvroLogicalTypes: Bool {return self._useAvroLogicalTypes != nil}
  /// Clears the value of `useAvroLogicalTypes`. Subsequent reads from it will return its default value.
  package mutating func clearUseAvroLogicalTypes() {self._useAvroLogicalTypes = nil}

  /// Optional. Model extract options only applicable when extracting models.
  package var modelExtractOptions: Google_Cloud_Bigquery_V2_JobConfigurationExtract.ModelExtractOptions {
    get {return _modelExtractOptions ?? Google_Cloud_Bigquery_V2_JobConfigurationExtract.ModelExtractOptions()}
    set {_modelExtractOptions = newValue}
  }
  /// Returns true if `modelExtractOptions` has been explicitly set.
  package var hasModelExtractOptions: Bool {return self._modelExtractOptions != nil}
  /// Clears the value of `modelExtractOptions`. Subsequent reads from it will return its default value.
  package mutating func clearModelExtractOptions() {self._modelExtractOptions = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Required. Source reference for the export.
  package enum OneOf_Source: Equatable, Sendable {
    /// A reference to the table being exported.
    case sourceTable(Google_Cloud_Bigquery_V2_TableReference)
    /// A reference to the model being exported.
    case sourceModel(Google_Cloud_Bigquery_V2_ModelReference)

  }

  /// Options related to model extraction.
  package struct ModelExtractOptions: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// The 1-based ID of the trial to be exported from a hyperparameter tuning
    /// model. If not specified, the trial with id =
    /// [Model](https://cloud.google.com/bigquery/docs/reference/rest/v2/models#resource:-model).defaultTrialId
    /// is exported. This field is ignored for models not trained with
    /// hyperparameter tuning.
    package var trialID: SwiftProtobuf.Google_Protobuf_Int64Value {
      get {return _trialID ?? SwiftProtobuf.Google_Protobuf_Int64Value()}
      set {_trialID = newValue}
    }
    /// Returns true if `trialID` has been explicitly set.
    package var hasTrialID: Bool {return self._trialID != nil}
    /// Clears the value of `trialID`. Subsequent reads from it will return its default value.
    package mutating func clearTrialID() {self._trialID = nil}

    package var unknownFields = SwiftProtobuf.UnknownStorage()

    package init() {}

    fileprivate var _trialID: SwiftProtobuf.Google_Protobuf_Int64Value? = nil
  }

  package init() {}

  fileprivate var _printHeader: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
  fileprivate var _useAvroLogicalTypes: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
  fileprivate var _modelExtractOptions: Google_Cloud_Bigquery_V2_JobConfigurationExtract.ModelExtractOptions? = nil
}

package struct Google_Cloud_Bigquery_V2_JobConfiguration: @unchecked Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. The type of the job. Can be QUERY, LOAD, EXTRACT, COPY or
  /// UNKNOWN.
  package var jobType: String {
    get {return _storage._jobType}
    set {_uniqueStorage()._jobType = newValue}
  }

  /// [Pick one] Configures a query job.
  package var query: Google_Cloud_Bigquery_V2_JobConfigurationQuery {
    get {return _storage._query ?? Google_Cloud_Bigquery_V2_JobConfigurationQuery()}
    set {_uniqueStorage()._query = newValue}
  }
  /// Returns true if `query` has been explicitly set.
  package var hasQuery: Bool {return _storage._query != nil}
  /// Clears the value of `query`. Subsequent reads from it will return its default value.
  package mutating func clearQuery() {_uniqueStorage()._query = nil}

  /// [Pick one] Configures a load job.
  package var load: Google_Cloud_Bigquery_V2_JobConfigurationLoad {
    get {return _storage._load ?? Google_Cloud_Bigquery_V2_JobConfigurationLoad()}
    set {_uniqueStorage()._load = newValue}
  }
  /// Returns true if `load` has been explicitly set.
  package var hasLoad: Bool {return _storage._load != nil}
  /// Clears the value of `load`. Subsequent reads from it will return its default value.
  package mutating func clearLoad() {_uniqueStorage()._load = nil}

  /// [Pick one] Copies a table.
  package var copy: Google_Cloud_Bigquery_V2_JobConfigurationTableCopy {
    get {return _storage._copy ?? Google_Cloud_Bigquery_V2_JobConfigurationTableCopy()}
    set {_uniqueStorage()._copy = newValue}
  }
  /// Returns true if `copy` has been explicitly set.
  package var hasCopy: Bool {return _storage._copy != nil}
  /// Clears the value of `copy`. Subsequent reads from it will return its default value.
  package mutating func clearCopy() {_uniqueStorage()._copy = nil}

  /// [Pick one] Configures an extract job.
  package var extract: Google_Cloud_Bigquery_V2_JobConfigurationExtract {
    get {return _storage._extract ?? Google_Cloud_Bigquery_V2_JobConfigurationExtract()}
    set {_uniqueStorage()._extract = newValue}
  }
  /// Returns true if `extract` has been explicitly set.
  package var hasExtract: Bool {return _storage._extract != nil}
  /// Clears the value of `extract`. Subsequent reads from it will return its default value.
  package mutating func clearExtract() {_uniqueStorage()._extract = nil}

  /// Optional. If set, don't actually run this job. A valid query will return
  /// a mostly empty response with some processing statistics, while an invalid
  /// query will return the same error it would if it wasn't a dry run. Behavior
  /// of non-query jobs is undefined.
  package var dryRun: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._dryRun ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._dryRun = newValue}
  }
  /// Returns true if `dryRun` has been explicitly set.
  package var hasDryRun: Bool {return _storage._dryRun != nil}
  /// Clears the value of `dryRun`. Subsequent reads from it will return its default value.
  package mutating func clearDryRun() {_uniqueStorage()._dryRun = nil}

  /// Optional. Job timeout in milliseconds. If this time limit is exceeded,
  /// BigQuery will attempt to stop a longer job, but may not always succeed in
  /// canceling it before the job completes. For example, a job that takes more
  /// than 60 seconds to complete has a better chance of being stopped than a job
  /// that takes 10 seconds to complete.
  package var jobTimeoutMs: SwiftProtobuf.Google_Protobuf_Int64Value {
    get {return _storage._jobTimeoutMs ?? SwiftProtobuf.Google_Protobuf_Int64Value()}
    set {_uniqueStorage()._jobTimeoutMs = newValue}
  }
  /// Returns true if `jobTimeoutMs` has been explicitly set.
  package var hasJobTimeoutMs: Bool {return _storage._jobTimeoutMs != nil}
  /// Clears the value of `jobTimeoutMs`. Subsequent reads from it will return its default value.
  package mutating func clearJobTimeoutMs() {_uniqueStorage()._jobTimeoutMs = nil}

  /// The labels associated with this job. You can use these to organize and
  /// group your jobs.
  /// Label keys and values can be no longer than 63 characters, can only contain
  /// lowercase letters, numeric characters, underscores and dashes.
  /// International characters are allowed. Label values are optional.  Label
  /// keys must start with a letter and each label in the list must have a
  /// different key.
  package var labels: Dictionary<String,String> {
    get {return _storage._labels}
    set {_uniqueStorage()._labels = newValue}
  }

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.bigquery.v2"

extension Google_Cloud_Bigquery_V2_DestinationTableProperties: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".DestinationTableProperties"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "friendly_name"),
    2: .same(proto: "description"),
    3: .same(proto: "labels"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._friendlyName) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._description_p) }()
      case 3: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.labels) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._friendlyName {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    try { if let v = self._description_p {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    if !self.labels.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.labels, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_DestinationTableProperties, rhs: Google_Cloud_Bigquery_V2_DestinationTableProperties) -> Bool {
    if lhs._friendlyName != rhs._friendlyName {return false}
    if lhs._description_p != rhs._description_p {return false}
    if lhs.labels != rhs.labels {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_ConnectionProperty: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".ConnectionProperty"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "key"),
    2: .same(proto: "value"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.key) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.value) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.key.isEmpty {
      try visitor.visitSingularStringField(value: self.key, fieldNumber: 1)
    }
    if !self.value.isEmpty {
      try visitor.visitSingularStringField(value: self.value, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_ConnectionProperty, rhs: Google_Cloud_Bigquery_V2_ConnectionProperty) -> Bool {
    if lhs.key != rhs.key {return false}
    if lhs.value != rhs.value {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_JobConfigurationQuery: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".JobConfigurationQuery"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "query"),
    2: .standard(proto: "destination_table"),
    23: .standard(proto: "external_table_definitions"),
    4: .standard(proto: "user_defined_function_resources"),
    5: .standard(proto: "create_disposition"),
    6: .standard(proto: "write_disposition"),
    7: .standard(proto: "default_dataset"),
    8: .same(proto: "priority"),
    10: .standard(proto: "allow_large_results"),
    11: .standard(proto: "use_query_cache"),
    12: .standard(proto: "flatten_results"),
    14: .standard(proto: "maximum_bytes_billed"),
    15: .standard(proto: "use_legacy_sql"),
    16: .standard(proto: "parameter_mode"),
    17: .standard(proto: "query_parameters"),
    35: .standard(proto: "system_variables"),
    18: .standard(proto: "schema_update_options"),
    19: .standard(proto: "time_partitioning"),
    22: .standard(proto: "range_partitioning"),
    20: .same(proto: "clustering"),
    21: .standard(proto: "destination_encryption_configuration"),
    24: .standard(proto: "script_options"),
    33: .standard(proto: "connection_properties"),
    34: .standard(proto: "create_session"),
    36: .same(proto: "continuous"),
  ]

  fileprivate class _StorageClass {
    var _query: String = String()
    var _destinationTable: Google_Cloud_Bigquery_V2_TableReference? = nil
    var _externalTableDefinitions: Dictionary<String,Google_Cloud_Bigquery_V2_ExternalDataConfiguration> = [:]
    var _userDefinedFunctionResources: [Google_Cloud_Bigquery_V2_UserDefinedFunctionResource] = []
    var _createDisposition: String = String()
    var _writeDisposition: String = String()
    var _defaultDataset: Google_Cloud_Bigquery_V2_DatasetReference? = nil
    var _priority: String = String()
    var _allowLargeResults: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
    var _useQueryCache: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
    var _flattenResults: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
    var _maximumBytesBilled: SwiftProtobuf.Google_Protobuf_Int64Value? = nil
    var _useLegacySql: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
    var _parameterMode: String = String()
    var _queryParameters: [Google_Cloud_Bigquery_V2_QueryParameter] = []
    var _systemVariables: Google_Cloud_Bigquery_V2_SystemVariables? = nil
    var _schemaUpdateOptions: [String] = []
    var _timePartitioning: Google_Cloud_Bigquery_V2_TimePartitioning? = nil
    var _rangePartitioning: Google_Cloud_Bigquery_V2_RangePartitioning? = nil
    var _clustering: Google_Cloud_Bigquery_V2_Clustering? = nil
    var _destinationEncryptionConfiguration: Google_Cloud_Bigquery_V2_EncryptionConfiguration? = nil
    var _scriptOptions: Google_Cloud_Bigquery_V2_ScriptOptions? = nil
    var _connectionProperties: [Google_Cloud_Bigquery_V2_ConnectionProperty] = []
    var _createSession: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
    var _continuous: SwiftProtobuf.Google_Protobuf_BoolValue? = nil

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _query = source._query
      _destinationTable = source._destinationTable
      _externalTableDefinitions = source._externalTableDefinitions
      _userDefinedFunctionResources = source._userDefinedFunctionResources
      _createDisposition = source._createDisposition
      _writeDisposition = source._writeDisposition
      _defaultDataset = source._defaultDataset
      _priority = source._priority
      _allowLargeResults = source._allowLargeResults
      _useQueryCache = source._useQueryCache
      _flattenResults = source._flattenResults
      _maximumBytesBilled = source._maximumBytesBilled
      _useLegacySql = source._useLegacySql
      _parameterMode = source._parameterMode
      _queryParameters = source._queryParameters
      _systemVariables = source._systemVariables
      _schemaUpdateOptions = source._schemaUpdateOptions
      _timePartitioning = source._timePartitioning
      _rangePartitioning = source._rangePartitioning
      _clustering = source._clustering
      _destinationEncryptionConfiguration = source._destinationEncryptionConfiguration
      _scriptOptions = source._scriptOptions
      _connectionProperties = source._connectionProperties
      _createSession = source._createSession
      _continuous = source._continuous
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularStringField(value: &_storage._query) }()
        case 2: try { try decoder.decodeSingularMessageField(value: &_storage._destinationTable) }()
        case 4: try { try decoder.decodeRepeatedMessageField(value: &_storage._userDefinedFunctionResources) }()
        case 5: try { try decoder.decodeSingularStringField(value: &_storage._createDisposition) }()
        case 6: try { try decoder.decodeSingularStringField(value: &_storage._writeDisposition) }()
        case 7: try { try decoder.decodeSingularMessageField(value: &_storage._defaultDataset) }()
        case 8: try { try decoder.decodeSingularStringField(value: &_storage._priority) }()
        case 10: try { try decoder.decodeSingularMessageField(value: &_storage._allowLargeResults) }()
        case 11: try { try decoder.decodeSingularMessageField(value: &_storage._useQueryCache) }()
        case 12: try { try decoder.decodeSingularMessageField(value: &_storage._flattenResults) }()
        case 14: try { try decoder.decodeSingularMessageField(value: &_storage._maximumBytesBilled) }()
        case 15: try { try decoder.decodeSingularMessageField(value: &_storage._useLegacySql) }()
        case 16: try { try decoder.decodeSingularStringField(value: &_storage._parameterMode) }()
        case 17: try { try decoder.decodeRepeatedMessageField(value: &_storage._queryParameters) }()
        case 18: try { try decoder.decodeRepeatedStringField(value: &_storage._schemaUpdateOptions) }()
        case 19: try { try decoder.decodeSingularMessageField(value: &_storage._timePartitioning) }()
        case 20: try { try decoder.decodeSingularMessageField(value: &_storage._clustering) }()
        case 21: try { try decoder.decodeSingularMessageField(value: &_storage._destinationEncryptionConfiguration) }()
        case 22: try { try decoder.decodeSingularMessageField(value: &_storage._rangePartitioning) }()
        case 23: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,Google_Cloud_Bigquery_V2_ExternalDataConfiguration>.self, value: &_storage._externalTableDefinitions) }()
        case 24: try { try decoder.decodeSingularMessageField(value: &_storage._scriptOptions) }()
        case 33: try { try decoder.decodeRepeatedMessageField(value: &_storage._connectionProperties) }()
        case 34: try { try decoder.decodeSingularMessageField(value: &_storage._createSession) }()
        case 35: try { try decoder.decodeSingularMessageField(value: &_storage._systemVariables) }()
        case 36: try { try decoder.decodeSingularMessageField(value: &_storage._continuous) }()
        default: break
        }
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      if !_storage._query.isEmpty {
        try visitor.visitSingularStringField(value: _storage._query, fieldNumber: 1)
      }
      try { if let v = _storage._destinationTable {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      } }()
      if !_storage._userDefinedFunctionResources.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._userDefinedFunctionResources, fieldNumber: 4)
      }
      if !_storage._createDisposition.isEmpty {
        try visitor.visitSingularStringField(value: _storage._createDisposition, fieldNumber: 5)
      }
      if !_storage._writeDisposition.isEmpty {
        try visitor.visitSingularStringField(value: _storage._writeDisposition, fieldNumber: 6)
      }
      try { if let v = _storage._defaultDataset {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
      } }()
      if !_storage._priority.isEmpty {
        try visitor.visitSingularStringField(value: _storage._priority, fieldNumber: 8)
      }
      try { if let v = _storage._allowLargeResults {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 10)
      } }()
      try { if let v = _storage._useQueryCache {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 11)
      } }()
      try { if let v = _storage._flattenResults {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 12)
      } }()
      try { if let v = _storage._maximumBytesBilled {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 14)
      } }()
      try { if let v = _storage._useLegacySql {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 15)
      } }()
      if !_storage._parameterMode.isEmpty {
        try visitor.visitSingularStringField(value: _storage._parameterMode, fieldNumber: 16)
      }
      if !_storage._queryParameters.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._queryParameters, fieldNumber: 17)
      }
      if !_storage._schemaUpdateOptions.isEmpty {
        try visitor.visitRepeatedStringField(value: _storage._schemaUpdateOptions, fieldNumber: 18)
      }
      try { if let v = _storage._timePartitioning {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 19)
      } }()
      try { if let v = _storage._clustering {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 20)
      } }()
      try { if let v = _storage._destinationEncryptionConfiguration {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 21)
      } }()
      try { if let v = _storage._rangePartitioning {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 22)
      } }()
      if !_storage._externalTableDefinitions.isEmpty {
        try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,Google_Cloud_Bigquery_V2_ExternalDataConfiguration>.self, value: _storage._externalTableDefinitions, fieldNumber: 23)
      }
      try { if let v = _storage._scriptOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 24)
      } }()
      if !_storage._connectionProperties.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._connectionProperties, fieldNumber: 33)
      }
      try { if let v = _storage._createSession {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 34)
      } }()
      try { if let v = _storage._systemVariables {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 35)
      } }()
      try { if let v = _storage._continuous {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 36)
      } }()
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_JobConfigurationQuery, rhs: Google_Cloud_Bigquery_V2_JobConfigurationQuery) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._query != rhs_storage._query {return false}
        if _storage._destinationTable != rhs_storage._destinationTable {return false}
        if _storage._externalTableDefinitions != rhs_storage._externalTableDefinitions {return false}
        if _storage._userDefinedFunctionResources != rhs_storage._userDefinedFunctionResources {return false}
        if _storage._createDisposition != rhs_storage._createDisposition {return false}
        if _storage._writeDisposition != rhs_storage._writeDisposition {return false}
        if _storage._defaultDataset != rhs_storage._defaultDataset {return false}
        if _storage._priority != rhs_storage._priority {return false}
        if _storage._allowLargeResults != rhs_storage._allowLargeResults {return false}
        if _storage._useQueryCache != rhs_storage._useQueryCache {return false}
        if _storage._flattenResults != rhs_storage._flattenResults {return false}
        if _storage._maximumBytesBilled != rhs_storage._maximumBytesBilled {return false}
        if _storage._useLegacySql != rhs_storage._useLegacySql {return false}
        if _storage._parameterMode != rhs_storage._parameterMode {return false}
        if _storage._queryParameters != rhs_storage._queryParameters {return false}
        if _storage._systemVariables != rhs_storage._systemVariables {return false}
        if _storage._schemaUpdateOptions != rhs_storage._schemaUpdateOptions {return false}
        if _storage._timePartitioning != rhs_storage._timePartitioning {return false}
        if _storage._rangePartitioning != rhs_storage._rangePartitioning {return false}
        if _storage._clustering != rhs_storage._clustering {return false}
        if _storage._destinationEncryptionConfiguration != rhs_storage._destinationEncryptionConfiguration {return false}
        if _storage._scriptOptions != rhs_storage._scriptOptions {return false}
        if _storage._connectionProperties != rhs_storage._connectionProperties {return false}
        if _storage._createSession != rhs_storage._createSession {return false}
        if _storage._continuous != rhs_storage._continuous {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_ScriptOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".ScriptOptions"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "statement_timeout_ms"),
    2: .standard(proto: "statement_byte_budget"),
    4: .standard(proto: "key_result_statement"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._statementTimeoutMs) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._statementByteBudget) }()
      case 4: try { try decoder.decodeSingularEnumField(value: &self.keyResultStatement) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._statementTimeoutMs {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    try { if let v = self._statementByteBudget {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    if self.keyResultStatement != .unspecified {
      try visitor.visitSingularEnumField(value: self.keyResultStatement, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_ScriptOptions, rhs: Google_Cloud_Bigquery_V2_ScriptOptions) -> Bool {
    if lhs._statementTimeoutMs != rhs._statementTimeoutMs {return false}
    if lhs._statementByteBudget != rhs._statementByteBudget {return false}
    if lhs.keyResultStatement != rhs.keyResultStatement {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_ScriptOptions.KeyResultStatementKind: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "KEY_RESULT_STATEMENT_KIND_UNSPECIFIED"),
    1: .same(proto: "LAST"),
    2: .same(proto: "FIRST_SELECT"),
  ]
}

extension Google_Cloud_Bigquery_V2_JobConfigurationLoad: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".JobConfigurationLoad"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "source_uris"),
    49: .standard(proto: "file_set_spec_type"),
    2: .same(proto: "schema"),
    3: .standard(proto: "destination_table"),
    4: .standard(proto: "destination_table_properties"),
    5: .standard(proto: "create_disposition"),
    6: .standard(proto: "write_disposition"),
    7: .standard(proto: "null_marker"),
    8: .standard(proto: "field_delimiter"),
    9: .standard(proto: "skip_leading_rows"),
    10: .same(proto: "encoding"),
    11: .same(proto: "quote"),
    12: .standard(proto: "max_bad_records"),
    15: .standard(proto: "allow_quoted_newlines"),
    16: .standard(proto: "source_format"),
    17: .standard(proto: "allow_jagged_rows"),
    18: .standard(proto: "ignore_unknown_values"),
    19: .standard(proto: "projection_fields"),
    20: .same(proto: "autodetect"),
    21: .standard(proto: "schema_update_options"),
    22: .standard(proto: "time_partitioning"),
    26: .standard(proto: "range_partitioning"),
    23: .same(proto: "clustering"),
    24: .standard(proto: "destination_encryption_configuration"),
    25: .standard(proto: "use_avro_logical_types"),
    45: .standard(proto: "reference_file_schema_uri"),
    37: .standard(proto: "hive_partitioning_options"),
    39: .standard(proto: "decimal_target_types"),
    41: .standard(proto: "json_extension"),
    42: .standard(proto: "parquet_options"),
    44: .standard(proto: "preserve_ascii_control_characters"),
    46: .standard(proto: "connection_properties"),
    47: .standard(proto: "create_session"),
    50: .standard(proto: "column_name_character_map"),
    51: .standard(proto: "copy_files_only"),
  ]

  fileprivate class _StorageClass {
    var _sourceUris: [String] = []
    var _fileSetSpecType: Google_Cloud_Bigquery_V2_FileSetSpecType = .fileSystemMatch
    var _schema: Google_Cloud_Bigquery_V2_TableSchema? = nil
    var _destinationTable: Google_Cloud_Bigquery_V2_TableReference? = nil
    var _destinationTableProperties: Google_Cloud_Bigquery_V2_DestinationTableProperties? = nil
    var _createDisposition: String = String()
    var _writeDisposition: String = String()
    var _nullMarker: SwiftProtobuf.Google_Protobuf_StringValue? = nil
    var _fieldDelimiter: String = String()
    var _skipLeadingRows: SwiftProtobuf.Google_Protobuf_Int32Value? = nil
    var _encoding: String = String()
    var _quote: SwiftProtobuf.Google_Protobuf_StringValue? = nil
    var _maxBadRecords: SwiftProtobuf.Google_Protobuf_Int32Value? = nil
    var _allowQuotedNewlines: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
    var _sourceFormat: String = String()
    var _allowJaggedRows: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
    var _ignoreUnknownValues: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
    var _projectionFields: [String] = []
    var _autodetect: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
    var _schemaUpdateOptions: [String] = []
    var _timePartitioning: Google_Cloud_Bigquery_V2_TimePartitioning? = nil
    var _rangePartitioning: Google_Cloud_Bigquery_V2_RangePartitioning? = nil
    var _clustering: Google_Cloud_Bigquery_V2_Clustering? = nil
    var _destinationEncryptionConfiguration: Google_Cloud_Bigquery_V2_EncryptionConfiguration? = nil
    var _useAvroLogicalTypes: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
    var _referenceFileSchemaUri: SwiftProtobuf.Google_Protobuf_StringValue? = nil
    var _hivePartitioningOptions: Google_Cloud_Bigquery_V2_HivePartitioningOptions? = nil
    var _decimalTargetTypes: [Google_Cloud_Bigquery_V2_DecimalTargetType] = []
    var _jsonExtension: Google_Cloud_Bigquery_V2_JsonExtension = .unspecified
    var _parquetOptions: Google_Cloud_Bigquery_V2_ParquetOptions? = nil
    var _preserveAsciiControlCharacters: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
    var _connectionProperties: [Google_Cloud_Bigquery_V2_ConnectionProperty] = []
    var _createSession: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
    var _columnNameCharacterMap: Google_Cloud_Bigquery_V2_JobConfigurationLoad.ColumnNameCharacterMap = .unspecified
    var _copyFilesOnly: SwiftProtobuf.Google_Protobuf_BoolValue? = nil

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _sourceUris = source._sourceUris
      _fileSetSpecType = source._fileSetSpecType
      _schema = source._schema
      _destinationTable = source._destinationTable
      _destinationTableProperties = source._destinationTableProperties
      _createDisposition = source._createDisposition
      _writeDisposition = source._writeDisposition
      _nullMarker = source._nullMarker
      _fieldDelimiter = source._fieldDelimiter
      _skipLeadingRows = source._skipLeadingRows
      _encoding = source._encoding
      _quote = source._quote
      _maxBadRecords = source._maxBadRecords
      _allowQuotedNewlines = source._allowQuotedNewlines
      _sourceFormat = source._sourceFormat
      _allowJaggedRows = source._allowJaggedRows
      _ignoreUnknownValues = source._ignoreUnknownValues
      _projectionFields = source._projectionFields
      _autodetect = source._autodetect
      _schemaUpdateOptions = source._schemaUpdateOptions
      _timePartitioning = source._timePartitioning
      _rangePartitioning = source._rangePartitioning
      _clustering = source._clustering
      _destinationEncryptionConfiguration = source._destinationEncryptionConfiguration
      _useAvroLogicalTypes = source._useAvroLogicalTypes
      _referenceFileSchemaUri = source._referenceFileSchemaUri
      _hivePartitioningOptions = source._hivePartitioningOptions
      _decimalTargetTypes = source._decimalTargetTypes
      _jsonExtension = source._jsonExtension
      _parquetOptions = source._parquetOptions
      _preserveAsciiControlCharacters = source._preserveAsciiControlCharacters
      _connectionProperties = source._connectionProperties
      _createSession = source._createSession
      _columnNameCharacterMap = source._columnNameCharacterMap
      _copyFilesOnly = source._copyFilesOnly
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeRepeatedStringField(value: &_storage._sourceUris) }()
        case 2: try { try decoder.decodeSingularMessageField(value: &_storage._schema) }()
        case 3: try { try decoder.decodeSingularMessageField(value: &_storage._destinationTable) }()
        case 4: try { try decoder.decodeSingularMessageField(value: &_storage._destinationTableProperties) }()
        case 5: try { try decoder.decodeSingularStringField(value: &_storage._createDisposition) }()
        case 6: try { try decoder.decodeSingularStringField(value: &_storage._writeDisposition) }()
        case 7: try { try decoder.decodeSingularMessageField(value: &_storage._nullMarker) }()
        case 8: try { try decoder.decodeSingularStringField(value: &_storage._fieldDelimiter) }()
        case 9: try { try decoder.decodeSingularMessageField(value: &_storage._skipLeadingRows) }()
        case 10: try { try decoder.decodeSingularStringField(value: &_storage._encoding) }()
        case 11: try { try decoder.decodeSingularMessageField(value: &_storage._quote) }()
        case 12: try { try decoder.decodeSingularMessageField(value: &_storage._maxBadRecords) }()
        case 15: try { try decoder.decodeSingularMessageField(value: &_storage._allowQuotedNewlines) }()
        case 16: try { try decoder.decodeSingularStringField(value: &_storage._sourceFormat) }()
        case 17: try { try decoder.decodeSingularMessageField(value: &_storage._allowJaggedRows) }()
        case 18: try { try decoder.decodeSingularMessageField(value: &_storage._ignoreUnknownValues) }()
        case 19: try { try decoder.decodeRepeatedStringField(value: &_storage._projectionFields) }()
        case 20: try { try decoder.decodeSingularMessageField(value: &_storage._autodetect) }()
        case 21: try { try decoder.decodeRepeatedStringField(value: &_storage._schemaUpdateOptions) }()
        case 22: try { try decoder.decodeSingularMessageField(value: &_storage._timePartitioning) }()
        case 23: try { try decoder.decodeSingularMessageField(value: &_storage._clustering) }()
        case 24: try { try decoder.decodeSingularMessageField(value: &_storage._destinationEncryptionConfiguration) }()
        case 25: try { try decoder.decodeSingularMessageField(value: &_storage._useAvroLogicalTypes) }()
        case 26: try { try decoder.decodeSingularMessageField(value: &_storage._rangePartitioning) }()
        case 37: try { try decoder.decodeSingularMessageField(value: &_storage._hivePartitioningOptions) }()
        case 39: try { try decoder.decodeRepeatedEnumField(value: &_storage._decimalTargetTypes) }()
        case 41: try { try decoder.decodeSingularEnumField(value: &_storage._jsonExtension) }()
        case 42: try { try decoder.decodeSingularMessageField(value: &_storage._parquetOptions) }()
        case 44: try { try decoder.decodeSingularMessageField(value: &_storage._preserveAsciiControlCharacters) }()
        case 45: try { try decoder.decodeSingularMessageField(value: &_storage._referenceFileSchemaUri) }()
        case 46: try { try decoder.decodeRepeatedMessageField(value: &_storage._connectionProperties) }()
        case 47: try { try decoder.decodeSingularMessageField(value: &_storage._createSession) }()
        case 49: try { try decoder.decodeSingularEnumField(value: &_storage._fileSetSpecType) }()
        case 50: try { try decoder.decodeSingularEnumField(value: &_storage._columnNameCharacterMap) }()
        case 51: try { try decoder.decodeSingularMessageField(value: &_storage._copyFilesOnly) }()
        default: break
        }
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      if !_storage._sourceUris.isEmpty {
        try visitor.visitRepeatedStringField(value: _storage._sourceUris, fieldNumber: 1)
      }
      try { if let v = _storage._schema {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      } }()
      try { if let v = _storage._destinationTable {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
      } }()
      try { if let v = _storage._destinationTableProperties {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
      } }()
      if !_storage._createDisposition.isEmpty {
        try visitor.visitSingularStringField(value: _storage._createDisposition, fieldNumber: 5)
      }
      if !_storage._writeDisposition.isEmpty {
        try visitor.visitSingularStringField(value: _storage._writeDisposition, fieldNumber: 6)
      }
      try { if let v = _storage._nullMarker {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
      } }()
      if !_storage._fieldDelimiter.isEmpty {
        try visitor.visitSingularStringField(value: _storage._fieldDelimiter, fieldNumber: 8)
      }
      try { if let v = _storage._skipLeadingRows {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
      } }()
      if !_storage._encoding.isEmpty {
        try visitor.visitSingularStringField(value: _storage._encoding, fieldNumber: 10)
      }
      try { if let v = _storage._quote {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 11)
      } }()
      try { if let v = _storage._maxBadRecords {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 12)
      } }()
      try { if let v = _storage._allowQuotedNewlines {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 15)
      } }()
      if !_storage._sourceFormat.isEmpty {
        try visitor.visitSingularStringField(value: _storage._sourceFormat, fieldNumber: 16)
      }
      try { if let v = _storage._allowJaggedRows {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 17)
      } }()
      try { if let v = _storage._ignoreUnknownValues {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 18)
      } }()
      if !_storage._projectionFields.isEmpty {
        try visitor.visitRepeatedStringField(value: _storage._projectionFields, fieldNumber: 19)
      }
      try { if let v = _storage._autodetect {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 20)
      } }()
      if !_storage._schemaUpdateOptions.isEmpty {
        try visitor.visitRepeatedStringField(value: _storage._schemaUpdateOptions, fieldNumber: 21)
      }
      try { if let v = _storage._timePartitioning {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 22)
      } }()
      try { if let v = _storage._clustering {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 23)
      } }()
      try { if let v = _storage._destinationEncryptionConfiguration {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 24)
      } }()
      try { if let v = _storage._useAvroLogicalTypes {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 25)
      } }()
      try { if let v = _storage._rangePartitioning {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 26)
      } }()
      try { if let v = _storage._hivePartitioningOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 37)
      } }()
      if !_storage._decimalTargetTypes.isEmpty {
        try visitor.visitPackedEnumField(value: _storage._decimalTargetTypes, fieldNumber: 39)
      }
      if _storage._jsonExtension != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._jsonExtension, fieldNumber: 41)
      }
      try { if let v = _storage._parquetOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 42)
      } }()
      try { if let v = _storage._preserveAsciiControlCharacters {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 44)
      } }()
      try { if let v = _storage._referenceFileSchemaUri {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 45)
      } }()
      if !_storage._connectionProperties.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._connectionProperties, fieldNumber: 46)
      }
      try { if let v = _storage._createSession {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 47)
      } }()
      if _storage._fileSetSpecType != .fileSystemMatch {
        try visitor.visitSingularEnumField(value: _storage._fileSetSpecType, fieldNumber: 49)
      }
      if _storage._columnNameCharacterMap != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._columnNameCharacterMap, fieldNumber: 50)
      }
      try { if let v = _storage._copyFilesOnly {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 51)
      } }()
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_JobConfigurationLoad, rhs: Google_Cloud_Bigquery_V2_JobConfigurationLoad) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._sourceUris != rhs_storage._sourceUris {return false}
        if _storage._fileSetSpecType != rhs_storage._fileSetSpecType {return false}
        if _storage._schema != rhs_storage._schema {return false}
        if _storage._destinationTable != rhs_storage._destinationTable {return false}
        if _storage._destinationTableProperties != rhs_storage._destinationTableProperties {return false}
        if _storage._createDisposition != rhs_storage._createDisposition {return false}
        if _storage._writeDisposition != rhs_storage._writeDisposition {return false}
        if _storage._nullMarker != rhs_storage._nullMarker {return false}
        if _storage._fieldDelimiter != rhs_storage._fieldDelimiter {return false}
        if _storage._skipLeadingRows != rhs_storage._skipLeadingRows {return false}
        if _storage._encoding != rhs_storage._encoding {return false}
        if _storage._quote != rhs_storage._quote {return false}
        if _storage._maxBadRecords != rhs_storage._maxBadRecords {return false}
        if _storage._allowQuotedNewlines != rhs_storage._allowQuotedNewlines {return false}
        if _storage._sourceFormat != rhs_storage._sourceFormat {return false}
        if _storage._allowJaggedRows != rhs_storage._allowJaggedRows {return false}
        if _storage._ignoreUnknownValues != rhs_storage._ignoreUnknownValues {return false}
        if _storage._projectionFields != rhs_storage._projectionFields {return false}
        if _storage._autodetect != rhs_storage._autodetect {return false}
        if _storage._schemaUpdateOptions != rhs_storage._schemaUpdateOptions {return false}
        if _storage._timePartitioning != rhs_storage._timePartitioning {return false}
        if _storage._rangePartitioning != rhs_storage._rangePartitioning {return false}
        if _storage._clustering != rhs_storage._clustering {return false}
        if _storage._destinationEncryptionConfiguration != rhs_storage._destinationEncryptionConfiguration {return false}
        if _storage._useAvroLogicalTypes != rhs_storage._useAvroLogicalTypes {return false}
        if _storage._referenceFileSchemaUri != rhs_storage._referenceFileSchemaUri {return false}
        if _storage._hivePartitioningOptions != rhs_storage._hivePartitioningOptions {return false}
        if _storage._decimalTargetTypes != rhs_storage._decimalTargetTypes {return false}
        if _storage._jsonExtension != rhs_storage._jsonExtension {return false}
        if _storage._parquetOptions != rhs_storage._parquetOptions {return false}
        if _storage._preserveAsciiControlCharacters != rhs_storage._preserveAsciiControlCharacters {return false}
        if _storage._connectionProperties != rhs_storage._connectionProperties {return false}
        if _storage._createSession != rhs_storage._createSession {return false}
        if _storage._columnNameCharacterMap != rhs_storage._columnNameCharacterMap {return false}
        if _storage._copyFilesOnly != rhs_storage._copyFilesOnly {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_JobConfigurationLoad.ColumnNameCharacterMap: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "COLUMN_NAME_CHARACTER_MAP_UNSPECIFIED"),
    1: .same(proto: "STRICT"),
    2: .same(proto: "V1"),
    3: .same(proto: "V2"),
  ]
}

extension Google_Cloud_Bigquery_V2_JobConfigurationTableCopy: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".JobConfigurationTableCopy"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "source_table"),
    2: .standard(proto: "source_tables"),
    3: .standard(proto: "destination_table"),
    4: .standard(proto: "create_disposition"),
    5: .standard(proto: "write_disposition"),
    6: .standard(proto: "destination_encryption_configuration"),
    8: .standard(proto: "operation_type"),
    9: .standard(proto: "destination_expiration_time"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._sourceTable) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.sourceTables) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._destinationTable) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.createDisposition) }()
      case 5: try { try decoder.decodeSingularStringField(value: &self.writeDisposition) }()
      case 6: try { try decoder.decodeSingularMessageField(value: &self._destinationEncryptionConfiguration) }()
      case 8: try { try decoder.decodeSingularEnumField(value: &self.operationType) }()
      case 9: try { try decoder.decodeSingularMessageField(value: &self._destinationExpirationTime) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._sourceTable {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    if !self.sourceTables.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.sourceTables, fieldNumber: 2)
    }
    try { if let v = self._destinationTable {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    } }()
    if !self.createDisposition.isEmpty {
      try visitor.visitSingularStringField(value: self.createDisposition, fieldNumber: 4)
    }
    if !self.writeDisposition.isEmpty {
      try visitor.visitSingularStringField(value: self.writeDisposition, fieldNumber: 5)
    }
    try { if let v = self._destinationEncryptionConfiguration {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
    } }()
    if self.operationType != .unspecified {
      try visitor.visitSingularEnumField(value: self.operationType, fieldNumber: 8)
    }
    try { if let v = self._destinationExpirationTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_JobConfigurationTableCopy, rhs: Google_Cloud_Bigquery_V2_JobConfigurationTableCopy) -> Bool {
    if lhs._sourceTable != rhs._sourceTable {return false}
    if lhs.sourceTables != rhs.sourceTables {return false}
    if lhs._destinationTable != rhs._destinationTable {return false}
    if lhs.createDisposition != rhs.createDisposition {return false}
    if lhs.writeDisposition != rhs.writeDisposition {return false}
    if lhs._destinationEncryptionConfiguration != rhs._destinationEncryptionConfiguration {return false}
    if lhs.operationType != rhs.operationType {return false}
    if lhs._destinationExpirationTime != rhs._destinationExpirationTime {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_JobConfigurationTableCopy.OperationType: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "OPERATION_TYPE_UNSPECIFIED"),
    1: .same(proto: "COPY"),
    2: .same(proto: "SNAPSHOT"),
    3: .same(proto: "RESTORE"),
    4: .same(proto: "CLONE"),
  ]
}

extension Google_Cloud_Bigquery_V2_JobConfigurationExtract: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".JobConfigurationExtract"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "source_table"),
    9: .standard(proto: "source_model"),
    3: .standard(proto: "destination_uris"),
    4: .standard(proto: "print_header"),
    5: .standard(proto: "field_delimiter"),
    6: .standard(proto: "destination_format"),
    7: .same(proto: "compression"),
    13: .standard(proto: "use_avro_logical_types"),
    14: .standard(proto: "model_extract_options"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        var v: Google_Cloud_Bigquery_V2_TableReference?
        var hadOneofValue = false
        if let current = self.source {
          hadOneofValue = true
          if case .sourceTable(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.source = .sourceTable(v)
        }
      }()
      case 3: try { try decoder.decodeRepeatedStringField(value: &self.destinationUris) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._printHeader) }()
      case 5: try { try decoder.decodeSingularStringField(value: &self.fieldDelimiter) }()
      case 6: try { try decoder.decodeSingularStringField(value: &self.destinationFormat) }()
      case 7: try { try decoder.decodeSingularStringField(value: &self.compression) }()
      case 9: try {
        var v: Google_Cloud_Bigquery_V2_ModelReference?
        var hadOneofValue = false
        if let current = self.source {
          hadOneofValue = true
          if case .sourceModel(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {
          if hadOneofValue {try decoder.handleConflictingOneOf()}
          self.source = .sourceModel(v)
        }
      }()
      case 13: try { try decoder.decodeSingularMessageField(value: &self._useAvroLogicalTypes) }()
      case 14: try { try decoder.decodeSingularMessageField(value: &self._modelExtractOptions) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if case .sourceTable(let v)? = self.source {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    if !self.destinationUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.destinationUris, fieldNumber: 3)
    }
    try { if let v = self._printHeader {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    } }()
    if !self.fieldDelimiter.isEmpty {
      try visitor.visitSingularStringField(value: self.fieldDelimiter, fieldNumber: 5)
    }
    if !self.destinationFormat.isEmpty {
      try visitor.visitSingularStringField(value: self.destinationFormat, fieldNumber: 6)
    }
    if !self.compression.isEmpty {
      try visitor.visitSingularStringField(value: self.compression, fieldNumber: 7)
    }
    try { if case .sourceModel(let v)? = self.source {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
    } }()
    try { if let v = self._useAvroLogicalTypes {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 13)
    } }()
    try { if let v = self._modelExtractOptions {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 14)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_JobConfigurationExtract, rhs: Google_Cloud_Bigquery_V2_JobConfigurationExtract) -> Bool {
    if lhs.source != rhs.source {return false}
    if lhs.destinationUris != rhs.destinationUris {return false}
    if lhs._printHeader != rhs._printHeader {return false}
    if lhs.fieldDelimiter != rhs.fieldDelimiter {return false}
    if lhs.destinationFormat != rhs.destinationFormat {return false}
    if lhs.compression != rhs.compression {return false}
    if lhs._useAvroLogicalTypes != rhs._useAvroLogicalTypes {return false}
    if lhs._modelExtractOptions != rhs._modelExtractOptions {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_JobConfigurationExtract.ModelExtractOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = Google_Cloud_Bigquery_V2_JobConfigurationExtract.protoMessageName + ".ModelExtractOptions"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "trial_id"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._trialID) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._trialID {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_JobConfigurationExtract.ModelExtractOptions, rhs: Google_Cloud_Bigquery_V2_JobConfigurationExtract.ModelExtractOptions) -> Bool {
    if lhs._trialID != rhs._trialID {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_JobConfiguration: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".JobConfiguration"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    8: .standard(proto: "job_type"),
    1: .same(proto: "query"),
    2: .same(proto: "load"),
    3: .same(proto: "copy"),
    4: .same(proto: "extract"),
    5: .standard(proto: "dry_run"),
    6: .standard(proto: "job_timeout_ms"),
    7: .same(proto: "labels"),
  ]

  fileprivate class _StorageClass {
    var _jobType: String = String()
    var _query: Google_Cloud_Bigquery_V2_JobConfigurationQuery? = nil
    var _load: Google_Cloud_Bigquery_V2_JobConfigurationLoad? = nil
    var _copy: Google_Cloud_Bigquery_V2_JobConfigurationTableCopy? = nil
    var _extract: Google_Cloud_Bigquery_V2_JobConfigurationExtract? = nil
    var _dryRun: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
    var _jobTimeoutMs: SwiftProtobuf.Google_Protobuf_Int64Value? = nil
    var _labels: Dictionary<String,String> = [:]

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _jobType = source._jobType
      _query = source._query
      _load = source._load
      _copy = source._copy
      _extract = source._extract
      _dryRun = source._dryRun
      _jobTimeoutMs = source._jobTimeoutMs
      _labels = source._labels
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularMessageField(value: &_storage._query) }()
        case 2: try { try decoder.decodeSingularMessageField(value: &_storage._load) }()
        case 3: try { try decoder.decodeSingularMessageField(value: &_storage._copy) }()
        case 4: try { try decoder.decodeSingularMessageField(value: &_storage._extract) }()
        case 5: try { try decoder.decodeSingularMessageField(value: &_storage._dryRun) }()
        case 6: try { try decoder.decodeSingularMessageField(value: &_storage._jobTimeoutMs) }()
        case 7: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &_storage._labels) }()
        case 8: try { try decoder.decodeSingularStringField(value: &_storage._jobType) }()
        default: break
        }
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      try { if let v = _storage._query {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      } }()
      try { if let v = _storage._load {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      } }()
      try { if let v = _storage._copy {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
      } }()
      try { if let v = _storage._extract {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
      } }()
      try { if let v = _storage._dryRun {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
      } }()
      try { if let v = _storage._jobTimeoutMs {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
      } }()
      if !_storage._labels.isEmpty {
        try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: _storage._labels, fieldNumber: 7)
      }
      if !_storage._jobType.isEmpty {
        try visitor.visitSingularStringField(value: _storage._jobType, fieldNumber: 8)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_JobConfiguration, rhs: Google_Cloud_Bigquery_V2_JobConfiguration) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._jobType != rhs_storage._jobType {return false}
        if _storage._query != rhs_storage._query {return false}
        if _storage._load != rhs_storage._load {return false}
        if _storage._copy != rhs_storage._copy {return false}
        if _storage._extract != rhs_storage._extract {return false}
        if _storage._dryRun != rhs_storage._dryRun {return false}
        if _storage._jobTimeoutMs != rhs_storage._jobTimeoutMs {return false}
        if _storage._labels != rhs_storage._labels {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
