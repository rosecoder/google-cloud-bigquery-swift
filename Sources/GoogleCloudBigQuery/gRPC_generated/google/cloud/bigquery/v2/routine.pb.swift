// DO NOT EDIT.
// swift-format-ignore-file
// swiftlint:disable all
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/bigquery/v2/routine.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// A user-defined function or a stored procedure.
package struct Google_Cloud_Bigquery_V2_Routine: @unchecked Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. A hash of this resource.
  package var etag: String {
    get {return _storage._etag}
    set {_uniqueStorage()._etag = newValue}
  }

  /// Required. Reference describing the ID of this routine.
  package var routineReference: Google_Cloud_Bigquery_V2_RoutineReference {
    get {return _storage._routineReference ?? Google_Cloud_Bigquery_V2_RoutineReference()}
    set {_uniqueStorage()._routineReference = newValue}
  }
  /// Returns true if `routineReference` has been explicitly set.
  package var hasRoutineReference: Bool {return _storage._routineReference != nil}
  /// Clears the value of `routineReference`. Subsequent reads from it will return its default value.
  package mutating func clearRoutineReference() {_uniqueStorage()._routineReference = nil}

  /// Required. The type of routine.
  package var routineType: Google_Cloud_Bigquery_V2_Routine.RoutineType {
    get {return _storage._routineType}
    set {_uniqueStorage()._routineType = newValue}
  }

  /// Output only. The time when this routine was created, in milliseconds since
  /// the epoch.
  package var creationTime: Int64 {
    get {return _storage._creationTime}
    set {_uniqueStorage()._creationTime = newValue}
  }

  /// Output only. The time when this routine was last modified, in milliseconds
  /// since the epoch.
  package var lastModifiedTime: Int64 {
    get {return _storage._lastModifiedTime}
    set {_uniqueStorage()._lastModifiedTime = newValue}
  }

  /// Optional. Defaults to "SQL" if remote_function_options field is absent, not
  /// set otherwise.
  package var language: Google_Cloud_Bigquery_V2_Routine.Language {
    get {return _storage._language}
    set {_uniqueStorage()._language = newValue}
  }

  /// Optional.
  package var arguments: [Google_Cloud_Bigquery_V2_Routine.Argument] {
    get {return _storage._arguments}
    set {_uniqueStorage()._arguments = newValue}
  }

  /// Optional if language = "SQL"; required otherwise.
  /// Cannot be set if routine_type = "TABLE_VALUED_FUNCTION".
  ///
  /// If absent, the return type is inferred from definition_body at query time
  /// in each query that references this routine. If present, then the evaluated
  /// result will be cast to the specified returned type at query time.
  ///
  /// For example, for the functions created with the following statements:
  ///
  /// * `CREATE FUNCTION Add(x FLOAT64, y FLOAT64) RETURNS FLOAT64 AS (x + y);`
  ///
  /// * `CREATE FUNCTION Increment(x FLOAT64) AS (Add(x, 1));`
  ///
  /// * `CREATE FUNCTION Decrement(x FLOAT64) RETURNS FLOAT64 AS (Add(x, -1));`
  ///
  /// The return_type is `{type_kind: "FLOAT64"}` for `Add` and `Decrement`, and
  /// is absent for `Increment` (inferred as FLOAT64 at query time).
  ///
  /// Suppose the function `Add` is replaced by
  ///   `CREATE OR REPLACE FUNCTION Add(x INT64, y INT64) AS (x + y);`
  ///
  /// Then the inferred return type of `Increment` is automatically changed to
  /// INT64 at query time, while the return type of `Decrement` remains FLOAT64.
  package var returnType: Google_Cloud_Bigquery_V2_StandardSqlDataType {
    get {return _storage._returnType ?? Google_Cloud_Bigquery_V2_StandardSqlDataType()}
    set {_uniqueStorage()._returnType = newValue}
  }
  /// Returns true if `returnType` has been explicitly set.
  package var hasReturnType: Bool {return _storage._returnType != nil}
  /// Clears the value of `returnType`. Subsequent reads from it will return its default value.
  package mutating func clearReturnType() {_uniqueStorage()._returnType = nil}

  /// Optional. Can be set only if routine_type = "TABLE_VALUED_FUNCTION".
  ///
  /// If absent, the return table type is inferred from definition_body at query
  /// time in each query that references this routine. If present, then the
  /// columns in the evaluated table result will be cast to match the column
  /// types specified in return table type, at query time.
  package var returnTableType: Google_Cloud_Bigquery_V2_StandardSqlTableType {
    get {return _storage._returnTableType ?? Google_Cloud_Bigquery_V2_StandardSqlTableType()}
    set {_uniqueStorage()._returnTableType = newValue}
  }
  /// Returns true if `returnTableType` has been explicitly set.
  package var hasReturnTableType: Bool {return _storage._returnTableType != nil}
  /// Clears the value of `returnTableType`. Subsequent reads from it will return its default value.
  package mutating func clearReturnTableType() {_uniqueStorage()._returnTableType = nil}

  /// Optional. If language = "JAVASCRIPT", this field stores the path of the
  /// imported JAVASCRIPT libraries.
  package var importedLibraries: [String] {
    get {return _storage._importedLibraries}
    set {_uniqueStorage()._importedLibraries = newValue}
  }

  /// Required. The body of the routine.
  ///
  /// For functions, this is the expression in the AS clause.
  ///
  /// If language=SQL, it is the substring inside (but excluding) the
  /// parentheses. For example, for the function created with the following
  /// statement:
  ///
  /// `CREATE FUNCTION JoinLines(x string, y string) as (concat(x, "\n", y))`
  ///
  /// The definition_body is `concat(x, "\n", y)` (\n is not replaced with
  /// linebreak).
  ///
  /// If language=JAVASCRIPT, it is the evaluated string in the AS clause.
  /// For example, for the function created with the following statement:
  ///
  /// `CREATE FUNCTION f() RETURNS STRING LANGUAGE js AS 'return "\n";\n'`
  ///
  /// The definition_body is
  ///
  /// `return "\n";\n`
  ///
  /// Note that both \n are replaced with linebreaks.
  package var definitionBody: String {
    get {return _storage._definitionBody}
    set {_uniqueStorage()._definitionBody = newValue}
  }

  /// Optional. The description of the routine, if defined.
  package var description_p: String {
    get {return _storage._description_p}
    set {_uniqueStorage()._description_p = newValue}
  }

  /// Optional. The determinism level of the JavaScript UDF, if defined.
  package var determinismLevel: Google_Cloud_Bigquery_V2_Routine.DeterminismLevel {
    get {return _storage._determinismLevel}
    set {_uniqueStorage()._determinismLevel = newValue}
  }

  /// Optional. The security mode of the routine, if defined. If not defined, the
  /// security mode is automatically determined from the routine's configuration.
  package var securityMode: Google_Cloud_Bigquery_V2_Routine.SecurityMode {
    get {return _storage._securityMode}
    set {_uniqueStorage()._securityMode = newValue}
  }

  /// Optional. Use this option to catch many common errors. Error checking is
  /// not exhaustive, and successfully creating a procedure doesn't guarantee
  /// that the procedure will successfully execute at runtime. If `strictMode` is
  /// set to `TRUE`, the procedure body is further checked for errors such as
  /// non-existent tables or columns. The `CREATE PROCEDURE` statement fails if
  /// the body fails any of these checks.
  ///
  /// If `strictMode` is set to `FALSE`, the procedure body is checked only for
  /// syntax. For procedures that invoke themselves recursively, specify
  /// `strictMode=FALSE` to avoid non-existent procedure errors during
  /// validation.
  ///
  /// Default value is `TRUE`.
  package var strictMode: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _storage._strictMode ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_uniqueStorage()._strictMode = newValue}
  }
  /// Returns true if `strictMode` has been explicitly set.
  package var hasStrictMode: Bool {return _storage._strictMode != nil}
  /// Clears the value of `strictMode`. Subsequent reads from it will return its default value.
  package mutating func clearStrictMode() {_uniqueStorage()._strictMode = nil}

  /// Optional. Remote function specific options.
  package var remoteFunctionOptions: Google_Cloud_Bigquery_V2_Routine.RemoteFunctionOptions {
    get {return _storage._remoteFunctionOptions ?? Google_Cloud_Bigquery_V2_Routine.RemoteFunctionOptions()}
    set {_uniqueStorage()._remoteFunctionOptions = newValue}
  }
  /// Returns true if `remoteFunctionOptions` has been explicitly set.
  package var hasRemoteFunctionOptions: Bool {return _storage._remoteFunctionOptions != nil}
  /// Clears the value of `remoteFunctionOptions`. Subsequent reads from it will return its default value.
  package mutating func clearRemoteFunctionOptions() {_uniqueStorage()._remoteFunctionOptions = nil}

  /// Optional. Spark specific options.
  package var sparkOptions: Google_Cloud_Bigquery_V2_SparkOptions {
    get {return _storage._sparkOptions ?? Google_Cloud_Bigquery_V2_SparkOptions()}
    set {_uniqueStorage()._sparkOptions = newValue}
  }
  /// Returns true if `sparkOptions` has been explicitly set.
  package var hasSparkOptions: Bool {return _storage._sparkOptions != nil}
  /// Clears the value of `sparkOptions`. Subsequent reads from it will return its default value.
  package mutating func clearSparkOptions() {_uniqueStorage()._sparkOptions = nil}

  /// Optional. If set to `DATA_MASKING`, the function is validated and made
  /// available as a masking function. For more information, see [Create custom
  /// masking
  /// routines](https://cloud.google.com/bigquery/docs/user-defined-functions#custom-mask).
  package var dataGovernanceType: Google_Cloud_Bigquery_V2_Routine.DataGovernanceType {
    get {return _storage._dataGovernanceType}
    set {_uniqueStorage()._dataGovernanceType = newValue}
  }

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The fine-grained type of the routine.
  package enum RoutineType: SwiftProtobuf.Enum, Swift.CaseIterable {
    package typealias RawValue = Int

    /// Default value.
    case unspecified // = 0

    /// Non-built-in persistent scalar function.
    case scalarFunction // = 1

    /// Stored procedure.
    case procedure // = 2

    /// Non-built-in persistent TVF.
    case tableValuedFunction // = 3

    /// Non-built-in persistent aggregate function.
    case aggregateFunction // = 4
    case UNRECOGNIZED(Int)

    package init() {
      self = .unspecified
    }

    package init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .scalarFunction
      case 2: self = .procedure
      case 3: self = .tableValuedFunction
      case 4: self = .aggregateFunction
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    package var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .scalarFunction: return 1
      case .procedure: return 2
      case .tableValuedFunction: return 3
      case .aggregateFunction: return 4
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    package static let allCases: [Google_Cloud_Bigquery_V2_Routine.RoutineType] = [
      .unspecified,
      .scalarFunction,
      .procedure,
      .tableValuedFunction,
      .aggregateFunction,
    ]

  }

  /// The language of the routine.
  package enum Language: SwiftProtobuf.Enum, Swift.CaseIterable {
    package typealias RawValue = Int

    /// Default value.
    case unspecified // = 0

    /// SQL language.
    case sql // = 1

    /// JavaScript language.
    case javascript // = 2

    /// Python language.
    case python // = 3

    /// Java language.
    case java // = 4

    /// Scala language.
    case scala // = 5
    case UNRECOGNIZED(Int)

    package init() {
      self = .unspecified
    }

    package init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .sql
      case 2: self = .javascript
      case 3: self = .python
      case 4: self = .java
      case 5: self = .scala
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    package var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .sql: return 1
      case .javascript: return 2
      case .python: return 3
      case .java: return 4
      case .scala: return 5
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    package static let allCases: [Google_Cloud_Bigquery_V2_Routine.Language] = [
      .unspecified,
      .sql,
      .javascript,
      .python,
      .java,
      .scala,
    ]

  }

  /// JavaScript UDF determinism levels.
  ///
  /// If all JavaScript UDFs are DETERMINISTIC, the query result is
  /// potentially cachable (see below). If any JavaScript UDF is
  /// NOT_DETERMINISTIC, the query result is not cacheable.
  ///
  /// Even if a JavaScript UDF is deterministic, many other factors can prevent
  /// usage of cached query results. Example factors include but not limited to:
  /// DDL/DML, non-deterministic SQL function calls, update of referenced
  /// tables/views/UDFs or imported JavaScript libraries.
  ///
  /// SQL UDFs cannot have determinism specified. Their determinism is
  /// automatically determined.
  package enum DeterminismLevel: SwiftProtobuf.Enum, Swift.CaseIterable {
    package typealias RawValue = Int

    /// The determinism of the UDF is unspecified.
    case unspecified // = 0

    /// The UDF is deterministic, meaning that 2 function calls with the same
    /// inputs always produce the same result, even across 2 query runs.
    case deterministic // = 1

    /// The UDF is not deterministic.
    case notDeterministic // = 2
    case UNRECOGNIZED(Int)

    package init() {
      self = .unspecified
    }

    package init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .deterministic
      case 2: self = .notDeterministic
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    package var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .deterministic: return 1
      case .notDeterministic: return 2
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    package static let allCases: [Google_Cloud_Bigquery_V2_Routine.DeterminismLevel] = [
      .unspecified,
      .deterministic,
      .notDeterministic,
    ]

  }

  /// Security mode.
  package enum SecurityMode: SwiftProtobuf.Enum, Swift.CaseIterable {
    package typealias RawValue = Int

    /// The security mode of the routine is unspecified.
    case unspecified // = 0

    /// The routine is to be executed with the privileges of the user who
    /// defines it.
    case definer // = 1

    /// The routine is to be executed with the privileges of the user who
    /// invokes it.
    case invoker // = 2
    case UNRECOGNIZED(Int)

    package init() {
      self = .unspecified
    }

    package init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .definer
      case 2: self = .invoker
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    package var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .definer: return 1
      case .invoker: return 2
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    package static let allCases: [Google_Cloud_Bigquery_V2_Routine.SecurityMode] = [
      .unspecified,
      .definer,
      .invoker,
    ]

  }

  /// Data governance type values. Only supports `DATA_MASKING`.
  package enum DataGovernanceType: SwiftProtobuf.Enum, Swift.CaseIterable {
    package typealias RawValue = Int

    /// The data governance type is unspecified.
    case unspecified // = 0

    /// The data governance type is data masking.
    case dataMasking // = 1
    case UNRECOGNIZED(Int)

    package init() {
      self = .unspecified
    }

    package init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .dataMasking
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    package var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .dataMasking: return 1
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    package static let allCases: [Google_Cloud_Bigquery_V2_Routine.DataGovernanceType] = [
      .unspecified,
      .dataMasking,
    ]

  }

  /// Input/output argument of a function or a stored procedure.
  package struct Argument: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Optional. The name of this argument. Can be absent for function return
    /// argument.
    package var name: String = String()

    /// Optional. Defaults to FIXED_TYPE.
    package var argumentKind: Google_Cloud_Bigquery_V2_Routine.Argument.ArgumentKind = .unspecified

    /// Optional. Specifies whether the argument is input or output.
    /// Can be set for procedures only.
    package var mode: Google_Cloud_Bigquery_V2_Routine.Argument.Mode = .unspecified

    /// Set if argument_kind == FIXED_TYPE.
    package var dataType: Google_Cloud_Bigquery_V2_StandardSqlDataType {
      get {return _dataType ?? Google_Cloud_Bigquery_V2_StandardSqlDataType()}
      set {_dataType = newValue}
    }
    /// Returns true if `dataType` has been explicitly set.
    package var hasDataType: Bool {return self._dataType != nil}
    /// Clears the value of `dataType`. Subsequent reads from it will return its default value.
    package mutating func clearDataType() {self._dataType = nil}

    /// Optional. Whether the argument is an aggregate function parameter.
    /// Must be Unset for routine types other than AGGREGATE_FUNCTION.
    /// For AGGREGATE_FUNCTION, if set to false, it is equivalent to adding "NOT
    /// AGGREGATE" clause in DDL; Otherwise, it is equivalent to omitting "NOT
    /// AGGREGATE" clause in DDL.
    package var isAggregate: SwiftProtobuf.Google_Protobuf_BoolValue {
      get {return _isAggregate ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
      set {_isAggregate = newValue}
    }
    /// Returns true if `isAggregate` has been explicitly set.
    package var hasIsAggregate: Bool {return self._isAggregate != nil}
    /// Clears the value of `isAggregate`. Subsequent reads from it will return its default value.
    package mutating func clearIsAggregate() {self._isAggregate = nil}

    package var unknownFields = SwiftProtobuf.UnknownStorage()

    /// Represents the kind of a given argument.
    package enum ArgumentKind: SwiftProtobuf.Enum, Swift.CaseIterable {
      package typealias RawValue = Int

      /// Default value.
      case unspecified // = 0

      /// The argument is a variable with fully specified type, which can be a
      /// struct or an array, but not a table.
      case fixedType // = 1

      /// The argument is any type, including struct or array, but not a table.
      case anyType // = 2
      case UNRECOGNIZED(Int)

      package init() {
        self = .unspecified
      }

      package init?(rawValue: Int) {
        switch rawValue {
        case 0: self = .unspecified
        case 1: self = .fixedType
        case 2: self = .anyType
        default: self = .UNRECOGNIZED(rawValue)
        }
      }

      package var rawValue: Int {
        switch self {
        case .unspecified: return 0
        case .fixedType: return 1
        case .anyType: return 2
        case .UNRECOGNIZED(let i): return i
        }
      }

      // The compiler won't synthesize support with the UNRECOGNIZED case.
      package static let allCases: [Google_Cloud_Bigquery_V2_Routine.Argument.ArgumentKind] = [
        .unspecified,
        .fixedType,
        .anyType,
      ]

    }

    /// The input/output mode of the argument.
    package enum Mode: SwiftProtobuf.Enum, Swift.CaseIterable {
      package typealias RawValue = Int

      /// Default value.
      case unspecified // = 0

      /// The argument is input-only.
      case `in` // = 1

      /// The argument is output-only.
      case out // = 2

      /// The argument is both an input and an output.
      case `inout` // = 3
      case UNRECOGNIZED(Int)

      package init() {
        self = .unspecified
      }

      package init?(rawValue: Int) {
        switch rawValue {
        case 0: self = .unspecified
        case 1: self = .in
        case 2: self = .out
        case 3: self = .inout
        default: self = .UNRECOGNIZED(rawValue)
        }
      }

      package var rawValue: Int {
        switch self {
        case .unspecified: return 0
        case .in: return 1
        case .out: return 2
        case .inout: return 3
        case .UNRECOGNIZED(let i): return i
        }
      }

      // The compiler won't synthesize support with the UNRECOGNIZED case.
      package static let allCases: [Google_Cloud_Bigquery_V2_Routine.Argument.Mode] = [
        .unspecified,
        .in,
        .out,
        .inout,
      ]

    }

    package init() {}

    fileprivate var _dataType: Google_Cloud_Bigquery_V2_StandardSqlDataType? = nil
    fileprivate var _isAggregate: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
  }

  /// Options for a remote user-defined function.
  package struct RemoteFunctionOptions: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Endpoint of the user-provided remote service, e.g.
    /// ```https://us-east1-my_gcf_project.cloudfunctions.net/remote_add```
    package var endpoint: String = String()

    /// Fully qualified name of the user-provided connection object which holds
    /// the authentication information to send requests to the remote service.
    /// Format:
    /// ```"projects/{projectId}/locations/{locationId}/connections/{connectionId}"```
    package var connection: String = String()

    /// User-defined context as a set of key/value pairs, which will be sent as
    /// function invocation context together with batched arguments in the
    /// requests to the remote service. The total number of bytes of keys and
    /// values must be less than 8KB.
    package var userDefinedContext: Dictionary<String,String> = [:]

    /// Max number of rows in each batch sent to the remote service.
    /// If absent or if 0, BigQuery dynamically decides the number of rows in a
    /// batch.
    package var maxBatchingRows: Int64 = 0

    package var unknownFields = SwiftProtobuf.UnknownStorage()

    package init() {}
  }

  package init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// Options for a user-defined Spark routine.
package struct Google_Cloud_Bigquery_V2_SparkOptions: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Fully qualified name of the user-provided Spark connection object. Format:
  /// ```"projects/{project_id}/locations/{location_id}/connections/{connection_id}"```
  package var connection: String = String()

  /// Runtime version. If not specified, the default runtime version is used.
  package var runtimeVersion: String = String()

  /// Custom container image for the runtime environment.
  package var containerImage: String = String()

  /// Configuration properties as a set of key/value pairs, which will be passed
  /// on to the Spark application. For more information, see
  /// [Apache Spark](https://spark.apache.org/docs/latest/index.html) and the
  /// [procedure option
  /// list](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#procedure_option_list).
  package var properties: Dictionary<String,String> = [:]

  /// The main file/jar URI of the Spark application. Exactly one of the
  /// definition_body field and the main_file_uri field must be set for Python.
  /// Exactly one of main_class and main_file_uri field
  /// should be set for Java/Scala language type.
  package var mainFileUri: String = String()

  /// Python files to be placed on the PYTHONPATH for PySpark application.
  /// Supported file types: `.py`, `.egg`, and `.zip`. For more information
  /// about Apache Spark, see
  /// [Apache Spark](https://spark.apache.org/docs/latest/index.html).
  package var pyFileUris: [String] = []

  /// JARs to include on the driver and executor CLASSPATH.
  /// For more information about Apache Spark, see
  /// [Apache Spark](https://spark.apache.org/docs/latest/index.html).
  package var jarUris: [String] = []

  /// Files to be placed in the working directory of each executor.
  /// For more information about Apache Spark, see
  /// [Apache Spark](https://spark.apache.org/docs/latest/index.html).
  package var fileUris: [String] = []

  /// Archive files to be extracted into the working directory of each executor.
  /// For more information about Apache Spark, see
  /// [Apache Spark](https://spark.apache.org/docs/latest/index.html).
  package var archiveUris: [String] = []

  /// The fully qualified name of a class in jar_uris, for example,
  /// com.example.wordcount. Exactly one of main_class and main_jar_uri field
  ///  should be set for Java/Scala language type.
  package var mainClass: String = String()

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}
}

/// Describes the format for getting information about a routine.
package struct Google_Cloud_Bigquery_V2_GetRoutineRequest: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Project ID of the requested routine
  package var projectID: String = String()

  /// Required. Dataset ID of the requested routine
  package var datasetID: String = String()

  /// Required. Routine ID of the requested routine
  package var routineID: String = String()

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}
}

/// Describes the format for inserting a routine.
package struct Google_Cloud_Bigquery_V2_InsertRoutineRequest: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Project ID of the new routine
  package var projectID: String = String()

  /// Required. Dataset ID of the new routine
  package var datasetID: String = String()

  /// Required. A routine resource to insert
  package var routine: Google_Cloud_Bigquery_V2_Routine {
    get {return _routine ?? Google_Cloud_Bigquery_V2_Routine()}
    set {_routine = newValue}
  }
  /// Returns true if `routine` has been explicitly set.
  package var hasRoutine: Bool {return self._routine != nil}
  /// Clears the value of `routine`. Subsequent reads from it will return its default value.
  package mutating func clearRoutine() {self._routine = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _routine: Google_Cloud_Bigquery_V2_Routine? = nil
}

/// Describes the format for updating a routine.
package struct Google_Cloud_Bigquery_V2_UpdateRoutineRequest: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Project ID of the routine to update
  package var projectID: String = String()

  /// Required. Dataset ID of the routine to update
  package var datasetID: String = String()

  /// Required. Routine ID of the routine to update
  package var routineID: String = String()

  /// Required. A routine resource which will replace the specified routine
  package var routine: Google_Cloud_Bigquery_V2_Routine {
    get {return _routine ?? Google_Cloud_Bigquery_V2_Routine()}
    set {_routine = newValue}
  }
  /// Returns true if `routine` has been explicitly set.
  package var hasRoutine: Bool {return self._routine != nil}
  /// Clears the value of `routine`. Subsequent reads from it will return its default value.
  package mutating func clearRoutine() {self._routine = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _routine: Google_Cloud_Bigquery_V2_Routine? = nil
}

/// Describes the format for the partial update (patch) of a routine.
package struct Google_Cloud_Bigquery_V2_PatchRoutineRequest: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Project ID of the routine to update
  package var projectID: String = String()

  /// Required. Dataset ID of the routine to update
  package var datasetID: String = String()

  /// Required. Routine ID of the routine to update
  package var routineID: String = String()

  /// Required. A routine resource which will be used to partially
  /// update the specified routine
  package var routine: Google_Cloud_Bigquery_V2_Routine {
    get {return _routine ?? Google_Cloud_Bigquery_V2_Routine()}
    set {_routine = newValue}
  }
  /// Returns true if `routine` has been explicitly set.
  package var hasRoutine: Bool {return self._routine != nil}
  /// Clears the value of `routine`. Subsequent reads from it will return its default value.
  package mutating func clearRoutine() {self._routine = nil}

  /// Only the Routine fields in the field mask are updated
  /// by the given routine. Repeated routine fields will be fully replaced
  /// if contained in the field mask.
  package var fieldMask: SwiftProtobuf.Google_Protobuf_FieldMask {
    get {return _fieldMask ?? SwiftProtobuf.Google_Protobuf_FieldMask()}
    set {_fieldMask = newValue}
  }
  /// Returns true if `fieldMask` has been explicitly set.
  package var hasFieldMask: Bool {return self._fieldMask != nil}
  /// Clears the value of `fieldMask`. Subsequent reads from it will return its default value.
  package mutating func clearFieldMask() {self._fieldMask = nil}

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _routine: Google_Cloud_Bigquery_V2_Routine? = nil
  fileprivate var _fieldMask: SwiftProtobuf.Google_Protobuf_FieldMask? = nil
}

/// Describes the format for deleting a routine.
package struct Google_Cloud_Bigquery_V2_DeleteRoutineRequest: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Project ID of the routine to delete
  package var projectID: String = String()

  /// Required. Dataset ID of the routine to delete
  package var datasetID: String = String()

  /// Required. Routine ID of the routine to delete
  package var routineID: String = String()

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}
}

/// Describes the format for listing routines.
package struct Google_Cloud_Bigquery_V2_ListRoutinesRequest: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Project ID of the routines to list
  package var projectID: String = String()

  /// Required. Dataset ID of the routines to list
  package var datasetID: String = String()

  /// The maximum number of results to return in a single response page.
  /// Leverage the page tokens to iterate through the entire collection.
  package var maxResults: SwiftProtobuf.Google_Protobuf_UInt32Value {
    get {return _maxResults ?? SwiftProtobuf.Google_Protobuf_UInt32Value()}
    set {_maxResults = newValue}
  }
  /// Returns true if `maxResults` has been explicitly set.
  package var hasMaxResults: Bool {return self._maxResults != nil}
  /// Clears the value of `maxResults`. Subsequent reads from it will return its default value.
  package mutating func clearMaxResults() {self._maxResults = nil}

  /// Page token, returned by a previous call, to request the next page of
  /// results
  package var pageToken: String = String()

  /// If set, then only the Routines matching this filter are returned.
  /// The supported format is `routineType:{RoutineType}`, where `{RoutineType}`
  /// is a RoutineType enum. For example: `routineType:SCALAR_FUNCTION`.
  package var filter: String = String()

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _maxResults: SwiftProtobuf.Google_Protobuf_UInt32Value? = nil
}

/// Describes the format of a single result page when listing routines.
package struct Google_Cloud_Bigquery_V2_ListRoutinesResponse: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Routines in the requested dataset. Unless read_mask is set in the request,
  /// only the following fields are populated:
  /// etag, project_id, dataset_id, routine_id, routine_type, creation_time,
  /// last_modified_time, language, and remote_function_options.
  package var routines: [Google_Cloud_Bigquery_V2_Routine] = []

  /// A token to request the next page of results.
  package var nextPageToken: String = String()

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.bigquery.v2"

extension Google_Cloud_Bigquery_V2_Routine: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".Routine"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "etag"),
    2: .standard(proto: "routine_reference"),
    3: .standard(proto: "routine_type"),
    4: .standard(proto: "creation_time"),
    5: .standard(proto: "last_modified_time"),
    6: .same(proto: "language"),
    7: .same(proto: "arguments"),
    10: .standard(proto: "return_type"),
    13: .standard(proto: "return_table_type"),
    8: .standard(proto: "imported_libraries"),
    9: .standard(proto: "definition_body"),
    11: .same(proto: "description"),
    12: .standard(proto: "determinism_level"),
    18: .standard(proto: "security_mode"),
    14: .standard(proto: "strict_mode"),
    15: .standard(proto: "remote_function_options"),
    16: .standard(proto: "spark_options"),
    17: .standard(proto: "data_governance_type"),
  ]

  fileprivate class _StorageClass {
    var _etag: String = String()
    var _routineReference: Google_Cloud_Bigquery_V2_RoutineReference? = nil
    var _routineType: Google_Cloud_Bigquery_V2_Routine.RoutineType = .unspecified
    var _creationTime: Int64 = 0
    var _lastModifiedTime: Int64 = 0
    var _language: Google_Cloud_Bigquery_V2_Routine.Language = .unspecified
    var _arguments: [Google_Cloud_Bigquery_V2_Routine.Argument] = []
    var _returnType: Google_Cloud_Bigquery_V2_StandardSqlDataType? = nil
    var _returnTableType: Google_Cloud_Bigquery_V2_StandardSqlTableType? = nil
    var _importedLibraries: [String] = []
    var _definitionBody: String = String()
    var _description_p: String = String()
    var _determinismLevel: Google_Cloud_Bigquery_V2_Routine.DeterminismLevel = .unspecified
    var _securityMode: Google_Cloud_Bigquery_V2_Routine.SecurityMode = .unspecified
    var _strictMode: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
    var _remoteFunctionOptions: Google_Cloud_Bigquery_V2_Routine.RemoteFunctionOptions? = nil
    var _sparkOptions: Google_Cloud_Bigquery_V2_SparkOptions? = nil
    var _dataGovernanceType: Google_Cloud_Bigquery_V2_Routine.DataGovernanceType = .unspecified

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _etag = source._etag
      _routineReference = source._routineReference
      _routineType = source._routineType
      _creationTime = source._creationTime
      _lastModifiedTime = source._lastModifiedTime
      _language = source._language
      _arguments = source._arguments
      _returnType = source._returnType
      _returnTableType = source._returnTableType
      _importedLibraries = source._importedLibraries
      _definitionBody = source._definitionBody
      _description_p = source._description_p
      _determinismLevel = source._determinismLevel
      _securityMode = source._securityMode
      _strictMode = source._strictMode
      _remoteFunctionOptions = source._remoteFunctionOptions
      _sparkOptions = source._sparkOptions
      _dataGovernanceType = source._dataGovernanceType
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularStringField(value: &_storage._etag) }()
        case 2: try { try decoder.decodeSingularMessageField(value: &_storage._routineReference) }()
        case 3: try { try decoder.decodeSingularEnumField(value: &_storage._routineType) }()
        case 4: try { try decoder.decodeSingularInt64Field(value: &_storage._creationTime) }()
        case 5: try { try decoder.decodeSingularInt64Field(value: &_storage._lastModifiedTime) }()
        case 6: try { try decoder.decodeSingularEnumField(value: &_storage._language) }()
        case 7: try { try decoder.decodeRepeatedMessageField(value: &_storage._arguments) }()
        case 8: try { try decoder.decodeRepeatedStringField(value: &_storage._importedLibraries) }()
        case 9: try { try decoder.decodeSingularStringField(value: &_storage._definitionBody) }()
        case 10: try { try decoder.decodeSingularMessageField(value: &_storage._returnType) }()
        case 11: try { try decoder.decodeSingularStringField(value: &_storage._description_p) }()
        case 12: try { try decoder.decodeSingularEnumField(value: &_storage._determinismLevel) }()
        case 13: try { try decoder.decodeSingularMessageField(value: &_storage._returnTableType) }()
        case 14: try { try decoder.decodeSingularMessageField(value: &_storage._strictMode) }()
        case 15: try { try decoder.decodeSingularMessageField(value: &_storage._remoteFunctionOptions) }()
        case 16: try { try decoder.decodeSingularMessageField(value: &_storage._sparkOptions) }()
        case 17: try { try decoder.decodeSingularEnumField(value: &_storage._dataGovernanceType) }()
        case 18: try { try decoder.decodeSingularEnumField(value: &_storage._securityMode) }()
        default: break
        }
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      if !_storage._etag.isEmpty {
        try visitor.visitSingularStringField(value: _storage._etag, fieldNumber: 1)
      }
      try { if let v = _storage._routineReference {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      } }()
      if _storage._routineType != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._routineType, fieldNumber: 3)
      }
      if _storage._creationTime != 0 {
        try visitor.visitSingularInt64Field(value: _storage._creationTime, fieldNumber: 4)
      }
      if _storage._lastModifiedTime != 0 {
        try visitor.visitSingularInt64Field(value: _storage._lastModifiedTime, fieldNumber: 5)
      }
      if _storage._language != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._language, fieldNumber: 6)
      }
      if !_storage._arguments.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._arguments, fieldNumber: 7)
      }
      if !_storage._importedLibraries.isEmpty {
        try visitor.visitRepeatedStringField(value: _storage._importedLibraries, fieldNumber: 8)
      }
      if !_storage._definitionBody.isEmpty {
        try visitor.visitSingularStringField(value: _storage._definitionBody, fieldNumber: 9)
      }
      try { if let v = _storage._returnType {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 10)
      } }()
      if !_storage._description_p.isEmpty {
        try visitor.visitSingularStringField(value: _storage._description_p, fieldNumber: 11)
      }
      if _storage._determinismLevel != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._determinismLevel, fieldNumber: 12)
      }
      try { if let v = _storage._returnTableType {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 13)
      } }()
      try { if let v = _storage._strictMode {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 14)
      } }()
      try { if let v = _storage._remoteFunctionOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 15)
      } }()
      try { if let v = _storage._sparkOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 16)
      } }()
      if _storage._dataGovernanceType != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._dataGovernanceType, fieldNumber: 17)
      }
      if _storage._securityMode != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._securityMode, fieldNumber: 18)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_Routine, rhs: Google_Cloud_Bigquery_V2_Routine) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._etag != rhs_storage._etag {return false}
        if _storage._routineReference != rhs_storage._routineReference {return false}
        if _storage._routineType != rhs_storage._routineType {return false}
        if _storage._creationTime != rhs_storage._creationTime {return false}
        if _storage._lastModifiedTime != rhs_storage._lastModifiedTime {return false}
        if _storage._language != rhs_storage._language {return false}
        if _storage._arguments != rhs_storage._arguments {return false}
        if _storage._returnType != rhs_storage._returnType {return false}
        if _storage._returnTableType != rhs_storage._returnTableType {return false}
        if _storage._importedLibraries != rhs_storage._importedLibraries {return false}
        if _storage._definitionBody != rhs_storage._definitionBody {return false}
        if _storage._description_p != rhs_storage._description_p {return false}
        if _storage._determinismLevel != rhs_storage._determinismLevel {return false}
        if _storage._securityMode != rhs_storage._securityMode {return false}
        if _storage._strictMode != rhs_storage._strictMode {return false}
        if _storage._remoteFunctionOptions != rhs_storage._remoteFunctionOptions {return false}
        if _storage._sparkOptions != rhs_storage._sparkOptions {return false}
        if _storage._dataGovernanceType != rhs_storage._dataGovernanceType {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_Routine.RoutineType: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "ROUTINE_TYPE_UNSPECIFIED"),
    1: .same(proto: "SCALAR_FUNCTION"),
    2: .same(proto: "PROCEDURE"),
    3: .same(proto: "TABLE_VALUED_FUNCTION"),
    4: .same(proto: "AGGREGATE_FUNCTION"),
  ]
}

extension Google_Cloud_Bigquery_V2_Routine.Language: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "LANGUAGE_UNSPECIFIED"),
    1: .same(proto: "SQL"),
    2: .same(proto: "JAVASCRIPT"),
    3: .same(proto: "PYTHON"),
    4: .same(proto: "JAVA"),
    5: .same(proto: "SCALA"),
  ]
}

extension Google_Cloud_Bigquery_V2_Routine.DeterminismLevel: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "DETERMINISM_LEVEL_UNSPECIFIED"),
    1: .same(proto: "DETERMINISTIC"),
    2: .same(proto: "NOT_DETERMINISTIC"),
  ]
}

extension Google_Cloud_Bigquery_V2_Routine.SecurityMode: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "SECURITY_MODE_UNSPECIFIED"),
    1: .same(proto: "DEFINER"),
    2: .same(proto: "INVOKER"),
  ]
}

extension Google_Cloud_Bigquery_V2_Routine.DataGovernanceType: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "DATA_GOVERNANCE_TYPE_UNSPECIFIED"),
    1: .same(proto: "DATA_MASKING"),
  ]
}

extension Google_Cloud_Bigquery_V2_Routine.Argument: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = Google_Cloud_Bigquery_V2_Routine.protoMessageName + ".Argument"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .standard(proto: "argument_kind"),
    3: .same(proto: "mode"),
    4: .standard(proto: "data_type"),
    6: .standard(proto: "is_aggregate"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 2: try { try decoder.decodeSingularEnumField(value: &self.argumentKind) }()
      case 3: try { try decoder.decodeSingularEnumField(value: &self.mode) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._dataType) }()
      case 6: try { try decoder.decodeSingularMessageField(value: &self._isAggregate) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if self.argumentKind != .unspecified {
      try visitor.visitSingularEnumField(value: self.argumentKind, fieldNumber: 2)
    }
    if self.mode != .unspecified {
      try visitor.visitSingularEnumField(value: self.mode, fieldNumber: 3)
    }
    try { if let v = self._dataType {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    } }()
    try { if let v = self._isAggregate {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_Routine.Argument, rhs: Google_Cloud_Bigquery_V2_Routine.Argument) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs.argumentKind != rhs.argumentKind {return false}
    if lhs.mode != rhs.mode {return false}
    if lhs._dataType != rhs._dataType {return false}
    if lhs._isAggregate != rhs._isAggregate {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_Routine.Argument.ArgumentKind: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "ARGUMENT_KIND_UNSPECIFIED"),
    1: .same(proto: "FIXED_TYPE"),
    2: .same(proto: "ANY_TYPE"),
  ]
}

extension Google_Cloud_Bigquery_V2_Routine.Argument.Mode: SwiftProtobuf._ProtoNameProviding {
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "MODE_UNSPECIFIED"),
    1: .same(proto: "IN"),
    2: .same(proto: "OUT"),
    3: .same(proto: "INOUT"),
  ]
}

extension Google_Cloud_Bigquery_V2_Routine.RemoteFunctionOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = Google_Cloud_Bigquery_V2_Routine.protoMessageName + ".RemoteFunctionOptions"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "endpoint"),
    2: .same(proto: "connection"),
    3: .standard(proto: "user_defined_context"),
    4: .standard(proto: "max_batching_rows"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.endpoint) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.connection) }()
      case 3: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.userDefinedContext) }()
      case 4: try { try decoder.decodeSingularInt64Field(value: &self.maxBatchingRows) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.endpoint.isEmpty {
      try visitor.visitSingularStringField(value: self.endpoint, fieldNumber: 1)
    }
    if !self.connection.isEmpty {
      try visitor.visitSingularStringField(value: self.connection, fieldNumber: 2)
    }
    if !self.userDefinedContext.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.userDefinedContext, fieldNumber: 3)
    }
    if self.maxBatchingRows != 0 {
      try visitor.visitSingularInt64Field(value: self.maxBatchingRows, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_Routine.RemoteFunctionOptions, rhs: Google_Cloud_Bigquery_V2_Routine.RemoteFunctionOptions) -> Bool {
    if lhs.endpoint != rhs.endpoint {return false}
    if lhs.connection != rhs.connection {return false}
    if lhs.userDefinedContext != rhs.userDefinedContext {return false}
    if lhs.maxBatchingRows != rhs.maxBatchingRows {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_SparkOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".SparkOptions"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "connection"),
    2: .standard(proto: "runtime_version"),
    3: .standard(proto: "container_image"),
    4: .same(proto: "properties"),
    5: .standard(proto: "main_file_uri"),
    6: .standard(proto: "py_file_uris"),
    7: .standard(proto: "jar_uris"),
    8: .standard(proto: "file_uris"),
    9: .standard(proto: "archive_uris"),
    10: .standard(proto: "main_class"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.connection) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.runtimeVersion) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.containerImage) }()
      case 4: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.properties) }()
      case 5: try { try decoder.decodeSingularStringField(value: &self.mainFileUri) }()
      case 6: try { try decoder.decodeRepeatedStringField(value: &self.pyFileUris) }()
      case 7: try { try decoder.decodeRepeatedStringField(value: &self.jarUris) }()
      case 8: try { try decoder.decodeRepeatedStringField(value: &self.fileUris) }()
      case 9: try { try decoder.decodeRepeatedStringField(value: &self.archiveUris) }()
      case 10: try { try decoder.decodeSingularStringField(value: &self.mainClass) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.connection.isEmpty {
      try visitor.visitSingularStringField(value: self.connection, fieldNumber: 1)
    }
    if !self.runtimeVersion.isEmpty {
      try visitor.visitSingularStringField(value: self.runtimeVersion, fieldNumber: 2)
    }
    if !self.containerImage.isEmpty {
      try visitor.visitSingularStringField(value: self.containerImage, fieldNumber: 3)
    }
    if !self.properties.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.properties, fieldNumber: 4)
    }
    if !self.mainFileUri.isEmpty {
      try visitor.visitSingularStringField(value: self.mainFileUri, fieldNumber: 5)
    }
    if !self.pyFileUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.pyFileUris, fieldNumber: 6)
    }
    if !self.jarUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.jarUris, fieldNumber: 7)
    }
    if !self.fileUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.fileUris, fieldNumber: 8)
    }
    if !self.archiveUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.archiveUris, fieldNumber: 9)
    }
    if !self.mainClass.isEmpty {
      try visitor.visitSingularStringField(value: self.mainClass, fieldNumber: 10)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_SparkOptions, rhs: Google_Cloud_Bigquery_V2_SparkOptions) -> Bool {
    if lhs.connection != rhs.connection {return false}
    if lhs.runtimeVersion != rhs.runtimeVersion {return false}
    if lhs.containerImage != rhs.containerImage {return false}
    if lhs.properties != rhs.properties {return false}
    if lhs.mainFileUri != rhs.mainFileUri {return false}
    if lhs.pyFileUris != rhs.pyFileUris {return false}
    if lhs.jarUris != rhs.jarUris {return false}
    if lhs.fileUris != rhs.fileUris {return false}
    if lhs.archiveUris != rhs.archiveUris {return false}
    if lhs.mainClass != rhs.mainClass {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_GetRoutineRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".GetRoutineRequest"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    2: .standard(proto: "dataset_id"),
    3: .standard(proto: "routine_id"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.datasetID) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.routineID) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if !self.datasetID.isEmpty {
      try visitor.visitSingularStringField(value: self.datasetID, fieldNumber: 2)
    }
    if !self.routineID.isEmpty {
      try visitor.visitSingularStringField(value: self.routineID, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_GetRoutineRequest, rhs: Google_Cloud_Bigquery_V2_GetRoutineRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.datasetID != rhs.datasetID {return false}
    if lhs.routineID != rhs.routineID {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_InsertRoutineRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".InsertRoutineRequest"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    2: .standard(proto: "dataset_id"),
    3: .same(proto: "routine"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.datasetID) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._routine) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if !self.datasetID.isEmpty {
      try visitor.visitSingularStringField(value: self.datasetID, fieldNumber: 2)
    }
    try { if let v = self._routine {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_InsertRoutineRequest, rhs: Google_Cloud_Bigquery_V2_InsertRoutineRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.datasetID != rhs.datasetID {return false}
    if lhs._routine != rhs._routine {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_UpdateRoutineRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".UpdateRoutineRequest"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    2: .standard(proto: "dataset_id"),
    3: .standard(proto: "routine_id"),
    4: .same(proto: "routine"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.datasetID) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.routineID) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._routine) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if !self.datasetID.isEmpty {
      try visitor.visitSingularStringField(value: self.datasetID, fieldNumber: 2)
    }
    if !self.routineID.isEmpty {
      try visitor.visitSingularStringField(value: self.routineID, fieldNumber: 3)
    }
    try { if let v = self._routine {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_UpdateRoutineRequest, rhs: Google_Cloud_Bigquery_V2_UpdateRoutineRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.datasetID != rhs.datasetID {return false}
    if lhs.routineID != rhs.routineID {return false}
    if lhs._routine != rhs._routine {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_PatchRoutineRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".PatchRoutineRequest"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    2: .standard(proto: "dataset_id"),
    3: .standard(proto: "routine_id"),
    4: .same(proto: "routine"),
    5: .standard(proto: "field_mask"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.datasetID) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.routineID) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._routine) }()
      case 5: try { try decoder.decodeSingularMessageField(value: &self._fieldMask) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if !self.datasetID.isEmpty {
      try visitor.visitSingularStringField(value: self.datasetID, fieldNumber: 2)
    }
    if !self.routineID.isEmpty {
      try visitor.visitSingularStringField(value: self.routineID, fieldNumber: 3)
    }
    try { if let v = self._routine {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    } }()
    try { if let v = self._fieldMask {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_PatchRoutineRequest, rhs: Google_Cloud_Bigquery_V2_PatchRoutineRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.datasetID != rhs.datasetID {return false}
    if lhs.routineID != rhs.routineID {return false}
    if lhs._routine != rhs._routine {return false}
    if lhs._fieldMask != rhs._fieldMask {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_DeleteRoutineRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".DeleteRoutineRequest"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    2: .standard(proto: "dataset_id"),
    3: .standard(proto: "routine_id"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.datasetID) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.routineID) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if !self.datasetID.isEmpty {
      try visitor.visitSingularStringField(value: self.datasetID, fieldNumber: 2)
    }
    if !self.routineID.isEmpty {
      try visitor.visitSingularStringField(value: self.routineID, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_DeleteRoutineRequest, rhs: Google_Cloud_Bigquery_V2_DeleteRoutineRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.datasetID != rhs.datasetID {return false}
    if lhs.routineID != rhs.routineID {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_ListRoutinesRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".ListRoutinesRequest"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    2: .standard(proto: "dataset_id"),
    3: .standard(proto: "max_results"),
    4: .standard(proto: "page_token"),
    6: .same(proto: "filter"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.datasetID) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._maxResults) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.pageToken) }()
      case 6: try { try decoder.decodeSingularStringField(value: &self.filter) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if !self.datasetID.isEmpty {
      try visitor.visitSingularStringField(value: self.datasetID, fieldNumber: 2)
    }
    try { if let v = self._maxResults {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    } }()
    if !self.pageToken.isEmpty {
      try visitor.visitSingularStringField(value: self.pageToken, fieldNumber: 4)
    }
    if !self.filter.isEmpty {
      try visitor.visitSingularStringField(value: self.filter, fieldNumber: 6)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_ListRoutinesRequest, rhs: Google_Cloud_Bigquery_V2_ListRoutinesRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.datasetID != rhs.datasetID {return false}
    if lhs._maxResults != rhs._maxResults {return false}
    if lhs.pageToken != rhs.pageToken {return false}
    if lhs.filter != rhs.filter {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Bigquery_V2_ListRoutinesResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".ListRoutinesResponse"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "routines"),
    2: .standard(proto: "next_page_token"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.routines) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.nextPageToken) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.routines.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.routines, fieldNumber: 1)
    }
    if !self.nextPageToken.isEmpty {
      try visitor.visitSingularStringField(value: self.nextPageToken, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_ListRoutinesResponse, rhs: Google_Cloud_Bigquery_V2_ListRoutinesResponse) -> Bool {
    if lhs.routines != rhs.routines {return false}
    if lhs.nextPageToken != rhs.nextPageToken {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
