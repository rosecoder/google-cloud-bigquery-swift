// DO NOT EDIT.
// swift-format-ignore-file
// swiftlint:disable all
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/bigquery/v2/hive_partitioning.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// Options for configuring hive partitioning detect.
package struct Google_Cloud_Bigquery_V2_HivePartitioningOptions: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. When set, what mode of hive partitioning to use when reading
  /// data.  The following modes are supported:
  ///
  /// * AUTO: automatically infer partition key name(s) and type(s).
  ///
  /// * STRINGS: automatically infer partition key name(s).  All types are
  /// strings.
  ///
  /// * CUSTOM: partition key schema is encoded in the source URI prefix.
  ///
  /// Not all storage formats support hive partitioning. Requesting hive
  /// partitioning on an unsupported format will lead to an error.
  /// Currently supported formats are: JSON, CSV, ORC, Avro and Parquet.
  package var mode: String = String()

  /// Optional. When hive partition detection is requested, a common prefix for
  /// all source uris must be required.  The prefix must end immediately before
  /// the partition key encoding begins. For example, consider files following
  /// this data layout:
  ///
  /// gs://bucket/path_to_table/dt=2019-06-01/country=USA/id=7/file.avro
  ///
  /// gs://bucket/path_to_table/dt=2019-05-31/country=CA/id=3/file.avro
  ///
  /// When hive partitioning is requested with either AUTO or STRINGS detection,
  /// the common prefix can be either of gs://bucket/path_to_table or
  /// gs://bucket/path_to_table/.
  ///
  /// CUSTOM detection requires encoding the partitioning schema immediately
  /// after the common prefix.  For CUSTOM, any of
  ///
  /// * gs://bucket/path_to_table/{dt:DATE}/{country:STRING}/{id:INTEGER}
  ///
  /// * gs://bucket/path_to_table/{dt:STRING}/{country:STRING}/{id:INTEGER}
  ///
  /// * gs://bucket/path_to_table/{dt:DATE}/{country:STRING}/{id:STRING}
  ///
  /// would all be valid source URI prefixes.
  package var sourceUriPrefix: String = String()

  /// Optional. If set to true, queries over this table require a partition
  /// filter that can be used for partition elimination to be specified.
  ///
  /// Note that this field should only be true when creating a permanent
  /// external table or querying a temporary external table.
  ///
  /// Hive-partitioned loads with require_partition_filter explicitly set to
  /// true will fail.
  package var requirePartitionFilter: SwiftProtobuf.Google_Protobuf_BoolValue {
    get {return _requirePartitionFilter ?? SwiftProtobuf.Google_Protobuf_BoolValue()}
    set {_requirePartitionFilter = newValue}
  }
  /// Returns true if `requirePartitionFilter` has been explicitly set.
  package var hasRequirePartitionFilter: Bool {return self._requirePartitionFilter != nil}
  /// Clears the value of `requirePartitionFilter`. Subsequent reads from it will return its default value.
  package mutating func clearRequirePartitionFilter() {self._requirePartitionFilter = nil}

  /// Output only. For permanent external tables, this field is populated with
  /// the hive partition keys in the order they were inferred. The types of the
  /// partition keys can be deduced by checking the table schema (which will
  /// include the partition keys). Not every API will populate this field in the
  /// output. For example, Tables.Get will populate it, but Tables.List will not
  /// contain this field.
  package var fields: [String] = []

  package var unknownFields = SwiftProtobuf.UnknownStorage()

  package init() {}

  fileprivate var _requirePartitionFilter: SwiftProtobuf.Google_Protobuf_BoolValue? = nil
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.bigquery.v2"

extension Google_Cloud_Bigquery_V2_HivePartitioningOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  package static let protoMessageName: String = _protobuf_package + ".HivePartitioningOptions"
  package static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "mode"),
    2: .standard(proto: "source_uri_prefix"),
    3: .standard(proto: "require_partition_filter"),
    4: .same(proto: "fields"),
  ]

  package mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.mode) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.sourceUriPrefix) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._requirePartitionFilter) }()
      case 4: try { try decoder.decodeRepeatedStringField(value: &self.fields) }()
      default: break
      }
    }
  }

  package func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.mode.isEmpty {
      try visitor.visitSingularStringField(value: self.mode, fieldNumber: 1)
    }
    if !self.sourceUriPrefix.isEmpty {
      try visitor.visitSingularStringField(value: self.sourceUriPrefix, fieldNumber: 2)
    }
    try { if let v = self._requirePartitionFilter {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    } }()
    if !self.fields.isEmpty {
      try visitor.visitRepeatedStringField(value: self.fields, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  package static func ==(lhs: Google_Cloud_Bigquery_V2_HivePartitioningOptions, rhs: Google_Cloud_Bigquery_V2_HivePartitioningOptions) -> Bool {
    if lhs.mode != rhs.mode {return false}
    if lhs.sourceUriPrefix != rhs.sourceUriPrefix {return false}
    if lhs._requirePartitionFilter != rhs._requirePartitionFilter {return false}
    if lhs.fields != rhs.fields {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
